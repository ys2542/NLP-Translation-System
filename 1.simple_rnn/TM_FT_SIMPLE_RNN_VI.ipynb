{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re  \n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_zh = '/home/ys2542/Neural-Translation-System/iwslt-zh-en-processed/'\n",
    "path_vi = '/home/ys2542/Neural-Translation-System/iwslt-vi-en-processed/'\n",
    "ft_home = '/scratch/ys2542/NLP_FASTTEXT/'\n",
    "model_path = '/scratch/ys2542/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "UNK_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 100000 \n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_en = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_en = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_en = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_en = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_en[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = idx\n",
    "        idx2words_ft_en[idx] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ft_home + 'cc.vi.300.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_zh = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_zh = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_zh = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_zh = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_zh[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_zh[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_zh[s[0]] = idx\n",
    "        idx2words_ft_zh[idx] = s[0]\n",
    "        ordered_words_ft_zh.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_zh_train = open(path_vi + 'train.tok.vi', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_train = open(path_vi + 'train.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_val = open(path_vi + 'dev.tok.vi', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_val = open(path_vi + 'dev.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_test = open(path_vi + 'test.tok.vi', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_test = open(path_vi + 'test.tok.en', encoding = 'utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lines(lines, lang):\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "            line = ' '\n",
    "        if lang == 'en':\n",
    "            line = line.replace(\"&apos;\", \"\").replace(\"&quot;\", \"\")\n",
    "        if line[-1] != ' ':\n",
    "            line = line + ' '\n",
    "       \n",
    "        line = '<sos> ' + line + '<eos>'\n",
    "        data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh = clean_lines(lines_zh_train, 'vi')\n",
    "train_en = clean_lines(lines_en_train, 'en')\n",
    "\n",
    "val_zh = clean_lines(lines_zh_val, 'vi')\n",
    "val_en = clean_lines(lines_en_val, 'en')\n",
    "\n",
    "test_zh = clean_lines(lines_zh_test, 'vi')\n",
    "test_en = clean_lines(lines_en_test, 'en')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(data, lang):\n",
    "    indexes = []\n",
    "    for sentence in data:\n",
    "        index = []\n",
    "        for token in sentence.split():\n",
    "            if lang == 'vi':\n",
    "                try:\n",
    "                    index.append(words_ft_zh[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "            elif lang == 'en':\n",
    "                try:\n",
    "                    index.append(words_ft_en[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "        indexes.append(index)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh_indexes = indexesFromSentence(train_zh, 'vi')\n",
    "train_en_indexes = indexesFromSentence(train_en, 'en')\n",
    "\n",
    "val_zh_indexes = indexesFromSentence(val_zh, 'vi')\n",
    "val_en_indexes = indexesFromSentence(val_en, 'en')\n",
    "\n",
    "test_zh_indexes = indexesFromSentence(test_zh, 'vi')\n",
    "test_en_indexes = indexesFromSentence(test_en, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "length_zh = []\n",
    "for line in train_zh_indexes:\n",
    "        length_zh.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_zh)\n",
    "MAX_LENGTH_ZH = length_zh[int(len(train_zh_indexes)*0.99)]\n",
    "print(MAX_LENGTH_ZH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in train_en_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(train_en_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in val_zh_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(val_zh_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in val_en_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(val_en_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in test_zh_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(test_zh_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in test_en_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(test_en_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH_ZH = 74\n",
    "MAX_LENGTH_EN = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh_indexes_filtered = []\n",
    "train_en_indexes_filtered = []\n",
    "for i in range(len(train_zh_indexes)):\n",
    "    if len(train_zh_indexes[i]) <= MAX_LENGTH_ZH and len(train_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        train_zh_indexes_filtered.append(train_zh_indexes[i])\n",
    "        train_en_indexes_filtered.append(train_en_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_zh_indexes_filtered = []\n",
    "val_en_indexes_filtered = []\n",
    "for i in range(len(val_zh_indexes)):\n",
    "    if len(val_zh_indexes[i]) <= MAX_LENGTH_ZH and len(val_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        val_zh_indexes_filtered.append(val_zh_indexes[i])\n",
    "        val_en_indexes_filtered.append(val_en_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zh_indexes_filtered = []\n",
    "test_en_indexes_filtered = []\n",
    "for i in range(len(test_zh_indexes)):\n",
    "    if len(test_zh_indexes[i]) <= MAX_LENGTH_ZH and len(test_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        test_zh_indexes_filtered.append(test_zh_indexes[i])\n",
    "        test_en_indexes_filtered.append(test_en_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    def __init__(self, data_list1, data_list2):\n",
    "        \n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        \n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "            \n",
    "    def __getitem__(self, key):        \n",
    "        return [self.data_list1[key], self.data_list2[key], len(self.data_list1[key]), len(self.data_list2[key])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "        \n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_LENGTH_ZH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_LENGTH_EN-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        data_list1.append(padded_vec1[:MAX_LENGTH_ZH])\n",
    "        data_list2.append(padded_vec2[:MAX_LENGTH_EN])\n",
    "\n",
    "\n",
    "    return [torch.from_numpy(np.array(data_list1)).cuda(), torch.from_numpy(np.array(data_list2)).cuda(),\n",
    "                torch.LongTensor(length_list1).cuda(), torch.LongTensor(length_list2).cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = VocabDataset(train_zh_indexes_filtered, train_en_indexes_filtered)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_zh_indexes_filtered, val_en_indexes_filtered)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = VocabDataset(test_zh_indexes_filtered, test_en_indexes_filtered)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_loader2 = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size=EMBED_SIZE):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_zh).float(), freeze=True)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, embed_size=EMBED_SIZE):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_en).float(), freeze=True)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input)        \n",
    "        output = F.relu(output)        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size, input_length = input_tensor.size()\n",
    "    _, target_length = target_tensor.size()\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_token]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "        \n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di].unsqueeze(0) \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "                        \n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=100, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    \n",
    "    plot_losses_t = []\n",
    "    print_loss_total_t = 0  \n",
    "    plot_loss_total_t = 0  \n",
    "    \n",
    "    plot_losses_v = []\n",
    "    print_loss_total_v = 0  \n",
    "    plot_loss_total_v = 0 \n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data1, data2, length1, length2) in enumerate(train_loader):\n",
    "            input_tensor = data1\n",
    "            target_tensor = data2\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total_t += loss\n",
    "            plot_loss_total_t += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total_t / print_every\n",
    "                print_loss_total_t = 0\n",
    "                print('Train %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total_t / plot_every\n",
    "                plot_losses_t.append(plot_loss_avg)\n",
    "                plot_loss_total_t = 0\n",
    "                \n",
    "        for i, (data1, data2, length1, length2) in enumerate(val_loader):\n",
    "            input_tensor = data1\n",
    "            target_tensor = data2\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total_v += loss\n",
    "            plot_loss_total_v += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total_v / print_every\n",
    "                print_loss_total_v = 0\n",
    "                print('Val %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total_v / plot_every\n",
    "                plot_losses_v.append(plot_loss_avg)\n",
    "                plot_loss_total_v = 0\n",
    "                \n",
    "        torch.save(encoder.state_dict(), model_path + \"encoder_rnn_vi\"+str(hidden_size)+str(iter)+\".pth\")\n",
    "        torch.save(decoder.state_dict(), model_path + \"decoder_rnn_vi\"+str(hidden_size)+str(iter)+\".pth\")\n",
    "\n",
    "    return plot_losses_t, plot_losses_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0m 0s (- 0m 1s) (1 12%) 0.1152\n",
      "Train 0m 23s (- 2m 44s) (1 12%) 4.2711\n",
      "Train 0m 46s (- 5m 26s) (1 12%) 3.0601\n",
      "Train 1m 9s (- 8m 6s) (1 12%) 2.9433\n",
      "Train 1m 32s (- 10m 49s) (1 12%) 2.8984\n",
      "Train 1m 55s (- 13m 30s) (1 12%) 2.7955\n",
      "Train 2m 19s (- 16m 13s) (1 12%) 2.7729\n",
      "Train 2m 42s (- 18m 55s) (1 12%) 2.8186\n",
      "Train 3m 5s (- 21m 37s) (1 12%) 2.7191\n",
      "Train 3m 28s (- 24m 20s) (1 12%) 2.7052\n",
      "Train 3m 51s (- 27m 3s) (1 12%) 2.6997\n",
      "Train 4m 15s (- 29m 46s) (1 12%) 2.6596\n",
      "Train 4m 38s (- 32m 29s) (1 12%) 2.6835\n",
      "Train 5m 1s (- 35m 11s) (1 12%) 2.6197\n",
      "Train 5m 24s (- 37m 54s) (1 12%) 2.6206\n",
      "Train 5m 48s (- 40m 36s) (1 12%) 2.6064\n",
      "Train 6m 11s (- 43m 19s) (1 12%) 2.5944\n",
      "Train 6m 34s (- 46m 0s) (1 12%) 2.5607\n",
      "Train 6m 57s (- 48m 43s) (1 12%) 2.6318\n",
      "Train 7m 20s (- 51m 26s) (1 12%) 2.5954\n",
      "Train 7m 44s (- 54m 9s) (1 12%) 2.5433\n",
      "Train 8m 7s (- 56m 51s) (1 12%) 2.4916\n",
      "Train 8m 30s (- 59m 31s) (1 12%) 2.4823\n",
      "Train 8m 53s (- 62m 14s) (1 12%) 2.4884\n",
      "Train 9m 16s (- 64m 56s) (1 12%) 2.5114\n",
      "Train 9m 39s (- 67m 38s) (1 12%) 2.4561\n",
      "Train 10m 2s (- 70m 20s) (1 12%) 2.4637\n",
      "Train 10m 26s (- 73m 2s) (1 12%) 2.4741\n",
      "Train 10m 49s (- 75m 43s) (1 12%) 2.4474\n",
      "Train 11m 12s (- 78m 28s) (1 12%) 2.5368\n",
      "Train 11m 35s (- 81m 11s) (1 12%) 2.4672\n",
      "Train 11m 59s (- 83m 53s) (1 12%) 2.4587\n",
      "Train 12m 22s (- 86m 35s) (1 12%) 2.4277\n",
      "Train 12m 45s (- 89m 16s) (1 12%) 2.4170\n",
      "Train 13m 8s (- 91m 59s) (1 12%) 2.4582\n",
      "Train 13m 31s (- 94m 41s) (1 12%) 2.4927\n",
      "Train 13m 54s (- 97m 24s) (1 12%) 2.4447\n",
      "Train 14m 18s (- 100m 7s) (1 12%) 2.4776\n",
      "Val 14m 26s (- 101m 4s) (1 12%) 0.0214\n",
      "Train 14m 36s (- 43m 48s) (2 25%) 0.8066\n",
      "Train 14m 59s (- 44m 57s) (2 25%) 2.3962\n",
      "Train 15m 22s (- 46m 7s) (2 25%) 2.4067\n",
      "Train 15m 45s (- 47m 17s) (2 25%) 2.3731\n",
      "Train 16m 8s (- 48m 26s) (2 25%) 2.3158\n",
      "Train 16m 31s (- 49m 35s) (2 25%) 2.2930\n",
      "Train 16m 55s (- 50m 45s) (2 25%) 2.3994\n",
      "Train 17m 18s (- 51m 54s) (2 25%) 2.3865\n",
      "Train 17m 41s (- 53m 4s) (2 25%) 2.3751\n",
      "Train 18m 4s (- 54m 13s) (2 25%) 2.3456\n",
      "Train 18m 27s (- 55m 23s) (2 25%) 2.3833\n",
      "Train 18m 50s (- 56m 32s) (2 25%) 2.3800\n",
      "Train 19m 14s (- 57m 43s) (2 25%) 2.4067\n",
      "Train 19m 37s (- 58m 52s) (2 25%) 2.3657\n",
      "Train 20m 0s (- 60m 2s) (2 25%) 2.4210\n",
      "Train 20m 24s (- 61m 12s) (2 25%) 2.3121\n",
      "Train 20m 47s (- 62m 21s) (2 25%) 2.3214\n",
      "Train 21m 10s (- 63m 31s) (2 25%) 2.3977\n",
      "Train 21m 33s (- 64m 41s) (2 25%) 2.3532\n",
      "Train 21m 56s (- 65m 50s) (2 25%) 2.2906\n",
      "Train 22m 19s (- 66m 59s) (2 25%) 2.2652\n",
      "Train 22m 42s (- 68m 8s) (2 25%) 2.3102\n",
      "Train 23m 5s (- 69m 17s) (2 25%) 2.2527\n",
      "Train 23m 29s (- 70m 27s) (2 25%) 2.3637\n",
      "Train 23m 52s (- 71m 37s) (2 25%) 2.3971\n",
      "Train 24m 15s (- 72m 46s) (2 25%) 2.2702\n",
      "Train 24m 38s (- 73m 56s) (2 25%) 2.3651\n",
      "Train 25m 2s (- 75m 6s) (2 25%) 2.3774\n",
      "Train 25m 25s (- 76m 16s) (2 25%) 2.3146\n",
      "Train 25m 48s (- 77m 26s) (2 25%) 2.3313\n",
      "Train 26m 11s (- 78m 35s) (2 25%) 2.2625\n",
      "Train 26m 35s (- 79m 45s) (2 25%) 2.3717\n",
      "Train 26m 58s (- 80m 55s) (2 25%) 2.3269\n",
      "Train 27m 21s (- 82m 5s) (2 25%) 2.3407\n",
      "Train 27m 44s (- 83m 14s) (2 25%) 2.3097\n",
      "Train 28m 7s (- 84m 23s) (2 25%) 2.2459\n",
      "Train 28m 31s (- 85m 33s) (2 25%) 2.2819\n",
      "Train 28m 54s (- 86m 42s) (2 25%) 2.3487\n",
      "Val 29m 2s (- 87m 6s) (2 25%) 0.9057\n",
      "Train 29m 11s (- 48m 39s) (3 37%) 0.7732\n",
      "Train 29m 35s (- 49m 18s) (3 37%) 2.2987\n",
      "Train 29m 58s (- 49m 57s) (3 37%) 2.3124\n",
      "Train 30m 21s (- 50m 36s) (3 37%) 2.2728\n",
      "Train 30m 45s (- 51m 15s) (3 37%) 2.2860\n",
      "Train 31m 8s (- 51m 53s) (3 37%) 2.2825\n",
      "Train 31m 31s (- 52m 33s) (3 37%) 2.3141\n",
      "Train 31m 55s (- 53m 11s) (3 37%) 2.2891\n",
      "Train 32m 18s (- 53m 50s) (3 37%) 2.2858\n",
      "Train 32m 41s (- 54m 29s) (3 37%) 2.2335\n",
      "Train 33m 4s (- 55m 7s) (3 37%) 2.2487\n",
      "Train 33m 27s (- 55m 46s) (3 37%) 2.2760\n",
      "Train 33m 51s (- 56m 25s) (3 37%) 2.2629\n",
      "Train 34m 14s (- 57m 4s) (3 37%) 2.2881\n",
      "Train 34m 37s (- 57m 42s) (3 37%) 2.3187\n",
      "Train 35m 0s (- 58m 21s) (3 37%) 2.2131\n",
      "Train 35m 24s (- 59m 0s) (3 37%) 2.2481\n",
      "Train 35m 47s (- 59m 39s) (3 37%) 2.2607\n",
      "Train 36m 10s (- 60m 17s) (3 37%) 2.2008\n",
      "Train 36m 33s (- 60m 55s) (3 37%) 2.2491\n",
      "Train 36m 57s (- 61m 35s) (3 37%) 2.2962\n",
      "Train 37m 20s (- 62m 13s) (3 37%) 2.2259\n",
      "Train 37m 43s (- 62m 52s) (3 37%) 2.1756\n",
      "Train 38m 6s (- 63m 30s) (3 37%) 2.2670\n",
      "Train 38m 29s (- 64m 9s) (3 37%) 2.2037\n",
      "Train 38m 52s (- 64m 48s) (3 37%) 2.2687\n",
      "Train 39m 16s (- 65m 26s) (3 37%) 2.2681\n",
      "Train 39m 39s (- 66m 5s) (3 37%) 2.2119\n",
      "Train 40m 2s (- 66m 44s) (3 37%) 2.2499\n",
      "Train 40m 25s (- 67m 22s) (3 37%) 2.2786\n",
      "Train 40m 49s (- 68m 1s) (3 37%) 2.2680\n",
      "Train 41m 12s (- 68m 40s) (3 37%) 2.1996\n",
      "Train 41m 35s (- 69m 18s) (3 37%) 2.1872\n",
      "Train 41m 58s (- 69m 57s) (3 37%) 2.2019\n",
      "Train 42m 21s (- 70m 36s) (3 37%) 2.2612\n",
      "Train 42m 44s (- 71m 14s) (3 37%) 2.2097\n",
      "Train 43m 7s (- 71m 52s) (3 37%) 2.2460\n",
      "Train 43m 30s (- 72m 31s) (3 37%) 2.2064\n",
      "Val 43m 38s (- 72m 44s) (3 37%) 0.8832\n",
      "Train 43m 48s (- 43m 48s) (4 50%) 0.7657\n",
      "Train 44m 11s (- 44m 11s) (4 50%) 2.1688\n",
      "Train 44m 34s (- 44m 34s) (4 50%) 2.1419\n",
      "Train 44m 57s (- 44m 57s) (4 50%) 2.1395\n",
      "Train 45m 20s (- 45m 20s) (4 50%) 2.1596\n",
      "Train 45m 44s (- 45m 44s) (4 50%) 2.1346\n",
      "Train 46m 7s (- 46m 7s) (4 50%) 2.2185\n",
      "Train 46m 30s (- 46m 30s) (4 50%) 2.1729\n",
      "Train 46m 53s (- 46m 53s) (4 50%) 2.1659\n",
      "Train 47m 16s (- 47m 16s) (4 50%) 2.1427\n",
      "Train 47m 39s (- 47m 39s) (4 50%) 2.1788\n",
      "Train 48m 2s (- 48m 2s) (4 50%) 2.1259\n",
      "Train 48m 26s (- 48m 26s) (4 50%) 2.2061\n",
      "Train 48m 49s (- 48m 49s) (4 50%) 2.1931\n",
      "Train 49m 12s (- 49m 12s) (4 50%) 2.1395\n",
      "Train 49m 35s (- 49m 35s) (4 50%) 2.2383\n",
      "Train 49m 59s (- 49m 59s) (4 50%) 2.2063\n",
      "Train 50m 22s (- 50m 22s) (4 50%) 2.1187\n",
      "Train 50m 45s (- 50m 45s) (4 50%) 2.1476\n",
      "Train 51m 8s (- 51m 8s) (4 50%) 2.2085\n",
      "Train 51m 31s (- 51m 31s) (4 50%) 2.1420\n",
      "Train 51m 54s (- 51m 54s) (4 50%) 2.1888\n",
      "Train 52m 18s (- 52m 18s) (4 50%) 2.2180\n",
      "Train 52m 41s (- 52m 41s) (4 50%) 2.2007\n",
      "Train 53m 4s (- 53m 4s) (4 50%) 2.1838\n",
      "Train 53m 27s (- 53m 27s) (4 50%) 2.2389\n",
      "Train 53m 50s (- 53m 50s) (4 50%) 2.1512\n",
      "Train 54m 14s (- 54m 14s) (4 50%) 2.1615\n",
      "Train 54m 37s (- 54m 37s) (4 50%) 2.1272\n",
      "Train 55m 0s (- 55m 0s) (4 50%) 2.1843\n",
      "Train 55m 23s (- 55m 23s) (4 50%) 2.1329\n",
      "Train 55m 46s (- 55m 46s) (4 50%) 2.1300\n",
      "Train 56m 9s (- 56m 9s) (4 50%) 2.1295\n",
      "Train 56m 32s (- 56m 32s) (4 50%) 2.1862\n",
      "Train 56m 55s (- 56m 55s) (4 50%) 2.2028\n",
      "Train 57m 19s (- 57m 19s) (4 50%) 2.2302\n",
      "Train 57m 42s (- 57m 42s) (4 50%) 2.1554\n",
      "Train 58m 5s (- 58m 5s) (4 50%) 2.1622\n",
      "Val 58m 13s (- 58m 13s) (4 50%) 0.8155\n",
      "Train 58m 23s (- 35m 1s) (5 62%) 0.7230\n",
      "Train 58m 46s (- 35m 15s) (5 62%) 2.0878\n",
      "Train 59m 9s (- 35m 29s) (5 62%) 2.0697\n",
      "Train 59m 32s (- 35m 43s) (5 62%) 2.0949\n",
      "Train 59m 55s (- 35m 57s) (5 62%) 2.0801\n",
      "Train 60m 19s (- 36m 11s) (5 62%) 2.1899\n",
      "Train 60m 42s (- 36m 25s) (5 62%) 2.0951\n",
      "Train 61m 5s (- 36m 39s) (5 62%) 2.1112\n",
      "Train 61m 28s (- 36m 53s) (5 62%) 2.1104\n",
      "Train 61m 51s (- 37m 7s) (5 62%) 2.1015\n",
      "Train 62m 15s (- 37m 21s) (5 62%) 2.2069\n",
      "Train 62m 38s (- 37m 35s) (5 62%) 2.1437\n",
      "Train 63m 1s (- 37m 49s) (5 62%) 2.1670\n",
      "Train 63m 25s (- 38m 3s) (5 62%) 2.1756\n",
      "Train 63m 48s (- 38m 16s) (5 62%) 2.0944\n",
      "Train 64m 11s (- 38m 30s) (5 62%) 2.1632\n",
      "Train 64m 34s (- 38m 44s) (5 62%) 2.1239\n",
      "Train 64m 57s (- 38m 58s) (5 62%) 2.0897\n",
      "Train 65m 21s (- 39m 12s) (5 62%) 2.1243\n",
      "Train 65m 44s (- 39m 26s) (5 62%) 2.1087\n",
      "Train 66m 7s (- 39m 40s) (5 62%) 2.1366\n",
      "Train 66m 30s (- 39m 54s) (5 62%) 2.1359\n",
      "Train 66m 54s (- 40m 8s) (5 62%) 2.1751\n",
      "Train 67m 17s (- 40m 22s) (5 62%) 2.1078\n",
      "Train 67m 40s (- 40m 36s) (5 62%) 2.1249\n",
      "Train 68m 3s (- 40m 50s) (5 62%) 2.1406\n",
      "Train 68m 26s (- 41m 4s) (5 62%) 2.1635\n",
      "Train 68m 50s (- 41m 18s) (5 62%) 2.1770\n",
      "Train 69m 13s (- 41m 31s) (5 62%) 2.1155\n",
      "Train 69m 36s (- 41m 45s) (5 62%) 2.1376\n",
      "Train 69m 59s (- 41m 59s) (5 62%) 2.1287\n",
      "Train 70m 22s (- 42m 13s) (5 62%) 2.1327\n",
      "Train 70m 46s (- 42m 27s) (5 62%) 2.1713\n",
      "Train 71m 9s (- 42m 41s) (5 62%) 2.1710\n",
      "Train 71m 32s (- 42m 55s) (5 62%) 2.1546\n",
      "Train 71m 55s (- 43m 9s) (5 62%) 2.1024\n",
      "Train 72m 18s (- 43m 23s) (5 62%) 2.0944\n",
      "Train 72m 41s (- 43m 37s) (5 62%) 2.0529\n",
      "Val 72m 49s (- 43m 41s) (5 62%) 0.7544\n",
      "Train 72m 59s (- 24m 19s) (6 75%) 0.7154\n",
      "Train 73m 22s (- 24m 27s) (6 75%) 2.1121\n",
      "Train 73m 45s (- 24m 35s) (6 75%) 2.0715\n",
      "Train 74m 8s (- 24m 42s) (6 75%) 1.9990\n",
      "Train 74m 31s (- 24m 50s) (6 75%) 2.0460\n",
      "Train 74m 55s (- 24m 58s) (6 75%) 2.1432\n",
      "Train 75m 18s (- 25m 6s) (6 75%) 2.0133\n",
      "Train 75m 41s (- 25m 13s) (6 75%) 2.1589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 76m 5s (- 25m 21s) (6 75%) 2.0720\n",
      "Train 76m 28s (- 25m 29s) (6 75%) 2.0964\n",
      "Train 76m 51s (- 25m 37s) (6 75%) 2.0872\n",
      "Train 77m 14s (- 25m 44s) (6 75%) 2.0781\n",
      "Train 77m 37s (- 25m 52s) (6 75%) 2.0175\n",
      "Train 78m 1s (- 26m 0s) (6 75%) 2.1078\n",
      "Train 78m 24s (- 26m 8s) (6 75%) 2.0770\n",
      "Train 78m 47s (- 26m 15s) (6 75%) 2.1405\n",
      "Train 79m 10s (- 26m 23s) (6 75%) 2.0168\n",
      "Train 79m 33s (- 26m 31s) (6 75%) 2.0141\n",
      "Train 79m 57s (- 26m 39s) (6 75%) 2.0717\n",
      "Train 80m 20s (- 26m 46s) (6 75%) 2.0565\n",
      "Train 80m 43s (- 26m 54s) (6 75%) 2.0573\n",
      "Train 81m 6s (- 27m 2s) (6 75%) 2.1475\n",
      "Train 81m 29s (- 27m 9s) (6 75%) 2.1027\n",
      "Train 81m 52s (- 27m 17s) (6 75%) 2.0725\n",
      "Train 82m 16s (- 27m 25s) (6 75%) 2.0610\n",
      "Train 82m 39s (- 27m 33s) (6 75%) 2.0887\n",
      "Train 83m 2s (- 27m 40s) (6 75%) 2.0542\n",
      "Train 83m 25s (- 27m 48s) (6 75%) 2.0959\n",
      "Train 83m 49s (- 27m 56s) (6 75%) 2.0902\n",
      "Train 84m 12s (- 28m 4s) (6 75%) 2.0471\n",
      "Train 84m 35s (- 28m 11s) (6 75%) 2.0896\n",
      "Train 84m 58s (- 28m 19s) (6 75%) 2.0791\n",
      "Train 85m 21s (- 28m 27s) (6 75%) 2.0697\n",
      "Train 85m 44s (- 28m 34s) (6 75%) 2.1154\n",
      "Train 86m 8s (- 28m 42s) (6 75%) 2.0806\n",
      "Train 86m 31s (- 28m 50s) (6 75%) 2.1125\n",
      "Train 86m 54s (- 28m 58s) (6 75%) 2.0535\n",
      "Train 87m 17s (- 29m 5s) (6 75%) 2.0353\n",
      "Val 87m 25s (- 29m 8s) (6 75%) 0.7851\n",
      "Train 87m 35s (- 12m 30s) (7 87%) 0.7305\n",
      "Train 87m 58s (- 12m 34s) (7 87%) 1.9740\n",
      "Train 88m 21s (- 12m 37s) (7 87%) 2.1231\n",
      "Train 88m 45s (- 12m 40s) (7 87%) 2.1117\n",
      "Train 89m 7s (- 12m 43s) (7 87%) 1.9830\n",
      "Train 89m 31s (- 12m 47s) (7 87%) 2.0554\n",
      "Train 89m 54s (- 12m 50s) (7 87%) 1.9776\n",
      "Train 90m 17s (- 12m 53s) (7 87%) 2.0586\n",
      "Train 90m 40s (- 12m 57s) (7 87%) 1.9428\n",
      "Train 91m 3s (- 13m 0s) (7 87%) 2.0524\n",
      "Train 91m 27s (- 13m 3s) (7 87%) 2.0026\n",
      "Train 91m 50s (- 13m 7s) (7 87%) 1.9873\n",
      "Train 92m 13s (- 13m 10s) (7 87%) 2.0405\n",
      "Train 92m 36s (- 13m 13s) (7 87%) 2.0404\n",
      "Train 92m 59s (- 13m 17s) (7 87%) 1.9792\n",
      "Train 93m 22s (- 13m 20s) (7 87%) 2.0416\n",
      "Train 93m 45s (- 13m 23s) (7 87%) 2.0429\n",
      "Train 94m 9s (- 13m 27s) (7 87%) 2.0613\n",
      "Train 94m 32s (- 13m 30s) (7 87%) 2.0459\n",
      "Train 94m 55s (- 13m 33s) (7 87%) 2.1229\n",
      "Train 95m 18s (- 13m 36s) (7 87%) 2.0420\n",
      "Train 95m 42s (- 13m 40s) (7 87%) 2.0682\n",
      "Train 96m 5s (- 13m 43s) (7 87%) 2.0014\n",
      "Train 96m 28s (- 13m 46s) (7 87%) 2.0107\n",
      "Train 96m 51s (- 13m 50s) (7 87%) 2.0067\n",
      "Train 97m 14s (- 13m 53s) (7 87%) 2.0121\n",
      "Train 97m 37s (- 13m 56s) (7 87%) 1.9853\n",
      "Train 98m 0s (- 14m 0s) (7 87%) 2.0490\n",
      "Train 98m 23s (- 14m 3s) (7 87%) 2.0668\n",
      "Train 98m 47s (- 14m 6s) (7 87%) 1.9836\n",
      "Train 99m 10s (- 14m 10s) (7 87%) 2.0861\n",
      "Train 99m 33s (- 14m 13s) (7 87%) 2.0170\n",
      "Train 99m 56s (- 14m 16s) (7 87%) 2.0682\n",
      "Train 100m 19s (- 14m 19s) (7 87%) 2.0107\n",
      "Train 100m 42s (- 14m 23s) (7 87%) 2.0178\n",
      "Train 101m 6s (- 14m 26s) (7 87%) 2.0945\n",
      "Train 101m 29s (- 14m 29s) (7 87%) 2.0084\n",
      "Train 101m 52s (- 14m 33s) (7 87%) 2.0101\n",
      "Val 102m 0s (- 14m 34s) (7 87%) 0.7423\n",
      "Train 102m 10s (- 0m 0s) (8 100%) 0.7097\n",
      "Train 102m 33s (- 0m 0s) (8 100%) 1.9899\n",
      "Train 102m 56s (- 0m 0s) (8 100%) 1.9685\n",
      "Train 103m 19s (- 0m 0s) (8 100%) 1.9515\n",
      "Train 103m 42s (- 0m 0s) (8 100%) 1.9725\n",
      "Train 104m 6s (- 0m 0s) (8 100%) 1.9728\n",
      "Train 104m 29s (- 0m 0s) (8 100%) 1.9860\n",
      "Train 104m 52s (- 0m 0s) (8 100%) 2.0597\n",
      "Train 105m 15s (- 0m 0s) (8 100%) 2.0592\n",
      "Train 105m 38s (- 0m 0s) (8 100%) 1.9414\n",
      "Train 106m 2s (- 0m 0s) (8 100%) 2.0200\n",
      "Train 106m 25s (- 0m 0s) (8 100%) 1.9493\n",
      "Train 106m 48s (- 0m 0s) (8 100%) 1.9821\n",
      "Train 107m 11s (- 0m 0s) (8 100%) 2.0670\n",
      "Train 107m 34s (- 0m 0s) (8 100%) 2.0230\n",
      "Train 107m 58s (- 0m 0s) (8 100%) 2.0626\n",
      "Train 108m 21s (- 0m 0s) (8 100%) 1.9867\n",
      "Train 108m 44s (- 0m 0s) (8 100%) 2.0507\n",
      "Train 109m 7s (- 0m 0s) (8 100%) 2.0158\n",
      "Train 109m 30s (- 0m 0s) (8 100%) 1.9920\n",
      "Train 109m 54s (- 0m 0s) (8 100%) 1.9680\n",
      "Train 110m 17s (- 0m 0s) (8 100%) 1.9696\n",
      "Train 110m 40s (- 0m 0s) (8 100%) 2.0273\n",
      "Train 111m 3s (- 0m 0s) (8 100%) 2.0067\n",
      "Train 111m 26s (- 0m 0s) (8 100%) 1.9562\n",
      "Train 111m 50s (- 0m 0s) (8 100%) 2.0110\n",
      "Train 112m 13s (- 0m 0s) (8 100%) 1.9886\n",
      "Train 112m 36s (- 0m 0s) (8 100%) 2.0176\n",
      "Train 112m 59s (- 0m 0s) (8 100%) 2.0440\n",
      "Train 113m 22s (- 0m 0s) (8 100%) 1.9848\n",
      "Train 113m 45s (- 0m 0s) (8 100%) 1.9840\n",
      "Train 114m 8s (- 0m 0s) (8 100%) 2.0438\n",
      "Train 114m 32s (- 0m 0s) (8 100%) 2.0583\n",
      "Train 114m 55s (- 0m 0s) (8 100%) 2.0699\n",
      "Train 115m 18s (- 0m 0s) (8 100%) 2.0224\n",
      "Train 115m 41s (- 0m 0s) (8 100%) 2.0000\n",
      "Train 116m 5s (- 0m 0s) (8 100%) 1.9901\n",
      "Train 116m 28s (- 0m 0s) (8 100%) 2.0444\n",
      "Val 116m 36s (- 0m 0s) (8 100%) 0.7484\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "plotloss_t1, plotloss_v1 = trainIters(encoder1, decoder1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0m 0s (- 0m 2s) (1 12%) 0.1152\n",
      "Train 0m 31s (- 3m 43s) (1 12%) 3.9394\n",
      "Train 1m 3s (- 7m 24s) (1 12%) 3.0295\n",
      "Train 1m 35s (- 11m 5s) (1 12%) 2.9601\n",
      "Train 2m 6s (- 14m 47s) (1 12%) 2.8919\n",
      "Train 2m 38s (- 18m 28s) (1 12%) 2.8505\n",
      "Train 3m 9s (- 22m 9s) (1 12%) 2.8239\n",
      "Train 3m 41s (- 25m 50s) (1 12%) 2.7827\n",
      "Train 4m 12s (- 29m 30s) (1 12%) 2.6437\n",
      "Train 4m 44s (- 33m 12s) (1 12%) 2.7586\n",
      "Train 5m 16s (- 36m 52s) (1 12%) 2.6074\n",
      "Train 5m 47s (- 40m 34s) (1 12%) 2.7056\n",
      "Train 6m 19s (- 44m 14s) (1 12%) 2.6324\n",
      "Train 6m 50s (- 47m 55s) (1 12%) 2.6892\n",
      "Train 7m 22s (- 51m 36s) (1 12%) 2.5968\n",
      "Train 7m 54s (- 55m 18s) (1 12%) 2.6748\n",
      "Train 8m 25s (- 59m 1s) (1 12%) 2.6606\n",
      "Train 8m 57s (- 62m 42s) (1 12%) 2.5748\n",
      "Train 9m 28s (- 66m 22s) (1 12%) 2.5242\n",
      "Train 10m 0s (- 70m 2s) (1 12%) 2.5468\n",
      "Train 10m 32s (- 73m 44s) (1 12%) 2.5934\n",
      "Train 11m 3s (- 77m 26s) (1 12%) 2.5389\n",
      "Train 11m 35s (- 81m 8s) (1 12%) 2.6184\n",
      "Train 12m 7s (- 84m 49s) (1 12%) 2.5038\n",
      "Train 12m 38s (- 88m 31s) (1 12%) 2.5652\n",
      "Train 13m 10s (- 92m 11s) (1 12%) 2.5583\n",
      "Train 13m 41s (- 95m 52s) (1 12%) 2.4543\n",
      "Train 14m 13s (- 99m 33s) (1 12%) 2.4614\n",
      "Train 14m 44s (- 103m 14s) (1 12%) 2.5470\n",
      "Train 15m 16s (- 106m 55s) (1 12%) 2.4562\n",
      "Train 15m 48s (- 110m 37s) (1 12%) 2.4887\n",
      "Train 16m 19s (- 114m 18s) (1 12%) 2.4861\n",
      "Train 16m 51s (- 117m 58s) (1 12%) 2.4476\n",
      "Train 17m 22s (- 121m 39s) (1 12%) 2.4817\n",
      "Train 17m 54s (- 125m 20s) (1 12%) 2.4689\n",
      "Train 18m 26s (- 129m 2s) (1 12%) 2.4668\n",
      "Train 18m 57s (- 132m 43s) (1 12%) 2.4356\n",
      "Train 19m 29s (- 136m 25s) (1 12%) 2.5298\n",
      "Val 19m 40s (- 137m 43s) (1 12%) 0.0201\n",
      "Train 19m 52s (- 59m 37s) (2 25%) 0.8820\n",
      "Train 20m 24s (- 61m 12s) (2 25%) 2.4193\n",
      "Train 20m 55s (- 62m 47s) (2 25%) 2.4328\n",
      "Train 21m 27s (- 64m 21s) (2 25%) 2.4029\n",
      "Train 21m 58s (- 65m 56s) (2 25%) 2.3901\n",
      "Train 22m 30s (- 67m 31s) (2 25%) 2.4336\n",
      "Train 23m 2s (- 69m 6s) (2 25%) 2.4105\n",
      "Train 23m 33s (- 70m 41s) (2 25%) 2.4191\n",
      "Train 24m 5s (- 72m 15s) (2 25%) 2.2787\n",
      "Train 24m 36s (- 73m 49s) (2 25%) 2.3473\n",
      "Train 25m 7s (- 75m 23s) (2 25%) 2.3184\n",
      "Train 25m 39s (- 76m 58s) (2 25%) 2.3548\n",
      "Train 26m 10s (- 78m 32s) (2 25%) 2.3656\n",
      "Train 26m 42s (- 80m 7s) (2 25%) 2.3605\n",
      "Train 27m 13s (- 81m 41s) (2 25%) 2.4058\n",
      "Train 27m 45s (- 83m 16s) (2 25%) 2.3459\n",
      "Train 28m 17s (- 84m 51s) (2 25%) 2.3933\n",
      "Train 28m 48s (- 86m 25s) (2 25%) 2.2823\n",
      "Train 29m 20s (- 88m 0s) (2 25%) 2.3476\n",
      "Train 29m 51s (- 89m 35s) (2 25%) 2.2969\n",
      "Train 30m 23s (- 91m 9s) (2 25%) 2.2888\n",
      "Train 30m 54s (- 92m 43s) (2 25%) 2.2882\n",
      "Train 31m 26s (- 94m 18s) (2 25%) 2.3493\n",
      "Train 31m 57s (- 95m 53s) (2 25%) 2.3690\n",
      "Train 32m 29s (- 97m 27s) (2 25%) 2.3353\n",
      "Train 33m 0s (- 99m 2s) (2 25%) 2.3792\n",
      "Train 33m 32s (- 100m 37s) (2 25%) 2.3681\n",
      "Train 34m 3s (- 102m 11s) (2 25%) 2.2461\n",
      "Train 34m 35s (- 103m 46s) (2 25%) 2.3175\n",
      "Train 35m 7s (- 105m 21s) (2 25%) 2.3200\n",
      "Train 35m 38s (- 106m 55s) (2 25%) 2.2582\n",
      "Train 36m 10s (- 108m 30s) (2 25%) 2.2985\n",
      "Train 36m 41s (- 110m 5s) (2 25%) 2.3256\n",
      "Train 37m 13s (- 111m 40s) (2 25%) 2.3720\n",
      "Train 37m 45s (- 113m 15s) (2 25%) 2.3498\n",
      "Train 38m 16s (- 114m 50s) (2 25%) 2.3525\n",
      "Train 38m 48s (- 116m 24s) (2 25%) 2.2770\n",
      "Train 39m 19s (- 117m 59s) (2 25%) 2.3652\n",
      "Val 39m 30s (- 118m 32s) (2 25%) 0.9387\n",
      "Train 39m 42s (- 66m 11s) (3 37%) 0.8033\n",
      "Train 40m 14s (- 67m 3s) (3 37%) 2.2151\n",
      "Train 40m 45s (- 67m 56s) (3 37%) 2.2365\n",
      "Train 41m 17s (- 68m 49s) (3 37%) 2.2467\n",
      "Train 41m 49s (- 69m 41s) (3 37%) 2.2711\n",
      "Train 42m 20s (- 70m 34s) (3 37%) 2.2639\n",
      "Train 42m 52s (- 71m 26s) (3 37%) 2.2790\n",
      "Train 43m 23s (- 72m 19s) (3 37%) 2.2817\n",
      "Train 43m 55s (- 73m 12s) (3 37%) 2.2669\n",
      "Train 44m 26s (- 74m 4s) (3 37%) 2.2495\n",
      "Train 44m 58s (- 74m 57s) (3 37%) 2.2732\n",
      "Train 45m 29s (- 75m 49s) (3 37%) 2.2231\n",
      "Train 46m 1s (- 76m 42s) (3 37%) 2.2686\n",
      "Train 46m 32s (- 77m 34s) (3 37%) 2.2428\n",
      "Train 47m 4s (- 78m 27s) (3 37%) 2.2838\n",
      "Train 47m 35s (- 79m 19s) (3 37%) 2.2126\n",
      "Train 48m 7s (- 80m 12s) (3 37%) 2.2815\n",
      "Train 48m 39s (- 81m 5s) (3 37%) 2.2162\n",
      "Train 49m 10s (- 81m 57s) (3 37%) 2.2210\n",
      "Train 49m 41s (- 82m 49s) (3 37%) 2.2257\n",
      "Train 50m 13s (- 83m 42s) (3 37%) 2.2170\n",
      "Train 50m 45s (- 84m 35s) (3 37%) 2.2813\n",
      "Train 51m 16s (- 85m 27s) (3 37%) 2.1601\n",
      "Train 51m 48s (- 86m 20s) (3 37%) 2.2225\n",
      "Train 52m 19s (- 87m 12s) (3 37%) 2.1488\n",
      "Train 52m 51s (- 88m 5s) (3 37%) 2.2235\n",
      "Train 53m 22s (- 88m 57s) (3 37%) 2.2207\n",
      "Train 53m 54s (- 89m 50s) (3 37%) 2.2347\n",
      "Train 54m 25s (- 90m 43s) (3 37%) 2.2794\n",
      "Train 54m 57s (- 91m 35s) (3 37%) 2.2047\n",
      "Train 55m 28s (- 92m 28s) (3 37%) 2.2519\n",
      "Train 56m 0s (- 93m 20s) (3 37%) 2.2194\n",
      "Train 56m 31s (- 94m 13s) (3 37%) 2.2575\n",
      "Train 57m 3s (- 95m 5s) (3 37%) 2.1393\n",
      "Train 57m 34s (- 95m 57s) (3 37%) 2.1685\n",
      "Train 58m 6s (- 96m 50s) (3 37%) 2.2197\n",
      "Train 58m 37s (- 97m 43s) (3 37%) 2.2373\n",
      "Train 59m 9s (- 98m 35s) (3 37%) 2.2075\n",
      "Val 59m 20s (- 98m 54s) (3 37%) 0.8274\n",
      "Train 59m 32s (- 59m 32s) (4 50%) 0.7580\n",
      "Train 60m 3s (- 60m 3s) (4 50%) 2.1125\n",
      "Train 60m 35s (- 60m 35s) (4 50%) 2.1501\n",
      "Train 61m 7s (- 61m 7s) (4 50%) 2.1956\n",
      "Train 61m 38s (- 61m 38s) (4 50%) 2.1787\n",
      "Train 62m 10s (- 62m 10s) (4 50%) 2.1318\n",
      "Train 62m 41s (- 62m 41s) (4 50%) 2.1062\n",
      "Train 63m 13s (- 63m 13s) (4 50%) 2.1868\n",
      "Train 63m 45s (- 63m 45s) (4 50%) 2.1676\n",
      "Train 64m 16s (- 64m 16s) (4 50%) 2.1463\n",
      "Train 64m 47s (- 64m 47s) (4 50%) 2.1215\n",
      "Train 65m 19s (- 65m 19s) (4 50%) 2.1499\n",
      "Train 65m 51s (- 65m 51s) (4 50%) 2.1920\n",
      "Train 66m 22s (- 66m 22s) (4 50%) 2.1493\n",
      "Train 66m 53s (- 66m 53s) (4 50%) 2.0919\n",
      "Train 67m 25s (- 67m 25s) (4 50%) 2.1827\n",
      "Train 67m 57s (- 67m 57s) (4 50%) 2.2075\n",
      "Train 68m 28s (- 68m 28s) (4 50%) 2.1462\n",
      "Train 69m 0s (- 69m 0s) (4 50%) 2.2195\n",
      "Train 69m 31s (- 69m 31s) (4 50%) 2.0736\n",
      "Train 70m 3s (- 70m 3s) (4 50%) 2.1439\n",
      "Train 70m 35s (- 70m 35s) (4 50%) 2.1538\n",
      "Train 71m 6s (- 71m 6s) (4 50%) 2.1760\n",
      "Train 71m 37s (- 71m 37s) (4 50%) 2.1238\n",
      "Train 72m 9s (- 72m 9s) (4 50%) 2.1854\n",
      "Train 72m 41s (- 72m 41s) (4 50%) 2.1664\n",
      "Train 73m 12s (- 73m 12s) (4 50%) 2.2092\n",
      "Train 73m 44s (- 73m 44s) (4 50%) 2.1937\n",
      "Train 74m 16s (- 74m 16s) (4 50%) 2.1739\n",
      "Train 74m 47s (- 74m 47s) (4 50%) 2.1220\n",
      "Train 75m 19s (- 75m 19s) (4 50%) 2.1586\n",
      "Train 75m 50s (- 75m 50s) (4 50%) 2.1656\n",
      "Train 76m 22s (- 76m 22s) (4 50%) 2.1525\n",
      "Train 76m 53s (- 76m 53s) (4 50%) 2.1473\n",
      "Train 77m 25s (- 77m 25s) (4 50%) 2.1960\n",
      "Train 77m 57s (- 77m 57s) (4 50%) 2.1280\n",
      "Train 78m 28s (- 78m 28s) (4 50%) 2.2693\n",
      "Train 79m 0s (- 79m 0s) (4 50%) 2.2186\n",
      "Val 79m 11s (- 79m 11s) (4 50%) 0.8553\n",
      "Train 79m 23s (- 47m 38s) (5 62%) 0.7368\n",
      "Train 79m 54s (- 47m 56s) (5 62%) 2.0540\n",
      "Train 80m 26s (- 48m 15s) (5 62%) 2.0445\n",
      "Train 80m 57s (- 48m 34s) (5 62%) 2.0743\n",
      "Train 81m 29s (- 48m 53s) (5 62%) 2.0983\n",
      "Train 82m 1s (- 49m 12s) (5 62%) 2.0651\n",
      "Train 82m 32s (- 49m 31s) (5 62%) 2.0697\n",
      "Train 83m 4s (- 49m 50s) (5 62%) 2.0351\n",
      "Train 83m 35s (- 50m 9s) (5 62%) 2.0686\n",
      "Train 84m 7s (- 50m 28s) (5 62%) 2.0574\n",
      "Train 84m 38s (- 50m 47s) (5 62%) 2.0147\n",
      "Train 85m 10s (- 51m 6s) (5 62%) 2.1007\n",
      "Train 85m 41s (- 51m 25s) (5 62%) 2.0743\n",
      "Train 86m 13s (- 51m 44s) (5 62%) 2.0584\n",
      "Train 86m 45s (- 52m 3s) (5 62%) 2.1856\n",
      "Train 87m 16s (- 52m 21s) (5 62%) 2.0908\n",
      "Train 87m 48s (- 52m 40s) (5 62%) 2.0454\n",
      "Train 88m 19s (- 52m 59s) (5 62%) 2.1747\n",
      "Train 88m 51s (- 53m 18s) (5 62%) 2.1319\n",
      "Train 89m 23s (- 53m 37s) (5 62%) 2.1088\n",
      "Train 89m 54s (- 53m 56s) (5 62%) 2.0649\n",
      "Train 90m 25s (- 54m 15s) (5 62%) 2.0170\n",
      "Train 90m 57s (- 54m 34s) (5 62%) 2.0998\n",
      "Train 91m 29s (- 54m 53s) (5 62%) 2.0762\n",
      "Train 92m 0s (- 55m 12s) (5 62%) 2.0767\n",
      "Train 92m 32s (- 55m 31s) (5 62%) 2.1409\n",
      "Train 93m 3s (- 55m 50s) (5 62%) 2.0898\n",
      "Train 93m 35s (- 56m 9s) (5 62%) 2.0859\n",
      "Train 94m 6s (- 56m 28s) (5 62%) 2.1114\n",
      "Train 94m 38s (- 56m 46s) (5 62%) 2.0763\n",
      "Train 95m 9s (- 57m 5s) (5 62%) 2.1438\n",
      "Train 95m 41s (- 57m 24s) (5 62%) 2.1308\n",
      "Train 96m 13s (- 57m 43s) (5 62%) 2.0942\n",
      "Train 96m 44s (- 58m 2s) (5 62%) 2.0649\n",
      "Train 97m 16s (- 58m 21s) (5 62%) 2.1432\n",
      "Train 97m 47s (- 58m 40s) (5 62%) 2.0043\n",
      "Train 98m 19s (- 58m 59s) (5 62%) 2.1121\n",
      "Train 98m 50s (- 59m 18s) (5 62%) 2.0578\n",
      "Val 99m 1s (- 59m 24s) (5 62%) 0.7772\n",
      "Train 99m 13s (- 33m 4s) (6 75%) 0.7660\n",
      "Train 99m 45s (- 33m 15s) (6 75%) 1.9818\n",
      "Train 100m 16s (- 33m 25s) (6 75%) 2.0030\n",
      "Train 100m 48s (- 33m 36s) (6 75%) 2.0441\n",
      "Train 101m 19s (- 33m 46s) (6 75%) 1.9965\n",
      "Train 101m 51s (- 33m 57s) (6 75%) 2.0785\n",
      "Train 102m 23s (- 34m 7s) (6 75%) 2.0697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 102m 54s (- 34m 18s) (6 75%) 2.0337\n",
      "Train 103m 26s (- 34m 28s) (6 75%) 2.0574\n",
      "Train 103m 58s (- 34m 39s) (6 75%) 2.0058\n",
      "Train 104m 29s (- 34m 49s) (6 75%) 2.0270\n",
      "Train 105m 0s (- 35m 0s) (6 75%) 1.9555\n",
      "Train 105m 32s (- 35m 10s) (6 75%) 2.0961\n",
      "Train 106m 4s (- 35m 21s) (6 75%) 2.0446\n",
      "Train 106m 35s (- 35m 31s) (6 75%) 2.0102\n",
      "Train 107m 7s (- 35m 42s) (6 75%) 1.9834\n",
      "Train 107m 38s (- 35m 52s) (6 75%) 2.0136\n",
      "Train 108m 10s (- 36m 3s) (6 75%) 2.0164\n",
      "Train 108m 42s (- 36m 14s) (6 75%) 2.1412\n",
      "Train 109m 13s (- 36m 24s) (6 75%) 2.0226\n",
      "Train 109m 45s (- 36m 35s) (6 75%) 2.0583\n",
      "Train 110m 16s (- 36m 45s) (6 75%) 2.0187\n",
      "Train 110m 48s (- 36m 56s) (6 75%) 2.0442\n",
      "Train 111m 19s (- 37m 6s) (6 75%) 2.0345\n",
      "Train 111m 51s (- 37m 17s) (6 75%) 2.1157\n",
      "Train 112m 22s (- 37m 27s) (6 75%) 2.0506\n",
      "Train 112m 54s (- 37m 38s) (6 75%) 2.0149\n",
      "Train 113m 26s (- 37m 48s) (6 75%) 2.0951\n",
      "Train 113m 57s (- 37m 59s) (6 75%) 2.0859\n",
      "Train 114m 29s (- 38m 9s) (6 75%) 2.0463\n",
      "Train 115m 0s (- 38m 20s) (6 75%) 2.1172\n",
      "Train 115m 32s (- 38m 30s) (6 75%) 2.0166\n",
      "Train 116m 3s (- 38m 41s) (6 75%) 2.0217\n",
      "Train 116m 35s (- 38m 51s) (6 75%) 2.1129\n",
      "Train 117m 7s (- 39m 2s) (6 75%) 2.0322\n",
      "Train 117m 38s (- 39m 12s) (6 75%) 2.0208\n",
      "Train 118m 10s (- 39m 23s) (6 75%) 2.0157\n",
      "Train 118m 41s (- 39m 33s) (6 75%) 2.1202\n",
      "Val 118m 52s (- 39m 37s) (6 75%) 0.7295\n",
      "Train 119m 4s (- 17m 0s) (7 87%) 0.6589\n",
      "Train 119m 36s (- 17m 5s) (7 87%) 2.0261\n",
      "Train 120m 8s (- 17m 9s) (7 87%) 1.9314\n",
      "Train 120m 39s (- 17m 14s) (7 87%) 1.9336\n",
      "Train 121m 10s (- 17m 18s) (7 87%) 1.9208\n",
      "Train 121m 42s (- 17m 23s) (7 87%) 2.0751\n",
      "Train 122m 14s (- 17m 27s) (7 87%) 1.9307\n",
      "Train 122m 45s (- 17m 32s) (7 87%) 1.9206\n",
      "Train 123m 17s (- 17m 36s) (7 87%) 1.9097\n",
      "Train 123m 48s (- 17m 41s) (7 87%) 1.9320\n",
      "Train 124m 20s (- 17m 45s) (7 87%) 1.9687\n",
      "Train 124m 51s (- 17m 50s) (7 87%) 2.0028\n",
      "Train 125m 23s (- 17m 54s) (7 87%) 1.9180\n",
      "Train 125m 54s (- 17m 59s) (7 87%) 1.9904\n",
      "Train 126m 26s (- 18m 3s) (7 87%) 1.9417\n",
      "Train 126m 57s (- 18m 8s) (7 87%) 2.1285\n",
      "Train 127m 29s (- 18m 12s) (7 87%) 2.1003\n",
      "Train 128m 1s (- 18m 17s) (7 87%) 1.9674\n",
      "Train 128m 32s (- 18m 21s) (7 87%) 1.9668\n",
      "Train 129m 4s (- 18m 26s) (7 87%) 2.0047\n",
      "Train 129m 35s (- 18m 30s) (7 87%) 2.0354\n",
      "Train 130m 7s (- 18m 35s) (7 87%) 1.9391\n",
      "Train 130m 38s (- 18m 39s) (7 87%) 1.9862\n",
      "Train 131m 10s (- 18m 44s) (7 87%) 1.9963\n",
      "Train 131m 41s (- 18m 48s) (7 87%) 1.9604\n",
      "Train 132m 13s (- 18m 53s) (7 87%) 1.9833\n",
      "Train 132m 44s (- 18m 57s) (7 87%) 1.9719\n",
      "Train 133m 16s (- 19m 2s) (7 87%) 1.8556\n",
      "Train 133m 47s (- 19m 6s) (7 87%) 2.0678\n",
      "Train 134m 19s (- 19m 11s) (7 87%) 1.9748\n",
      "Train 134m 50s (- 19m 15s) (7 87%) 1.8848\n",
      "Train 135m 22s (- 19m 20s) (7 87%) 1.9878\n",
      "Train 135m 53s (- 19m 24s) (7 87%) 2.0073\n",
      "Train 136m 25s (- 19m 29s) (7 87%) 2.0690\n",
      "Train 136m 57s (- 19m 33s) (7 87%) 2.1114\n",
      "Train 137m 28s (- 19m 38s) (7 87%) 1.9993\n",
      "Train 137m 59s (- 19m 42s) (7 87%) 1.9317\n",
      "Train 138m 31s (- 19m 47s) (7 87%) 1.9858\n",
      "Val 138m 42s (- 19m 48s) (7 87%) 0.7449\n",
      "Train 138m 54s (- 0m 0s) (8 100%) 0.7273\n",
      "Train 139m 26s (- 0m 0s) (8 100%) 1.8522\n",
      "Train 139m 57s (- 0m 0s) (8 100%) 1.8916\n",
      "Train 140m 29s (- 0m 0s) (8 100%) 1.9787\n",
      "Train 141m 0s (- 0m 0s) (8 100%) 1.9311\n",
      "Train 141m 32s (- 0m 0s) (8 100%) 1.8656\n",
      "Train 142m 3s (- 0m 0s) (8 100%) 1.8901\n",
      "Train 142m 35s (- 0m 0s) (8 100%) 1.9309\n",
      "Train 143m 6s (- 0m 0s) (8 100%) 1.9428\n",
      "Train 143m 38s (- 0m 0s) (8 100%) 1.9659\n",
      "Train 144m 10s (- 0m 0s) (8 100%) 1.9680\n",
      "Train 144m 41s (- 0m 0s) (8 100%) 1.9161\n",
      "Train 145m 13s (- 0m 0s) (8 100%) 1.9020\n",
      "Train 145m 44s (- 0m 0s) (8 100%) 1.9463\n",
      "Train 146m 16s (- 0m 0s) (8 100%) 1.9602\n",
      "Train 146m 47s (- 0m 0s) (8 100%) 2.0018\n",
      "Train 147m 19s (- 0m 0s) (8 100%) 1.9830\n",
      "Train 147m 51s (- 0m 0s) (8 100%) 2.0235\n",
      "Train 148m 22s (- 0m 0s) (8 100%) 1.7935\n",
      "Train 148m 54s (- 0m 0s) (8 100%) 2.0010\n",
      "Train 149m 25s (- 0m 0s) (8 100%) 1.9617\n",
      "Train 149m 57s (- 0m 0s) (8 100%) 1.9319\n",
      "Train 150m 28s (- 0m 0s) (8 100%) 1.8995\n",
      "Train 151m 0s (- 0m 0s) (8 100%) 1.9804\n",
      "Train 151m 31s (- 0m 0s) (8 100%) 1.8990\n",
      "Train 152m 3s (- 0m 0s) (8 100%) 1.9103\n",
      "Train 152m 34s (- 0m 0s) (8 100%) 2.0065\n",
      "Train 153m 6s (- 0m 0s) (8 100%) 1.9188\n",
      "Train 153m 38s (- 0m 0s) (8 100%) 1.9954\n",
      "Train 154m 9s (- 0m 0s) (8 100%) 1.9727\n",
      "Train 154m 41s (- 0m 0s) (8 100%) 2.0110\n",
      "Train 155m 12s (- 0m 0s) (8 100%) 2.0218\n",
      "Train 155m 44s (- 0m 0s) (8 100%) 1.9442\n",
      "Train 156m 15s (- 0m 0s) (8 100%) 1.8936\n",
      "Train 156m 47s (- 0m 0s) (8 100%) 2.0059\n",
      "Train 157m 18s (- 0m 0s) (8 100%) 1.9512\n",
      "Train 157m 50s (- 0m 0s) (8 100%) 1.9872\n",
      "Train 158m 21s (- 0m 0s) (8 100%) 2.0027\n",
      "Val 158m 32s (- 0m 0s) (8 100%) 0.7194\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 500\n",
    "encoder2 = EncoderRNN(hidden_size).to(device)\n",
    "decoder2 = DecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "plotloss_t2, plotloss_v2 = trainIters(encoder2, decoder2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "\n",
    "encoder3 = EncoderRNN(hidden_size).to(device)\n",
    "decoder3 = DecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "encoder3.load_state_dict(torch.load(model_path + \"encoder_rnn_vi3008.pth\"))\n",
    "decoder3.load_state_dict(torch.load(model_path + \"decoder_rnn_vi3008.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, encoder, decoder):\n",
    "    decoded_words_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data1, data2, length1, length2) in enumerate(loader):\n",
    "            input_tensor = data1\n",
    "            input_length = input_tensor.size()[0]\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden(input_length)\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "            \n",
    "            decoder_input = torch.tensor(np.array([[SOS_token]]), device=device)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoded_words = []\n",
    "            \n",
    "            for di in range(MAX_LENGTH_EN):\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input.reshape(1,1), decoder_hidden)\n",
    "                topv, topi = decoder_output.data.topk(1) \n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_words.append('<eos>')\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_words.append(idx2words_ft_en[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = decoder_input.unsqueeze(0)\n",
    "            decoded_words_list.append(decoded_words)\n",
    "        return decoded_words_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list = evaluate(val_loader2, encoder3, decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for validation dataset: 6.547676119621894\n"
     ]
    }
   ],
   "source": [
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(val_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in val_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for validation dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_list_nopad = []\n",
    "#for ii in range(len(predicted_list)):\n",
    "    #line = ''\n",
    "    #for jj in predicted_list[ii]:\n",
    "        #if jj != '<pad>':\n",
    "            #line = line + ' ' + jj\n",
    "    #predicted_list_nopad.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for iii in range(len(predicted_list_nopad)):\n",
    "    #if predicted_list_nopad[iii][-5:] != '<eos>':\n",
    "        #predicted_list_nopad[iii] = predicted_list_nopad[iii] + ' <eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_list = []\n",
    "#for iii in range(len(val_en_indexes_filtered)):\n",
    "    #line = ''\n",
    "    #for jjj in val_en_indexes_filtered[iii]:\n",
    "        #line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    #label_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('bleu score for validation dataset:',corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> I was a woman , and I was a in in . . \n",
      "> I m a child of 1984 , and I live in the city of Berlin . \n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list = evaluate(test_loader, encoder3, decoder3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for test dataset: 4.875237288827215\n"
     ]
    }
   ],
   "source": [
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(test_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in test_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_list_nopad = []\n",
    "#for ii in range(len(predicted_list)):\n",
    "    #line = ''\n",
    "    #for jj in predicted_list[ii]:\n",
    "        #if jj != '<pad>':\n",
    "            #line = line + ' ' + jj\n",
    "    #predicted_list_nopad.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for iii in range(len(predicted_list_nopad)):\n",
    "    #if predicted_list_nopad[iii][-5:] != '<eos>':\n",
    "        #predicted_list_nopad[iii] = predicted_list_nopad[iii] + ' <eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_list = []\n",
    "#for iii in range(len(test_en_indexes_filtered)):\n",
    "    #line = ''\n",
    "    #for jjj in test_en_indexes_filtered[iii]:\n",
    "        #line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    #label_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> You can a your your and and you you you and and you . .\n",
      "> You can put a knob in between and now you ve made a little dimmer . \n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "\n",
    "encoder4 = EncoderRNN(hidden_size).to(device)\n",
    "decoder4 = DecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "encoder4.load_state_dict(torch.load(model_path + \"encoder_rnn_vi5008.pth\"))\n",
    "decoder4.load_state_dict(torch.load(model_path + \"decoder_rnn_vi5008.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list = evaluate(val_loader2, encoder4, decoder4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for validation dataset: 8.158234021900313\n"
     ]
    }
   ],
   "source": [
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(val_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in val_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for validation dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> In India , there are a few people , and and , , , , , , , , , , , , ,\n",
      "> And there are only <unk> young black and Latino men in New York , so for them , it s not a question of , Will I get stopped ? \n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list = evaluate(test_loader, encoder4, decoder4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for test dataset: 5.092250995456274\n"
     ]
    }
   ],
   "source": [
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(test_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in test_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> I think I will think more more more . \n",
      "> I think I might have actually heard more hands . \n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
