{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re  \n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_zh = '/home/ys2542/Neural-Translation-System/iwslt-zh-en-processed/'\n",
    "path_vi = '/home/ys2542/Neural-Translation-System/iwslt-vi-en-processed/'\n",
    "ft_home = '/scratch/ys2542/NLP_FASTTEXT/'\n",
    "model_path = '/scratch/ys2542/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "UNK_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 100000 \n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_en = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_en = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_en = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_en = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_en[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = idx\n",
    "        idx2words_ft_en[idx] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ft_home + 'cc.zh.300.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_zh = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_zh = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_zh = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_zh = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_zh[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_zh[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_zh[s[0]] = idx\n",
    "        idx2words_ft_zh[idx] = s[0]\n",
    "        ordered_words_ft_zh.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_zh_train = open(path_zh + 'train.tok.zh', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_train = open(path_zh + 'train.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_val = open(path_zh + 'dev.tok.zh', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_val = open(path_zh + 'dev.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_test = open(path_zh + 'test.tok.zh', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_test = open(path_zh + 'test.tok.en', encoding = 'utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lines(lines, lang):\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "            line = ' '\n",
    "        if lang == 'en':\n",
    "            line = line.replace(\"&apos;\", \"\").replace(\"&quot;\", \"\")\n",
    "        if line[-1] != ' ':\n",
    "            line = line + ' '\n",
    "       \n",
    "        line = '<sos> ' + line + '<eos>'\n",
    "        data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh = clean_lines(lines_zh_train, 'zh')\n",
    "train_en = clean_lines(lines_en_train, 'en')\n",
    "\n",
    "val_zh = clean_lines(lines_zh_val, 'zh')\n",
    "val_en = clean_lines(lines_en_val, 'en')\n",
    "\n",
    "test_zh = clean_lines(lines_zh_test, 'zh')\n",
    "test_en = clean_lines(lines_en_test, 'en')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(data, lang):\n",
    "    indexes = []\n",
    "    for sentence in data:\n",
    "        index = []\n",
    "        for token in sentence.split():\n",
    "            if lang == 'zh':\n",
    "                try:\n",
    "                    index.append(words_ft_zh[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "            elif lang == 'en':\n",
    "                try:\n",
    "                    index.append(words_ft_en[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "        indexes.append(index)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh_indexes = indexesFromSentence(train_zh, 'zh')\n",
    "train_en_indexes = indexesFromSentence(train_en, 'en')\n",
    "\n",
    "val_zh_indexes = indexesFromSentence(val_zh, 'zh')\n",
    "val_en_indexes = indexesFromSentence(val_en, 'en')\n",
    "\n",
    "test_zh_indexes = indexesFromSentence(test_zh, 'zh')\n",
    "test_en_indexes = indexesFromSentence(test_en, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "length_zh = []\n",
    "for line in train_zh_indexes:\n",
    "        length_zh.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_zh)\n",
    "MAX_LENGTH_ZH = length_zh[int(len(train_zh_indexes)*0.99)]\n",
    "print(MAX_LENGTH_ZH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in train_en_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(train_en_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in val_zh_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(val_zh_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in val_en_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(val_en_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in test_zh_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(test_zh_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in test_en_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(test_en_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH_ZH = 69\n",
    "MAX_LENGTH_EN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh_indexes_filtered = []\n",
    "train_en_indexes_filtered = []\n",
    "for i in range(len(train_zh_indexes)):\n",
    "    if len(train_zh_indexes[i]) <= MAX_LENGTH_ZH and len(train_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        train_zh_indexes_filtered.append(train_zh_indexes[i])\n",
    "        train_en_indexes_filtered.append(train_en_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_zh_indexes_filtered = []\n",
    "val_en_indexes_filtered = []\n",
    "for i in range(len(val_zh_indexes)):\n",
    "    if len(val_zh_indexes[i]) <= MAX_LENGTH_ZH and len(val_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        val_zh_indexes_filtered.append(val_zh_indexes[i])\n",
    "        val_en_indexes_filtered.append(val_en_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zh_indexes_filtered = []\n",
    "test_en_indexes_filtered = []\n",
    "for i in range(len(test_zh_indexes)):\n",
    "    if len(test_zh_indexes[i]) <= MAX_LENGTH_ZH and len(test_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        test_zh_indexes_filtered.append(test_zh_indexes[i])\n",
    "        test_en_indexes_filtered.append(test_en_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    def __init__(self, data_list1, data_list2):\n",
    "        \n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        \n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "            \n",
    "    def __getitem__(self, key):        \n",
    "        return [self.data_list1[key], self.data_list2[key], len(self.data_list1[key]), len(self.data_list2[key])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "        \n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_LENGTH_ZH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_LENGTH_EN-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        data_list1.append(padded_vec1[:MAX_LENGTH_ZH])\n",
    "        data_list2.append(padded_vec2[:MAX_LENGTH_EN])\n",
    "\n",
    "\n",
    "    return [torch.from_numpy(np.array(data_list1)).cuda(), torch.from_numpy(np.array(data_list2)).cuda(),\n",
    "                torch.LongTensor(length_list1).cuda(), torch.LongTensor(length_list2).cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = VocabDataset(train_zh_indexes_filtered, train_en_indexes_filtered)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_zh_indexes_filtered, val_en_indexes_filtered)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = VocabDataset(test_zh_indexes_filtered, test_en_indexes_filtered)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_loader2 = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size=EMBED_SIZE):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_zh).float(), freeze=False)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH_ZH, embed_size=EMBED_SIZE):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_en).float(), freeze=False)\n",
    "        self.attn = nn.Linear(hidden_size + embed_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size + embed_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded, hidden), 2)), dim=2)\n",
    "        attn_applied = torch.bmm(attn_weights[0].unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size, input_length = input_tensor.size()\n",
    "    _, target_length = target_tensor.size()\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_token]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output)\n",
    "        \n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di].unsqueeze(0) \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output)\n",
    "                        \n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=100, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    \n",
    "    plot_losses_t = []\n",
    "    print_loss_total_t = 0  \n",
    "    plot_loss_total_t = 0  \n",
    "    \n",
    "    plot_losses_v = []\n",
    "    print_loss_total_v = 0  \n",
    "    plot_loss_total_v = 0 \n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data1, data2, length1, length2) in enumerate(train_loader):\n",
    "            input_tensor = data1\n",
    "            target_tensor = data2\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total_t += loss\n",
    "            plot_loss_total_t += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total_t / print_every\n",
    "                print_loss_total_t = 0\n",
    "                print('Train %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total_t / plot_every\n",
    "                plot_losses_t.append(plot_loss_avg)\n",
    "                plot_loss_total_t = 0\n",
    "                \n",
    "        for i, (data1, data2, length1, length2) in enumerate(val_loader):\n",
    "            input_tensor = data1\n",
    "            target_tensor = data2\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total_v += loss\n",
    "            plot_loss_total_v += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total_v / print_every\n",
    "                print_loss_total_v = 0\n",
    "                print('Val %s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total_v / plot_every\n",
    "                plot_losses_v.append(plot_loss_avg)\n",
    "                plot_loss_total_v = 0\n",
    "                \n",
    "        torch.save(encoder.state_dict(), model_path + \"encoder_rnn_attn\"+str(hidden_size)+str(iter)+\".pth\")\n",
    "        torch.save(decoder.state_dict(), model_path + \"decoder_rnn_attn\"+str(hidden_size)+str(iter)+\".pth\")\n",
    "\n",
    "    return plot_losses_t, plot_losses_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0m 0s (- 0m 5s) (1 12%) 0.1153\n",
      "Train 1m 9s (- 8m 8s) (1 12%) 3.9951\n",
      "Train 2m 20s (- 16m 22s) (1 12%) 3.0700\n",
      "Train 3m 30s (- 24m 36s) (1 12%) 3.0693\n",
      "Train 4m 42s (- 32m 58s) (1 12%) 2.9614\n",
      "Train 5m 54s (- 41m 20s) (1 12%) 2.9468\n",
      "Train 7m 5s (- 49m 41s) (1 12%) 2.8705\n",
      "Train 8m 17s (- 58m 4s) (1 12%) 2.9036\n",
      "Train 9m 29s (- 66m 25s) (1 12%) 2.9424\n",
      "Train 10m 40s (- 74m 44s) (1 12%) 2.8698\n",
      "Train 11m 52s (- 83m 4s) (1 12%) 2.8363\n",
      "Train 13m 3s (- 91m 26s) (1 12%) 2.7893\n",
      "Train 14m 15s (- 99m 46s) (1 12%) 2.7380\n",
      "Train 15m 27s (- 108m 10s) (1 12%) 2.7281\n",
      "Train 16m 38s (- 116m 30s) (1 12%) 2.7013\n",
      "Train 17m 50s (- 124m 54s) (1 12%) 2.6658\n",
      "Train 19m 2s (- 133m 17s) (1 12%) 2.6631\n",
      "Train 20m 13s (- 141m 37s) (1 12%) 2.5621\n",
      "Train 21m 25s (- 149m 58s) (1 12%) 2.6221\n",
      "Train 22m 37s (- 158m 19s) (1 12%) 2.5598\n",
      "Train 23m 48s (- 166m 42s) (1 12%) 2.5841\n",
      "Train 25m 0s (- 175m 4s) (1 12%) 2.5988\n",
      "Train 26m 12s (- 183m 30s) (1 12%) 2.5894\n",
      "Train 27m 24s (- 191m 50s) (1 12%) 2.5036\n",
      "Train 28m 36s (- 200m 12s) (1 12%) 2.5355\n",
      "Train 29m 47s (- 208m 33s) (1 12%) 2.5395\n",
      "Train 30m 59s (- 216m 54s) (1 12%) 2.4596\n",
      "Train 32m 10s (- 225m 14s) (1 12%) 2.4732\n",
      "Train 33m 22s (- 233m 36s) (1 12%) 2.4040\n",
      "Train 34m 34s (- 242m 0s) (1 12%) 2.4805\n",
      "Train 35m 46s (- 250m 23s) (1 12%) 2.4478\n",
      "Train 36m 57s (- 258m 44s) (1 12%) 2.4713\n",
      "Train 38m 9s (- 267m 4s) (1 12%) 2.4638\n",
      "Train 39m 20s (- 275m 26s) (1 12%) 2.4035\n",
      "Train 40m 32s (- 283m 46s) (1 12%) 2.3804\n",
      "Train 41m 43s (- 292m 6s) (1 12%) 2.4040\n",
      "Train 42m 55s (- 300m 26s) (1 12%) 2.4002\n",
      "Train 44m 7s (- 308m 51s) (1 12%) 2.4092\n",
      "Train 45m 19s (- 317m 14s) (1 12%) 2.3207\n",
      "Train 46m 31s (- 325m 37s) (1 12%) 2.4027\n",
      "Train 47m 42s (- 333m 59s) (1 12%) 2.3126\n",
      "Train 48m 54s (- 342m 18s) (1 12%) 2.3005\n",
      "Train 50m 5s (- 350m 40s) (1 12%) 2.3734\n",
      "Train 51m 17s (- 359m 2s) (1 12%) 2.3882\n",
      "Train 52m 29s (- 367m 23s) (1 12%) 2.3121\n",
      "Train 53m 40s (- 375m 44s) (1 12%) 2.3050\n",
      "Train 54m 51s (- 384m 3s) (1 12%) 2.2716\n",
      "Train 56m 3s (- 392m 25s) (1 12%) 2.3055\n",
      "Train 57m 15s (- 400m 47s) (1 12%) 2.3036\n",
      "Train 58m 26s (- 409m 5s) (1 12%) 2.2181\n",
      "Train 59m 38s (- 417m 26s) (1 12%) 2.2953\n",
      "Train 60m 49s (- 425m 46s) (1 12%) 2.2854\n",
      "Train 62m 1s (- 434m 7s) (1 12%) 2.1892\n",
      "Train 63m 12s (- 442m 30s) (1 12%) 2.2772\n",
      "Train 64m 24s (- 450m 49s) (1 12%) 2.2021\n",
      "Train 65m 35s (- 459m 8s) (1 12%) 2.2209\n",
      "Train 66m 47s (- 467m 30s) (1 12%) 2.2738\n",
      "Train 67m 58s (- 475m 50s) (1 12%) 2.2347\n",
      "Train 69m 10s (- 484m 11s) (1 12%) 2.2141\n",
      "Train 70m 21s (- 492m 30s) (1 12%) 2.2210\n",
      "Train 71m 33s (- 500m 52s) (1 12%) 2.2912\n",
      "Val 71m 35s (- 501m 6s) (1 12%) 0.0256\n",
      "Train 72m 1s (- 216m 4s) (2 25%) 0.0759\n",
      "Train 73m 12s (- 219m 37s) (2 25%) 2.1538\n",
      "Train 74m 24s (- 223m 12s) (2 25%) 2.2010\n",
      "Train 75m 35s (- 226m 45s) (2 25%) 2.2226\n",
      "Train 76m 45s (- 230m 16s) (2 25%) 2.1743\n",
      "Train 77m 55s (- 233m 47s) (2 25%) 2.2165\n",
      "Train 79m 6s (- 237m 18s) (2 25%) 2.2053\n",
      "Train 80m 16s (- 240m 48s) (2 25%) 2.2045\n",
      "Train 81m 25s (- 244m 17s) (2 25%) 2.1411\n",
      "Train 82m 35s (- 247m 46s) (2 25%) 2.1220\n",
      "Train 83m 45s (- 251m 16s) (2 25%) 2.1689\n",
      "Train 84m 55s (- 254m 46s) (2 25%) 2.1489\n",
      "Train 86m 5s (- 258m 17s) (2 25%) 2.2139\n",
      "Train 87m 15s (- 261m 46s) (2 25%) 2.1266\n",
      "Train 88m 25s (- 265m 16s) (2 25%) 2.1922\n",
      "Train 89m 35s (- 268m 46s) (2 25%) 2.1782\n",
      "Train 90m 45s (- 272m 16s) (2 25%) 2.1754\n",
      "Train 91m 55s (- 275m 47s) (2 25%) 2.1636\n",
      "Train 93m 5s (- 279m 17s) (2 25%) 2.2036\n",
      "Train 94m 15s (- 282m 47s) (2 25%) 2.1437\n",
      "Train 95m 25s (- 286m 17s) (2 25%) 2.1597\n",
      "Train 96m 35s (- 289m 47s) (2 25%) 2.1687\n",
      "Train 97m 45s (- 293m 17s) (2 25%) 2.1092\n",
      "Train 98m 56s (- 296m 48s) (2 25%) 2.1765\n",
      "Train 100m 5s (- 300m 17s) (2 25%) 2.1264\n",
      "Train 101m 15s (- 303m 47s) (2 25%) 2.1381\n",
      "Train 102m 26s (- 307m 18s) (2 25%) 2.2137\n",
      "Train 103m 36s (- 310m 48s) (2 25%) 2.1371\n",
      "Train 104m 46s (- 314m 20s) (2 25%) 2.2431\n",
      "Train 105m 56s (- 317m 50s) (2 25%) 2.1367\n",
      "Train 107m 6s (- 321m 20s) (2 25%) 2.1300\n",
      "Train 108m 16s (- 324m 50s) (2 25%) 2.0787\n",
      "Train 109m 26s (- 328m 19s) (2 25%) 2.0804\n",
      "Train 110m 36s (- 331m 48s) (2 25%) 2.1041\n",
      "Train 111m 46s (- 335m 19s) (2 25%) 2.1198\n",
      "Train 112m 56s (- 338m 49s) (2 25%) 2.1243\n",
      "Train 114m 6s (- 342m 20s) (2 25%) 2.1047\n",
      "Train 115m 16s (- 345m 50s) (2 25%) 2.0902\n",
      "Train 116m 27s (- 349m 21s) (2 25%) 2.1522\n",
      "Train 117m 36s (- 352m 50s) (2 25%) 2.0485\n",
      "Train 118m 46s (- 356m 20s) (2 25%) 2.0754\n",
      "Train 119m 56s (- 359m 50s) (2 25%) 2.0872\n",
      "Train 121m 6s (- 363m 19s) (2 25%) 2.1045\n",
      "Train 122m 16s (- 366m 50s) (2 25%) 2.1482\n",
      "Train 123m 26s (- 370m 20s) (2 25%) 2.1142\n",
      "Train 124m 36s (- 373m 50s) (2 25%) 2.0699\n",
      "Train 125m 47s (- 377m 21s) (2 25%) 2.1522\n",
      "Train 126m 57s (- 380m 51s) (2 25%) 2.1331\n",
      "Train 128m 6s (- 384m 20s) (2 25%) 2.0262\n",
      "Train 129m 16s (- 387m 50s) (2 25%) 2.0890\n",
      "Train 130m 26s (- 391m 20s) (2 25%) 2.1109\n",
      "Train 131m 36s (- 394m 50s) (2 25%) 2.0677\n",
      "Train 132m 46s (- 398m 19s) (2 25%) 2.0612\n",
      "Train 133m 56s (- 401m 49s) (2 25%) 2.0474\n",
      "Train 135m 6s (- 405m 20s) (2 25%) 2.1238\n",
      "Train 136m 16s (- 408m 50s) (2 25%) 2.0770\n",
      "Train 137m 26s (- 412m 20s) (2 25%) 2.0612\n",
      "Train 138m 36s (- 415m 49s) (2 25%) 2.0192\n",
      "Train 139m 46s (- 419m 20s) (2 25%) 2.1275\n",
      "Train 140m 56s (- 422m 49s) (2 25%) 2.0191\n",
      "Train 142m 6s (- 426m 20s) (2 25%) 2.1008\n",
      "Val 142m 8s (- 426m 26s) (2 25%) 0.8636\n",
      "Train 142m 34s (- 237m 36s) (3 37%) 0.0535\n",
      "Train 143m 44s (- 239m 34s) (3 37%) 2.0645\n",
      "Train 144m 54s (- 241m 30s) (3 37%) 1.9922\n",
      "Train 146m 4s (- 243m 27s) (3 37%) 2.0170\n",
      "Train 147m 13s (- 245m 23s) (3 37%) 1.8846\n",
      "Train 148m 24s (- 247m 20s) (3 37%) 2.0561\n",
      "Train 149m 33s (- 249m 16s) (3 37%) 1.9133\n",
      "Train 150m 43s (- 251m 13s) (3 37%) 2.0245\n",
      "Train 151m 53s (- 253m 9s) (3 37%) 1.9671\n",
      "Train 153m 4s (- 255m 7s) (3 37%) 2.0485\n",
      "Train 154m 13s (- 257m 3s) (3 37%) 1.9273\n",
      "Train 155m 24s (- 259m 0s) (3 37%) 2.0830\n",
      "Train 156m 34s (- 260m 58s) (3 37%) 2.0629\n",
      "Train 157m 45s (- 262m 55s) (3 37%) 2.0263\n",
      "Train 158m 54s (- 264m 51s) (3 37%) 1.9064\n",
      "Train 160m 4s (- 266m 47s) (3 37%) 1.9651\n",
      "Train 161m 14s (- 268m 44s) (3 37%) 2.0272\n",
      "Train 162m 24s (- 270m 41s) (3 37%) 1.9524\n",
      "Train 163m 34s (- 272m 37s) (3 37%) 1.9530\n",
      "Train 164m 44s (- 274m 33s) (3 37%) 1.9137\n",
      "Train 165m 53s (- 276m 29s) (3 37%) 1.9542\n",
      "Train 167m 4s (- 278m 26s) (3 37%) 2.0142\n",
      "Train 168m 14s (- 280m 23s) (3 37%) 2.0123\n",
      "Train 169m 24s (- 282m 20s) (3 37%) 2.0222\n",
      "Train 170m 34s (- 284m 18s) (3 37%) 2.0731\n",
      "Train 171m 44s (- 286m 14s) (3 37%) 2.0226\n",
      "Train 172m 55s (- 288m 11s) (3 37%) 1.9959\n",
      "Train 174m 4s (- 290m 8s) (3 37%) 1.9805\n",
      "Train 175m 15s (- 292m 5s) (3 37%) 1.9733\n",
      "Train 176m 24s (- 294m 1s) (3 37%) 2.0041\n",
      "Train 177m 34s (- 295m 57s) (3 37%) 1.9296\n",
      "Train 178m 44s (- 297m 54s) (3 37%) 2.0126\n",
      "Train 179m 54s (- 299m 50s) (3 37%) 1.9303\n",
      "Train 181m 4s (- 301m 47s) (3 37%) 2.0087\n",
      "Train 182m 14s (- 303m 44s) (3 37%) 1.9763\n",
      "Train 183m 24s (- 305m 41s) (3 37%) 1.9898\n",
      "Train 184m 34s (- 307m 36s) (3 37%) 1.9124\n",
      "Train 185m 44s (- 309m 33s) (3 37%) 1.9682\n",
      "Train 186m 54s (- 311m 30s) (3 37%) 2.0011\n",
      "Train 188m 4s (- 313m 27s) (3 37%) 1.9934\n",
      "Train 189m 14s (- 315m 23s) (3 37%) 1.9251\n",
      "Train 190m 23s (- 317m 19s) (3 37%) 1.9494\n",
      "Train 191m 34s (- 319m 17s) (3 37%) 2.0358\n",
      "Train 192m 43s (- 321m 13s) (3 37%) 1.9375\n",
      "Train 193m 54s (- 323m 10s) (3 37%) 2.0031\n",
      "Train 195m 4s (- 325m 6s) (3 37%) 1.9742\n",
      "Train 196m 14s (- 327m 3s) (3 37%) 2.0087\n",
      "Train 197m 23s (- 328m 59s) (3 37%) 1.9318\n",
      "Train 198m 33s (- 330m 56s) (3 37%) 1.9926\n",
      "Train 199m 44s (- 332m 54s) (3 37%) 2.0039\n",
      "Train 200m 55s (- 334m 52s) (3 37%) 1.9223\n",
      "Train 202m 8s (- 336m 53s) (3 37%) 1.9659\n",
      "Train 203m 20s (- 338m 54s) (3 37%) 2.0436\n",
      "Train 204m 32s (- 340m 54s) (3 37%) 2.0598\n",
      "Train 205m 43s (- 342m 53s) (3 37%) 1.9299\n",
      "Train 206m 55s (- 344m 52s) (3 37%) 2.0408\n",
      "Train 208m 6s (- 346m 50s) (3 37%) 1.8727\n",
      "Train 209m 17s (- 348m 49s) (3 37%) 1.9527\n",
      "Train 210m 29s (- 350m 48s) (3 37%) 1.9823\n",
      "Train 211m 40s (- 352m 47s) (3 37%) 1.9221\n",
      "Train 212m 51s (- 354m 46s) (3 37%) 1.9999\n",
      "Val 212m 53s (- 354m 49s) (3 37%) 0.7697\n",
      "Train 213m 19s (- 213m 19s) (4 50%) 0.0601\n",
      "Train 214m 30s (- 214m 30s) (4 50%) 1.8932\n",
      "Train 215m 42s (- 215m 42s) (4 50%) 1.8865\n",
      "Train 216m 53s (- 216m 53s) (4 50%) 1.8517\n",
      "Train 218m 4s (- 218m 4s) (4 50%) 1.9242\n",
      "Train 219m 15s (- 219m 15s) (4 50%) 1.8350\n",
      "Train 220m 27s (- 220m 27s) (4 50%) 1.8637\n",
      "Train 221m 39s (- 221m 39s) (4 50%) 1.9444\n",
      "Train 222m 50s (- 222m 50s) (4 50%) 1.8645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 224m 2s (- 224m 2s) (4 50%) 1.8963\n",
      "Train 225m 13s (- 225m 13s) (4 50%) 1.9147\n",
      "Train 226m 25s (- 226m 25s) (4 50%) 1.9360\n",
      "Train 227m 37s (- 227m 37s) (4 50%) 1.9148\n",
      "Train 228m 49s (- 228m 49s) (4 50%) 1.9366\n",
      "Train 230m 0s (- 230m 0s) (4 50%) 1.8476\n",
      "Train 231m 12s (- 231m 12s) (4 50%) 1.9414\n",
      "Train 232m 23s (- 232m 23s) (4 50%) 1.8837\n",
      "Train 233m 34s (- 233m 34s) (4 50%) 1.8676\n",
      "Train 234m 46s (- 234m 46s) (4 50%) 1.8636\n",
      "Train 235m 57s (- 235m 57s) (4 50%) 1.8832\n",
      "Train 237m 9s (- 237m 9s) (4 50%) 1.9297\n",
      "Train 238m 20s (- 238m 20s) (4 50%) 1.8700\n",
      "Train 239m 32s (- 239m 32s) (4 50%) 1.8875\n",
      "Train 240m 43s (- 240m 43s) (4 50%) 1.8513\n",
      "Train 241m 54s (- 241m 54s) (4 50%) 1.9023\n",
      "Train 243m 5s (- 243m 5s) (4 50%) 1.9086\n",
      "Train 244m 17s (- 244m 17s) (4 50%) 1.9014\n",
      "Train 245m 28s (- 245m 28s) (4 50%) 1.8271\n",
      "Train 246m 39s (- 246m 39s) (4 50%) 1.8636\n",
      "Train 247m 51s (- 247m 51s) (4 50%) 1.8893\n",
      "Train 249m 2s (- 249m 2s) (4 50%) 1.9259\n",
      "Train 250m 13s (- 250m 13s) (4 50%) 1.8593\n",
      "Train 251m 24s (- 251m 24s) (4 50%) 1.9111\n",
      "Train 252m 36s (- 252m 36s) (4 50%) 1.9005\n",
      "Train 253m 47s (- 253m 47s) (4 50%) 1.8652\n",
      "Train 254m 59s (- 254m 59s) (4 50%) 1.9118\n",
      "Train 256m 10s (- 256m 10s) (4 50%) 1.8759\n",
      "Train 257m 22s (- 257m 22s) (4 50%) 1.9269\n",
      "Train 258m 33s (- 258m 33s) (4 50%) 1.9169\n",
      "Train 259m 44s (- 259m 44s) (4 50%) 1.8413\n",
      "Train 260m 55s (- 260m 55s) (4 50%) 1.9144\n",
      "Train 262m 7s (- 262m 7s) (4 50%) 1.9044\n",
      "Train 263m 18s (- 263m 18s) (4 50%) 1.8705\n",
      "Train 264m 30s (- 264m 30s) (4 50%) 1.9132\n",
      "Train 265m 42s (- 265m 42s) (4 50%) 1.9807\n",
      "Train 266m 52s (- 266m 52s) (4 50%) 1.8921\n",
      "Train 268m 4s (- 268m 4s) (4 50%) 1.8966\n",
      "Train 269m 17s (- 269m 17s) (4 50%) 1.9795\n",
      "Train 270m 29s (- 270m 29s) (4 50%) 1.9392\n",
      "Train 271m 40s (- 271m 40s) (4 50%) 1.8598\n",
      "Train 272m 52s (- 272m 52s) (4 50%) 1.8849\n",
      "Train 274m 3s (- 274m 3s) (4 50%) 1.9287\n",
      "Train 275m 14s (- 275m 14s) (4 50%) 1.9346\n",
      "Train 276m 26s (- 276m 26s) (4 50%) 1.9201\n",
      "Train 277m 37s (- 277m 37s) (4 50%) 1.8145\n",
      "Train 278m 49s (- 278m 49s) (4 50%) 1.8631\n",
      "Train 280m 0s (- 280m 0s) (4 50%) 1.8959\n",
      "Train 281m 12s (- 281m 12s) (4 50%) 1.8822\n",
      "Train 282m 23s (- 282m 23s) (4 50%) 1.9480\n",
      "Train 283m 35s (- 283m 35s) (4 50%) 1.8718\n",
      "Train 284m 46s (- 284m 46s) (4 50%) 1.9214\n",
      "Val 284m 48s (- 284m 48s) (4 50%) 0.7200\n",
      "Train 285m 13s (- 171m 8s) (5 62%) 0.0784\n",
      "Train 286m 25s (- 171m 51s) (5 62%) 1.8052\n",
      "Train 287m 36s (- 172m 33s) (5 62%) 1.8441\n",
      "Train 288m 47s (- 173m 16s) (5 62%) 1.7687\n",
      "Train 289m 58s (- 173m 59s) (5 62%) 1.7911\n",
      "Train 291m 9s (- 174m 41s) (5 62%) 1.8235\n",
      "Train 292m 21s (- 175m 24s) (5 62%) 1.8096\n",
      "Train 293m 32s (- 176m 7s) (5 62%) 1.8692\n",
      "Train 294m 44s (- 176m 50s) (5 62%) 1.8218\n",
      "Train 295m 56s (- 177m 33s) (5 62%) 1.8857\n",
      "Train 297m 7s (- 178m 16s) (5 62%) 1.7805\n",
      "Train 298m 18s (- 178m 59s) (5 62%) 1.8090\n",
      "Train 299m 29s (- 179m 41s) (5 62%) 1.8067\n",
      "Train 300m 40s (- 180m 24s) (5 62%) 1.8453\n",
      "Train 301m 52s (- 181m 7s) (5 62%) 1.8039\n",
      "Train 303m 3s (- 181m 49s) (5 62%) 1.7838\n",
      "Train 304m 14s (- 182m 32s) (5 62%) 1.8984\n",
      "Train 305m 25s (- 183m 15s) (5 62%) 1.9022\n",
      "Train 306m 37s (- 183m 58s) (5 62%) 1.8377\n",
      "Train 307m 49s (- 184m 41s) (5 62%) 1.8642\n",
      "Train 309m 0s (- 185m 24s) (5 62%) 1.8362\n",
      "Train 310m 12s (- 186m 7s) (5 62%) 1.8310\n",
      "Train 311m 24s (- 186m 50s) (5 62%) 1.9032\n",
      "Train 312m 35s (- 187m 33s) (5 62%) 1.7824\n",
      "Train 313m 46s (- 188m 15s) (5 62%) 1.7783\n",
      "Train 314m 58s (- 188m 58s) (5 62%) 1.8787\n",
      "Train 316m 9s (- 189m 41s) (5 62%) 1.8108\n",
      "Train 317m 21s (- 190m 24s) (5 62%) 1.8572\n",
      "Train 318m 33s (- 191m 7s) (5 62%) 1.9171\n",
      "Train 319m 43s (- 191m 50s) (5 62%) 1.7398\n",
      "Train 320m 55s (- 192m 33s) (5 62%) 1.8301\n",
      "Train 322m 6s (- 193m 15s) (5 62%) 1.7700\n",
      "Train 323m 17s (- 193m 58s) (5 62%) 1.8607\n",
      "Train 324m 29s (- 194m 41s) (5 62%) 1.8690\n",
      "Train 325m 41s (- 195m 24s) (5 62%) 1.8913\n",
      "Train 326m 52s (- 196m 7s) (5 62%) 1.8892\n",
      "Train 328m 4s (- 196m 50s) (5 62%) 1.8031\n",
      "Train 329m 15s (- 197m 33s) (5 62%) 1.8194\n",
      "Train 330m 26s (- 198m 16s) (5 62%) 1.8299\n",
      "Train 331m 38s (- 198m 59s) (5 62%) 1.8118\n",
      "Train 332m 49s (- 199m 41s) (5 62%) 1.8088\n",
      "Train 334m 0s (- 200m 24s) (5 62%) 1.8675\n",
      "Train 335m 12s (- 201m 7s) (5 62%) 1.8683\n",
      "Train 336m 24s (- 201m 50s) (5 62%) 1.8574\n",
      "Train 337m 35s (- 202m 33s) (5 62%) 1.8270\n",
      "Train 338m 47s (- 203m 16s) (5 62%) 1.8946\n",
      "Train 339m 59s (- 203m 59s) (5 62%) 1.8217\n",
      "Train 341m 10s (- 204m 42s) (5 62%) 1.8151\n",
      "Train 342m 21s (- 205m 25s) (5 62%) 1.8382\n",
      "Train 343m 33s (- 206m 8s) (5 62%) 1.8319\n",
      "Train 344m 44s (- 206m 50s) (5 62%) 1.7050\n",
      "Train 345m 56s (- 207m 33s) (5 62%) 1.8954\n",
      "Train 347m 7s (- 208m 16s) (5 62%) 1.8423\n",
      "Train 348m 19s (- 208m 59s) (5 62%) 1.8272\n",
      "Train 349m 30s (- 209m 42s) (5 62%) 1.8285\n",
      "Train 350m 42s (- 210m 25s) (5 62%) 1.8449\n",
      "Train 351m 53s (- 211m 8s) (5 62%) 1.8247\n",
      "Train 353m 5s (- 211m 51s) (5 62%) 1.8972\n",
      "Train 354m 16s (- 212m 33s) (5 62%) 1.8704\n",
      "Train 355m 28s (- 213m 16s) (5 62%) 1.9044\n",
      "Train 356m 39s (- 213m 59s) (5 62%) 1.8801\n",
      "Val 356m 41s (- 214m 0s) (5 62%) 0.6714\n",
      "Train 357m 6s (- 119m 2s) (6 75%) 0.0464\n",
      "Train 358m 18s (- 119m 26s) (6 75%) 1.7718\n",
      "Train 359m 29s (- 119m 49s) (6 75%) 1.7849\n",
      "Train 360m 41s (- 120m 13s) (6 75%) 1.8006\n",
      "Train 361m 52s (- 120m 37s) (6 75%) 1.8316\n",
      "Train 363m 3s (- 121m 1s) (6 75%) 1.7361\n",
      "Train 364m 14s (- 121m 24s) (6 75%) 1.7423\n",
      "Train 365m 26s (- 121m 48s) (6 75%) 1.7694\n",
      "Train 366m 37s (- 122m 12s) (6 75%) 1.7518\n",
      "Train 367m 48s (- 122m 36s) (6 75%) 1.7738\n",
      "Train 368m 59s (- 122m 59s) (6 75%) 1.7731\n",
      "Train 370m 11s (- 123m 23s) (6 75%) 1.7875\n",
      "Train 371m 22s (- 123m 47s) (6 75%) 1.7493\n",
      "Train 372m 33s (- 124m 11s) (6 75%) 1.7905\n",
      "Train 373m 45s (- 124m 35s) (6 75%) 1.7603\n",
      "Train 374m 56s (- 124m 58s) (6 75%) 1.8326\n",
      "Train 376m 8s (- 125m 22s) (6 75%) 1.8052\n",
      "Train 377m 19s (- 125m 46s) (6 75%) 1.7496\n",
      "Train 378m 30s (- 126m 10s) (6 75%) 1.8340\n",
      "Train 379m 42s (- 126m 34s) (6 75%) 1.8312\n",
      "Train 380m 54s (- 126m 58s) (6 75%) 1.7924\n",
      "Train 382m 5s (- 127m 21s) (6 75%) 1.7939\n",
      "Train 383m 17s (- 127m 45s) (6 75%) 1.8122\n",
      "Train 384m 29s (- 128m 9s) (6 75%) 1.8333\n",
      "Train 385m 40s (- 128m 33s) (6 75%) 1.7743\n",
      "Train 386m 51s (- 128m 57s) (6 75%) 1.8105\n",
      "Train 388m 3s (- 129m 21s) (6 75%) 1.8524\n",
      "Train 389m 15s (- 129m 45s) (6 75%) 1.8400\n",
      "Train 390m 27s (- 130m 9s) (6 75%) 1.8387\n",
      "Train 391m 39s (- 130m 33s) (6 75%) 1.8759\n",
      "Train 392m 51s (- 130m 57s) (6 75%) 1.8131\n",
      "Train 394m 3s (- 131m 21s) (6 75%) 1.7722\n",
      "Train 395m 14s (- 131m 44s) (6 75%) 1.8494\n",
      "Train 396m 26s (- 132m 8s) (6 75%) 1.7939\n",
      "Train 397m 38s (- 132m 32s) (6 75%) 1.8441\n",
      "Train 398m 49s (- 132m 56s) (6 75%) 1.7574\n",
      "Train 400m 0s (- 133m 20s) (6 75%) 1.8248\n",
      "Train 401m 11s (- 133m 43s) (6 75%) 1.7390\n",
      "Train 402m 23s (- 134m 7s) (6 75%) 1.8130\n",
      "Train 403m 34s (- 134m 31s) (6 75%) 1.8645\n",
      "Train 404m 45s (- 134m 55s) (6 75%) 1.7584\n",
      "Train 405m 56s (- 135m 18s) (6 75%) 1.8170\n",
      "Train 407m 8s (- 135m 42s) (6 75%) 1.8135\n",
      "Train 408m 19s (- 136m 6s) (6 75%) 1.8172\n",
      "Train 409m 32s (- 136m 30s) (6 75%) 1.8908\n",
      "Train 410m 43s (- 136m 54s) (6 75%) 1.7683\n",
      "Train 411m 54s (- 137m 18s) (6 75%) 1.7699\n",
      "Train 413m 6s (- 137m 42s) (6 75%) 1.8064\n",
      "Train 414m 17s (- 138m 5s) (6 75%) 1.7964\n",
      "Train 415m 29s (- 138m 29s) (6 75%) 1.8373\n",
      "Train 416m 40s (- 138m 53s) (6 75%) 1.7876\n",
      "Train 417m 52s (- 139m 17s) (6 75%) 1.7790\n",
      "Train 419m 3s (- 139m 41s) (6 75%) 1.7836\n",
      "Train 420m 15s (- 140m 5s) (6 75%) 1.8091\n",
      "Train 421m 26s (- 140m 28s) (6 75%) 1.7749\n",
      "Train 422m 37s (- 140m 52s) (6 75%) 1.7662\n",
      "Train 423m 49s (- 141m 16s) (6 75%) 1.8236\n",
      "Train 425m 0s (- 141m 40s) (6 75%) 1.7823\n",
      "Train 426m 11s (- 142m 3s) (6 75%) 1.8364\n",
      "Train 427m 23s (- 142m 27s) (6 75%) 1.8341\n",
      "Train 428m 34s (- 142m 51s) (6 75%) 1.7222\n",
      "Val 428m 36s (- 142m 52s) (6 75%) 0.6265\n",
      "Train 429m 1s (- 61m 17s) (7 87%) 0.0668\n",
      "Train 430m 13s (- 61m 27s) (7 87%) 1.7442\n",
      "Train 431m 25s (- 61m 37s) (7 87%) 1.6947\n",
      "Train 432m 36s (- 61m 48s) (7 87%) 1.7126\n",
      "Train 433m 48s (- 61m 58s) (7 87%) 1.7159\n",
      "Train 434m 59s (- 62m 8s) (7 87%) 1.7420\n",
      "Train 436m 10s (- 62m 18s) (7 87%) 1.7388\n",
      "Train 437m 22s (- 62m 28s) (7 87%) 1.6956\n",
      "Train 438m 33s (- 62m 39s) (7 87%) 1.7781\n",
      "Train 439m 45s (- 62m 49s) (7 87%) 1.7450\n",
      "Train 440m 57s (- 62m 59s) (7 87%) 1.7704\n",
      "Train 442m 8s (- 63m 9s) (7 87%) 1.7723\n",
      "Train 443m 19s (- 63m 19s) (7 87%) 1.7438\n",
      "Train 444m 30s (- 63m 30s) (7 87%) 1.7450\n",
      "Train 445m 41s (- 63m 40s) (7 87%) 1.7017\n",
      "Train 446m 53s (- 63m 50s) (7 87%) 1.7729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 448m 4s (- 64m 0s) (7 87%) 1.7244\n",
      "Train 449m 15s (- 64m 10s) (7 87%) 1.7989\n",
      "Train 450m 26s (- 64m 20s) (7 87%) 1.6989\n",
      "Train 451m 37s (- 64m 31s) (7 87%) 1.7137\n",
      "Train 452m 49s (- 64m 41s) (7 87%) 1.7732\n",
      "Train 454m 0s (- 64m 51s) (7 87%) 1.7519\n",
      "Train 455m 11s (- 65m 1s) (7 87%) 1.7506\n",
      "Train 456m 22s (- 65m 11s) (7 87%) 1.7273\n",
      "Train 457m 33s (- 65m 21s) (7 87%) 1.7045\n",
      "Train 458m 45s (- 65m 32s) (7 87%) 1.7693\n",
      "Train 459m 56s (- 65m 42s) (7 87%) 1.7036\n",
      "Train 461m 8s (- 65m 52s) (7 87%) 1.7842\n",
      "Train 462m 19s (- 66m 2s) (7 87%) 1.7607\n",
      "Train 463m 31s (- 66m 13s) (7 87%) 1.7952\n",
      "Train 464m 42s (- 66m 23s) (7 87%) 1.7550\n",
      "Train 465m 54s (- 66m 33s) (7 87%) 1.8129\n",
      "Train 467m 5s (- 66m 43s) (7 87%) 1.7732\n",
      "Train 468m 16s (- 66m 53s) (7 87%) 1.7732\n",
      "Train 469m 27s (- 67m 3s) (7 87%) 1.7962\n",
      "Train 470m 39s (- 67m 14s) (7 87%) 1.8423\n",
      "Train 471m 51s (- 67m 24s) (7 87%) 1.8231\n",
      "Train 473m 3s (- 67m 34s) (7 87%) 1.8491\n",
      "Train 474m 15s (- 67m 45s) (7 87%) 1.7743\n",
      "Train 475m 26s (- 67m 55s) (7 87%) 1.8564\n",
      "Train 476m 38s (- 68m 5s) (7 87%) 1.7374\n",
      "Train 477m 49s (- 68m 15s) (7 87%) 1.8372\n",
      "Train 479m 1s (- 68m 25s) (7 87%) 1.7277\n",
      "Train 480m 13s (- 68m 36s) (7 87%) 1.8184\n",
      "Train 481m 24s (- 68m 46s) (7 87%) 1.7175\n",
      "Train 482m 35s (- 68m 56s) (7 87%) 1.7393\n",
      "Train 483m 46s (- 69m 6s) (7 87%) 1.6904\n",
      "Train 484m 58s (- 69m 16s) (7 87%) 1.8491\n",
      "Train 486m 10s (- 69m 27s) (7 87%) 1.7626\n",
      "Train 487m 22s (- 69m 37s) (7 87%) 1.8906\n",
      "Train 488m 33s (- 69m 47s) (7 87%) 1.7910\n",
      "Train 489m 45s (- 69m 57s) (7 87%) 1.7873\n",
      "Train 490m 56s (- 70m 8s) (7 87%) 1.7322\n",
      "Train 492m 7s (- 70m 18s) (7 87%) 1.7498\n",
      "Train 493m 18s (- 70m 28s) (7 87%) 1.6536\n",
      "Train 494m 30s (- 70m 38s) (7 87%) 1.7469\n",
      "Train 495m 41s (- 70m 48s) (7 87%) 1.7800\n",
      "Train 496m 53s (- 70m 59s) (7 87%) 1.7666\n",
      "Train 498m 4s (- 71m 9s) (7 87%) 1.8079\n",
      "Train 499m 15s (- 71m 19s) (7 87%) 1.7483\n",
      "Train 500m 26s (- 71m 29s) (7 87%) 1.6815\n",
      "Val 500m 28s (- 71m 29s) (7 87%) 0.6377\n",
      "Train 500m 53s (- 0m 0s) (8 100%) 0.0414\n",
      "Train 502m 5s (- 0m 0s) (8 100%) 1.7136\n",
      "Train 503m 16s (- 0m 0s) (8 100%) 1.7299\n",
      "Train 504m 28s (- 0m 0s) (8 100%) 1.7388\n",
      "Train 505m 40s (- 0m 0s) (8 100%) 1.8012\n",
      "Train 506m 51s (- 0m 0s) (8 100%) 1.7314\n",
      "Train 508m 2s (- 0m 0s) (8 100%) 1.6744\n",
      "Train 509m 14s (- 0m 0s) (8 100%) 1.7042\n",
      "Train 510m 25s (- 0m 0s) (8 100%) 1.6918\n",
      "Train 511m 37s (- 0m 0s) (8 100%) 1.7532\n",
      "Train 512m 49s (- 0m 0s) (8 100%) 1.7853\n",
      "Train 514m 0s (- 0m 0s) (8 100%) 1.7127\n",
      "Train 515m 12s (- 0m 0s) (8 100%) 1.7698\n",
      "Train 516m 24s (- 0m 0s) (8 100%) 1.8124\n",
      "Train 517m 35s (- 0m 0s) (8 100%) 1.7037\n",
      "Train 518m 47s (- 0m 0s) (8 100%) 1.7442\n",
      "Train 519m 58s (- 0m 0s) (8 100%) 1.6939\n",
      "Train 521m 10s (- 0m 0s) (8 100%) 1.6930\n",
      "Train 522m 21s (- 0m 0s) (8 100%) 1.6268\n",
      "Train 523m 32s (- 0m 0s) (8 100%) 1.6948\n",
      "Train 524m 43s (- 0m 0s) (8 100%) 1.7051\n",
      "Train 525m 55s (- 0m 0s) (8 100%) 1.7359\n",
      "Train 527m 7s (- 0m 0s) (8 100%) 1.7443\n",
      "Train 528m 18s (- 0m 0s) (8 100%) 1.7662\n",
      "Train 529m 30s (- 0m 0s) (8 100%) 1.7134\n",
      "Train 530m 41s (- 0m 0s) (8 100%) 1.7647\n",
      "Train 531m 52s (- 0m 0s) (8 100%) 1.7352\n",
      "Train 533m 3s (- 0m 0s) (8 100%) 1.7263\n",
      "Train 534m 15s (- 0m 0s) (8 100%) 1.7724\n",
      "Train 535m 26s (- 0m 0s) (8 100%) 1.7243\n",
      "Train 536m 38s (- 0m 0s) (8 100%) 1.7018\n",
      "Train 537m 49s (- 0m 0s) (8 100%) 1.7854\n",
      "Train 539m 1s (- 0m 0s) (8 100%) 1.7388\n",
      "Train 540m 12s (- 0m 0s) (8 100%) 1.7009\n",
      "Train 541m 24s (- 0m 0s) (8 100%) 1.7559\n",
      "Train 542m 35s (- 0m 0s) (8 100%) 1.6858\n",
      "Train 543m 47s (- 0m 0s) (8 100%) 1.7519\n",
      "Train 544m 58s (- 0m 0s) (8 100%) 1.7654\n",
      "Train 546m 9s (- 0m 0s) (8 100%) 1.6726\n",
      "Train 547m 20s (- 0m 0s) (8 100%) 1.7324\n",
      "Train 548m 32s (- 0m 0s) (8 100%) 1.7353\n",
      "Train 549m 44s (- 0m 0s) (8 100%) 1.7784\n",
      "Train 550m 55s (- 0m 0s) (8 100%) 1.7188\n",
      "Train 552m 6s (- 0m 0s) (8 100%) 1.7288\n",
      "Train 553m 18s (- 0m 0s) (8 100%) 1.7432\n",
      "Train 554m 29s (- 0m 0s) (8 100%) 1.7317\n",
      "Train 555m 41s (- 0m 0s) (8 100%) 1.7945\n",
      "Train 556m 52s (- 0m 0s) (8 100%) 1.7241\n",
      "Train 558m 4s (- 0m 0s) (8 100%) 1.7269\n",
      "Train 559m 15s (- 0m 0s) (8 100%) 1.7315\n",
      "Train 560m 26s (- 0m 0s) (8 100%) 1.7640\n",
      "Train 561m 38s (- 0m 0s) (8 100%) 1.8266\n",
      "Train 562m 49s (- 0m 0s) (8 100%) 1.6456\n",
      "Train 563m 59s (- 0m 0s) (8 100%) 1.6909\n",
      "Train 565m 11s (- 0m 0s) (8 100%) 1.7117\n",
      "Train 566m 22s (- 0m 0s) (8 100%) 1.6710\n",
      "Train 567m 33s (- 0m 0s) (8 100%) 1.7559\n",
      "Train 568m 44s (- 0m 0s) (8 100%) 1.6711\n",
      "Train 569m 55s (- 0m 0s) (8 100%) 1.7914\n",
      "Train 571m 7s (- 0m 0s) (8 100%) 1.7557\n",
      "Train 572m 18s (- 0m 0s) (8 100%) 1.7118\n",
      "Val 572m 20s (- 0m 0s) (8 100%) 0.6045\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(hidden_size).to(device)\n",
    "decoder1 = AttnDecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "plotloss_t1, plotloss_v1 = trainIters(encoder1, decoder1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "\n",
    "encoder3 = EncoderRNN(hidden_size).to(device)\n",
    "decoder3 = AttnDecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "#encoder3.load_state_dict(torch.load(model_path + \"encoder_rnn_attn3008.pth\"))\n",
    "#decoder3.load_state_dict(torch.load(model_path + \"decoder_rnn_attn3008.pth\"))\n",
    "\n",
    "encoder3.load_state_dict(torch.load(model_path + \"encoder_rnn_atten.pth\"))\n",
    "decoder3.load_state_dict(torch.load(model_path + \"decoder_rnn_atten.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, encoder, decoder):\n",
    "    decoded_words_list = []\n",
    "    decoder_attentions_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data1, data2, length1, length2) in enumerate(loader):\n",
    "            input_tensor = data1\n",
    "            input_length = input_tensor.size()[0]\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden(input_length)\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "            \n",
    "            decoder_input = torch.tensor(np.array([[SOS_token]]), device=device)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoded_words = []\n",
    "            decoder_attentions = torch.zeros(MAX_LENGTH_ZH, MAX_LENGTH_ZH)\n",
    "            \n",
    "            for di in range(MAX_LENGTH_EN):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.reshape(1,1), decoder_hidden, encoder_output)\n",
    "                decoder_attentions[di] = decoder_attention.data\n",
    "                topv, topi = decoder_output.data.topk(1) \n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_words.append('<eos>')\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_words.append(idx2words_ft_en[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = decoder_input.unsqueeze(0)\n",
    "                \n",
    "            decoded_words_list.append(decoded_words)\n",
    "            decoder_attentions_list.append(decoder_attentions[:di + 1])\n",
    "                   \n",
    "        return decoded_words_list, decoder_attentions_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list, attention_list = evaluate(val_loader2, encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_list_nopad = []\n",
    "#for ii in range(len(predicted_list)):\n",
    "    #line = ''\n",
    "    #for jj in predicted_list[ii]:\n",
    "        #if jj != '<pad>':\n",
    "            #line = line + ' ' + jj\n",
    "    #predicted_list_nopad.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for iii in range(len(predicted_list_nopad)):\n",
    "    #if predicted_list_nopad[iii][-5:] != '<eos>':\n",
    "        #predicted_list_nopad[iii] = predicted_list_nopad[iii] + ' <eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for validation dataset: 3.5405271581042284\n"
     ]
    }
   ],
   "source": [
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(val_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in val_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for validation dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_list = []\n",
    "#for iii in range(len(val_en_indexes_filtered)):\n",
    "    #line = ''\n",
    "    #for jjj in val_en_indexes_filtered[iii]:\n",
    "        #line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    #label_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = []\n",
    "for fp_item in predicted_list_nopad:\n",
    "    fp.append(fp_item[6:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('bleu score for validation dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = []\n",
    "for fl_item in label_list:\n",
    "    fl.append(fl_item[6:-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> There s a countries in the world .\n",
      "> Afghanistan looks so different from here in America . \n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list, attention_list = evaluate(test_loader, encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for test dataset: 4.0543943769984265\n"
     ]
    }
   ],
   "source": [
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(test_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in test_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_list_nopad = []\n",
    "#for ii in range(len(predicted_list)):\n",
    "    #line = ''\n",
    "    #for jj in predicted_list[ii]:\n",
    "        #if jj != '<pad>':\n",
    "            #line = line + ' ' + jj\n",
    "    #predicted_list_nopad.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for iii in range(len(predicted_list_nopad)):\n",
    "    #if predicted_list_nopad[iii][-5:] != '<eos>':\n",
    "        #predicted_list_nopad[iii] = predicted_list_nopad[iii] + ' <eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_list = []\n",
    "#for iii in range(len(test_en_indexes_filtered)):\n",
    "    #line = ''\n",
    "    #for jjj in test_en_indexes_filtered[iii]:\n",
    "        #line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    #label_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> We love love . . \n",
      "> We love innovation . \n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
