{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re  \n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from sacrebleu import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_zh = '/home/ys2542/Neural-Translation-System/iwslt-zh-en-processed/'\n",
    "path_vi = '/home/ys2542/Neural-Translation-System/iwslt-vi-en-processed/'\n",
    "ft_home = '/scratch/ys2542/NLP_FASTTEXT/'\n",
    "model_path = '/scratch/ys2542/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "UNK_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_load = 100000 \n",
    "EMBED_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_en = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_en = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_en = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_en = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_en[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = idx\n",
    "        idx2words_ft_en[idx] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ft_home + 'cc.vi.300.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_zh = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_zh = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_zh = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_zh = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_zh[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_zh[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_zh[s[0]] = idx\n",
    "        idx2words_ft_zh[idx] = s[0]\n",
    "        ordered_words_ft_zh.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_zh_train = open(path_vi + 'train.tok.vi', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_train = open(path_vi + 'train.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_val = open(path_vi + 'dev.tok.vi', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_val = open(path_vi + 'dev.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_test = open(path_vi + 'test.tok.vi', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_test = open(path_vi + 'test.tok.en', encoding = 'utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lines(lines, lang):\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "            line = ' '\n",
    "        if lang == 'en':\n",
    "            line = line.replace(\"&apos;\", \"\").replace(\"&quot;\", \"\")\n",
    "        if line[-1] != ' ':\n",
    "            line = line + ' '\n",
    "       \n",
    "        line = '<sos> ' + line + '<eos>'\n",
    "        data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh = clean_lines(lines_zh_train, 'vi')\n",
    "train_en = clean_lines(lines_en_train, 'en')\n",
    "\n",
    "val_zh = clean_lines(lines_zh_val, 'vi')\n",
    "val_en = clean_lines(lines_en_val, 'en')\n",
    "\n",
    "test_zh = clean_lines(lines_zh_test, 'vi')\n",
    "test_en = clean_lines(lines_en_test, 'en')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(data, lang):\n",
    "    indexes = []\n",
    "    for sentence in data:\n",
    "        index = []\n",
    "        for token in sentence.split():\n",
    "            if lang == 'vi':\n",
    "                try:\n",
    "                    index.append(words_ft_zh[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "            elif lang == 'en':\n",
    "                try:\n",
    "                    index.append(words_ft_en[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "        indexes.append(index)\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh_indexes = indexesFromSentence(train_zh, 'vi')\n",
    "train_en_indexes = indexesFromSentence(train_en, 'en')\n",
    "\n",
    "val_zh_indexes = indexesFromSentence(val_zh, 'vi')\n",
    "val_en_indexes = indexesFromSentence(val_en, 'en')\n",
    "\n",
    "test_zh_indexes = indexesFromSentence(test_zh, 'vi')\n",
    "test_en_indexes = indexesFromSentence(test_en, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "length_zh = []\n",
    "for line in train_zh_indexes:\n",
    "        length_zh.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_zh)\n",
    "MAX_LENGTH_ZH = length_zh[int(len(train_zh_indexes)*0.99)]\n",
    "print(MAX_LENGTH_ZH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in train_en_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(train_en_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in val_zh_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(val_zh_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in val_en_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(val_en_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in test_zh_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(test_zh_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "length_en = []\n",
    "for line in test_en_indexes:\n",
    "        length_en.append(len(line))\n",
    "        \n",
    "length_zh = sorted(length_en)\n",
    "MAX_LENGTH_EN = length_en[int(len(test_en_indexes)*0.99)]\n",
    "print(MAX_LENGTH_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH_ZH = 74\n",
    "MAX_LENGTH_EN = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zh_indexes_filtered = []\n",
    "train_en_indexes_filtered = []\n",
    "for i in range(len(train_zh_indexes)):\n",
    "    if len(train_zh_indexes[i]) <= MAX_LENGTH_ZH and len(train_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        train_zh_indexes_filtered.append(train_zh_indexes[i])\n",
    "        train_en_indexes_filtered.append(train_en_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_zh_indexes_filtered = []\n",
    "val_en_indexes_filtered = []\n",
    "for i in range(len(val_zh_indexes)):\n",
    "    if len(val_zh_indexes[i]) <= MAX_LENGTH_ZH and len(val_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        val_zh_indexes_filtered.append(val_zh_indexes[i])\n",
    "        val_en_indexes_filtered.append(val_en_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zh_indexes_filtered = []\n",
    "test_en_indexes_filtered = []\n",
    "for i in range(len(test_zh_indexes)):\n",
    "    if len(test_zh_indexes[i]) <= MAX_LENGTH_ZH and len(test_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        test_zh_indexes_filtered.append(test_zh_indexes[i])\n",
    "        test_en_indexes_filtered.append(test_en_indexes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    def __init__(self, data_list1, data_list2):\n",
    "        \n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        \n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "            \n",
    "    def __getitem__(self, key):        \n",
    "        return [self.data_list1[key], self.data_list2[key], len(self.data_list1[key]), len(self.data_list2[key])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "        \n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_LENGTH_ZH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_LENGTH_EN-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        data_list1.append(padded_vec1[:MAX_LENGTH_ZH])\n",
    "        data_list2.append(padded_vec2[:MAX_LENGTH_EN])\n",
    "\n",
    "\n",
    "    return [torch.from_numpy(np.array(data_list1)).cuda(), torch.from_numpy(np.array(data_list2)).cuda(),\n",
    "                torch.LongTensor(length_list1).cuda(), torch.LongTensor(length_list2).cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = VocabDataset(train_zh_indexes_filtered, train_en_indexes_filtered)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_zh_indexes_filtered, val_en_indexes_filtered)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = VocabDataset(test_zh_indexes_filtered, test_en_indexes_filtered)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "val_loader2 = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size=EMBED_SIZE):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_zh).float(), freeze=True)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH_ZH, embed_size=EMBED_SIZE):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_en).float(), freeze=True)\n",
    "        self.attn = nn.Linear(hidden_size + embed_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size + embed_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded, hidden), 2)), dim=2)\n",
    "        attn_applied = torch.bmm(attn_weights[0].unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size, input_length = input_tensor.size()\n",
    "    _, target_length = target_tensor.size()\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_token]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output)\n",
    "        \n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di].unsqueeze(0) \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output)\n",
    "                        \n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=100, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data1, data2, length1, length2) in enumerate(train_loader):\n",
    "            input_tensor = data1\n",
    "            target_tensor = data2\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                \n",
    "    torch.save(encoder.state_dict(), model_path + \"encoder_rnn_atten_vi.pth\")\n",
    "    torch.save(decoder.state_dict(), model_path + \"decoder_rnn_atten_vi.pth\")\n",
    "\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 1s (- 0m 4s) (1 20%) 0.1150\n",
      "1m 19s (- 5m 17s) (1 20%) 3.8997\n",
      "2m 37s (- 10m 31s) (1 20%) 3.0649\n",
      "3m 56s (- 15m 47s) (1 20%) 3.0374\n",
      "5m 15s (- 21m 2s) (1 20%) 2.9286\n",
      "6m 34s (- 26m 18s) (1 20%) 2.8891\n",
      "7m 53s (- 31m 32s) (1 20%) 2.9018\n",
      "9m 11s (- 36m 47s) (1 20%) 2.8604\n",
      "10m 31s (- 42m 4s) (1 20%) 2.8607\n",
      "11m 50s (- 47m 20s) (1 20%) 2.8460\n",
      "13m 9s (- 52m 38s) (1 20%) 2.7882\n",
      "14m 28s (- 57m 52s) (1 20%) 2.7800\n",
      "15m 47s (- 63m 9s) (1 20%) 2.8082\n",
      "17m 6s (- 68m 26s) (1 20%) 2.7821\n",
      "18m 25s (- 73m 42s) (1 20%) 2.7391\n",
      "19m 44s (- 78m 57s) (1 20%) 2.6882\n",
      "21m 3s (- 84m 13s) (1 20%) 2.6658\n",
      "22m 22s (- 89m 29s) (1 20%) 2.6730\n",
      "23m 40s (- 94m 43s) (1 20%) 2.6289\n",
      "25m 0s (- 100m 0s) (1 20%) 2.6840\n",
      "26m 19s (- 105m 16s) (1 20%) 2.5890\n",
      "27m 38s (- 110m 32s) (1 20%) 2.6255\n",
      "28m 57s (- 115m 49s) (1 20%) 2.6366\n",
      "30m 15s (- 121m 3s) (1 20%) 2.4967\n",
      "31m 34s (- 126m 19s) (1 20%) 2.5895\n",
      "32m 53s (- 131m 33s) (1 20%) 2.4800\n",
      "34m 12s (- 136m 49s) (1 20%) 2.5416\n",
      "35m 30s (- 142m 3s) (1 20%) 2.4835\n",
      "36m 49s (- 147m 18s) (1 20%) 2.4810\n",
      "38m 8s (- 152m 34s) (1 20%) 2.5108\n",
      "39m 27s (- 157m 49s) (1 20%) 2.4773\n",
      "40m 46s (- 163m 6s) (1 20%) 2.5348\n",
      "42m 5s (- 168m 21s) (1 20%) 2.4854\n",
      "43m 24s (- 173m 38s) (1 20%) 2.4741\n",
      "44m 43s (- 178m 54s) (1 20%) 2.4818\n",
      "46m 2s (- 184m 9s) (1 20%) 2.4487\n",
      "47m 21s (- 189m 25s) (1 20%) 2.4501\n",
      "48m 40s (- 194m 42s) (1 20%) 2.4548\n",
      "49m 7s (- 73m 41s) (2 40%) 0.8366\n",
      "50m 26s (- 75m 39s) (2 40%) 2.3397\n",
      "51m 45s (- 77m 38s) (2 40%) 2.3905\n",
      "53m 4s (- 79m 36s) (2 40%) 2.3610\n",
      "54m 23s (- 81m 35s) (2 40%) 2.3931\n",
      "55m 42s (- 83m 33s) (2 40%) 2.3493\n",
      "57m 0s (- 85m 31s) (2 40%) 2.3632\n",
      "58m 19s (- 87m 29s) (2 40%) 2.3527\n",
      "59m 38s (- 89m 27s) (2 40%) 2.2911\n",
      "60m 57s (- 91m 26s) (2 40%) 2.3431\n",
      "62m 16s (- 93m 24s) (2 40%) 2.3699\n",
      "63m 35s (- 95m 23s) (2 40%) 2.3923\n",
      "64m 54s (- 97m 21s) (2 40%) 2.3020\n",
      "66m 12s (- 99m 19s) (2 40%) 2.3348\n",
      "67m 31s (- 101m 17s) (2 40%) 2.2541\n",
      "68m 50s (- 103m 15s) (2 40%) 2.3479\n",
      "70m 8s (- 105m 13s) (2 40%) 2.2415\n",
      "71m 28s (- 107m 12s) (2 40%) 2.3282\n",
      "72m 47s (- 109m 10s) (2 40%) 2.3292\n",
      "74m 5s (- 111m 8s) (2 40%) 2.2745\n",
      "75m 24s (- 113m 7s) (2 40%) 2.3479\n",
      "76m 44s (- 115m 6s) (2 40%) 2.3035\n",
      "78m 2s (- 117m 4s) (2 40%) 2.3250\n",
      "79m 21s (- 119m 2s) (2 40%) 2.2630\n",
      "80m 40s (- 121m 0s) (2 40%) 2.2613\n",
      "81m 59s (- 122m 59s) (2 40%) 2.2422\n",
      "83m 18s (- 124m 57s) (2 40%) 2.3098\n",
      "84m 37s (- 126m 56s) (2 40%) 2.2250\n",
      "85m 56s (- 128m 54s) (2 40%) 2.2515\n",
      "87m 14s (- 130m 52s) (2 40%) 2.2252\n",
      "88m 33s (- 132m 50s) (2 40%) 2.1990\n",
      "89m 52s (- 134m 48s) (2 40%) 2.1722\n",
      "91m 11s (- 136m 46s) (2 40%) 2.2195\n",
      "92m 29s (- 138m 44s) (2 40%) 2.2100\n",
      "93m 48s (- 140m 43s) (2 40%) 2.2109\n",
      "95m 7s (- 142m 41s) (2 40%) 2.2152\n",
      "96m 26s (- 144m 40s) (2 40%) 2.2213\n",
      "97m 46s (- 146m 39s) (2 40%) 2.2985\n",
      "98m 13s (- 65m 29s) (3 60%) 0.7786\n",
      "99m 32s (- 66m 21s) (3 60%) 2.1037\n",
      "100m 51s (- 67m 14s) (3 60%) 2.1552\n",
      "102m 10s (- 68m 6s) (3 60%) 2.1865\n",
      "103m 28s (- 68m 59s) (3 60%) 2.0656\n",
      "104m 47s (- 69m 51s) (3 60%) 2.1349\n",
      "106m 6s (- 70m 44s) (3 60%) 2.0982\n",
      "107m 25s (- 71m 36s) (3 60%) 2.0965\n",
      "108m 44s (- 72m 29s) (3 60%) 2.2089\n",
      "110m 3s (- 73m 22s) (3 60%) 2.1580\n",
      "111m 23s (- 74m 15s) (3 60%) 2.1358\n",
      "112m 42s (- 75m 8s) (3 60%) 2.0779\n",
      "114m 2s (- 76m 1s) (3 60%) 2.1027\n",
      "115m 21s (- 76m 54s) (3 60%) 2.1136\n",
      "116m 41s (- 77m 47s) (3 60%) 2.1919\n",
      "118m 1s (- 78m 40s) (3 60%) 2.1255\n",
      "119m 20s (- 79m 33s) (3 60%) 2.1132\n",
      "120m 40s (- 80m 26s) (3 60%) 2.1336\n",
      "121m 59s (- 81m 19s) (3 60%) 2.1119\n",
      "123m 18s (- 82m 12s) (3 60%) 2.1109\n",
      "124m 37s (- 83m 5s) (3 60%) 2.0982\n",
      "125m 56s (- 83m 57s) (3 60%) 2.1132\n",
      "127m 14s (- 84m 49s) (3 60%) 2.0448\n",
      "128m 33s (- 85m 42s) (3 60%) 2.0709\n",
      "129m 52s (- 86m 34s) (3 60%) 2.1410\n",
      "131m 11s (- 87m 27s) (3 60%) 2.1267\n",
      "132m 29s (- 88m 19s) (3 60%) 2.0753\n",
      "133m 48s (- 89m 12s) (3 60%) 2.1374\n",
      "135m 7s (- 90m 5s) (3 60%) 2.0922\n",
      "136m 26s (- 90m 57s) (3 60%) 2.1160\n",
      "137m 45s (- 91m 50s) (3 60%) 2.0516\n",
      "139m 4s (- 92m 42s) (3 60%) 2.0831\n",
      "140m 23s (- 93m 35s) (3 60%) 2.1058\n",
      "141m 41s (- 94m 27s) (3 60%) 2.0302\n",
      "143m 0s (- 95m 20s) (3 60%) 2.0110\n",
      "144m 19s (- 96m 12s) (3 60%) 2.1138\n",
      "145m 38s (- 97m 5s) (3 60%) 2.1110\n",
      "146m 57s (- 97m 58s) (3 60%) 2.1343\n",
      "147m 24s (- 36m 51s) (4 80%) 0.6872\n",
      "148m 43s (- 37m 10s) (4 80%) 2.0903\n",
      "150m 2s (- 37m 30s) (4 80%) 1.9420\n",
      "151m 20s (- 37m 50s) (4 80%) 1.8961\n",
      "152m 39s (- 38m 9s) (4 80%) 2.0132\n",
      "153m 57s (- 38m 29s) (4 80%) 1.9566\n",
      "155m 16s (- 38m 49s) (4 80%) 2.0070\n",
      "156m 35s (- 39m 8s) (4 80%) 2.0226\n",
      "157m 54s (- 39m 28s) (4 80%) 1.9693\n",
      "159m 13s (- 39m 48s) (4 80%) 1.9876\n",
      "160m 31s (- 40m 7s) (4 80%) 1.9622\n",
      "161m 50s (- 40m 27s) (4 80%) 2.0023\n",
      "163m 9s (- 40m 47s) (4 80%) 1.9914\n",
      "164m 28s (- 41m 7s) (4 80%) 1.9753\n",
      "165m 47s (- 41m 26s) (4 80%) 2.0389\n",
      "167m 6s (- 41m 46s) (4 80%) 2.0365\n",
      "168m 25s (- 42m 6s) (4 80%) 2.0329\n",
      "169m 44s (- 42m 26s) (4 80%) 1.9859\n",
      "171m 2s (- 42m 45s) (4 80%) 1.9406\n",
      "172m 21s (- 43m 5s) (4 80%) 2.0062\n",
      "173m 40s (- 43m 25s) (4 80%) 2.0315\n",
      "174m 59s (- 43m 44s) (4 80%) 2.0408\n",
      "176m 17s (- 44m 4s) (4 80%) 1.9950\n",
      "177m 36s (- 44m 24s) (4 80%) 1.9959\n",
      "178m 55s (- 44m 43s) (4 80%) 2.0493\n",
      "180m 14s (- 45m 3s) (4 80%) 1.9814\n",
      "181m 32s (- 45m 23s) (4 80%) 1.9125\n",
      "182m 51s (- 45m 42s) (4 80%) 1.9564\n",
      "184m 9s (- 46m 2s) (4 80%) 1.9555\n",
      "185m 28s (- 46m 22s) (4 80%) 1.9823\n",
      "186m 47s (- 46m 41s) (4 80%) 1.9371\n",
      "188m 6s (- 47m 1s) (4 80%) 2.0255\n",
      "189m 25s (- 47m 21s) (4 80%) 2.0447\n",
      "190m 44s (- 47m 41s) (4 80%) 2.0431\n",
      "192m 2s (- 48m 0s) (4 80%) 1.9325\n",
      "193m 21s (- 48m 20s) (4 80%) 1.9868\n",
      "194m 39s (- 48m 39s) (4 80%) 1.9140\n",
      "195m 58s (- 48m 59s) (4 80%) 2.0090\n",
      "196m 26s (- 0m 0s) (5 100%) 0.6753\n",
      "197m 44s (- 0m 0s) (5 100%) 1.8761\n",
      "199m 3s (- 0m 0s) (5 100%) 1.9324\n",
      "200m 22s (- 0m 0s) (5 100%) 1.9566\n",
      "201m 41s (- 0m 0s) (5 100%) 1.9190\n",
      "203m 0s (- 0m 0s) (5 100%) 1.9812\n",
      "204m 18s (- 0m 0s) (5 100%) 1.8200\n",
      "205m 37s (- 0m 0s) (5 100%) 1.9236\n",
      "206m 56s (- 0m 0s) (5 100%) 1.9129\n",
      "208m 15s (- 0m 0s) (5 100%) 1.8610\n",
      "209m 34s (- 0m 0s) (5 100%) 1.9627\n",
      "210m 52s (- 0m 0s) (5 100%) 1.8462\n",
      "212m 11s (- 0m 0s) (5 100%) 1.8963\n",
      "213m 30s (- 0m 0s) (5 100%) 1.9173\n",
      "214m 49s (- 0m 0s) (5 100%) 1.9308\n",
      "216m 8s (- 0m 0s) (5 100%) 1.9430\n",
      "217m 27s (- 0m 0s) (5 100%) 2.0181\n",
      "218m 46s (- 0m 0s) (5 100%) 1.9196\n",
      "220m 6s (- 0m 0s) (5 100%) 1.9065\n",
      "221m 25s (- 0m 0s) (5 100%) 1.9786\n",
      "222m 44s (- 0m 0s) (5 100%) 1.8956\n",
      "224m 3s (- 0m 0s) (5 100%) 1.9180\n",
      "225m 22s (- 0m 0s) (5 100%) 1.8889\n",
      "226m 41s (- 0m 0s) (5 100%) 1.9199\n",
      "228m 0s (- 0m 0s) (5 100%) 1.9350\n",
      "229m 19s (- 0m 0s) (5 100%) 1.8811\n",
      "230m 38s (- 0m 0s) (5 100%) 1.9763\n",
      "231m 57s (- 0m 0s) (5 100%) 1.8530\n",
      "233m 15s (- 0m 0s) (5 100%) 1.8692\n",
      "234m 35s (- 0m 0s) (5 100%) 1.9624\n",
      "235m 53s (- 0m 0s) (5 100%) 1.8728\n",
      "237m 12s (- 0m 0s) (5 100%) 1.9137\n",
      "238m 31s (- 0m 0s) (5 100%) 1.8851\n",
      "239m 51s (- 0m 0s) (5 100%) 1.9742\n",
      "241m 10s (- 0m 0s) (5 100%) 1.9388\n",
      "242m 29s (- 0m 0s) (5 100%) 1.9501\n",
      "243m 49s (- 0m 0s) (5 100%) 1.9404\n",
      "245m 8s (- 0m 0s) (5 100%) 1.9455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11503860082381812,\n",
       " 3.899672348804961,\n",
       " 3.064934205275315,\n",
       " 3.0373861166147096,\n",
       " 2.928551674867288,\n",
       " 2.889070035494291,\n",
       " 2.901811183049128,\n",
       " 2.860360891880133,\n",
       " 2.860694850041315,\n",
       " 2.8460226772993042,\n",
       " 2.788154559013171,\n",
       " 2.7799516198573953,\n",
       " 2.808169641739284,\n",
       " 2.7821068220872136,\n",
       " 2.7391039236997945,\n",
       " 2.6881531368157807,\n",
       " 2.6657848945030804,\n",
       " 2.67299201574081,\n",
       " 2.628865657708584,\n",
       " 2.68402099218124,\n",
       " 2.589001822838416,\n",
       " 2.625510895557893,\n",
       " 2.6366046885955026,\n",
       " 2.4966913233047876,\n",
       " 2.5894644967103617,\n",
       " 2.4799841073843156,\n",
       " 2.541611396593924,\n",
       " 2.483535381219326,\n",
       " 2.481041050446339,\n",
       " 2.5108203966189655,\n",
       " 2.477260568080805,\n",
       " 2.534766681377705,\n",
       " 2.4854383282783714,\n",
       " 2.474125690949268,\n",
       " 2.481773545680902,\n",
       " 2.4486713644174425,\n",
       " 2.4501093898675377,\n",
       " 2.4548056519337185,\n",
       " 0.8366013629619893,\n",
       " 2.3397327745877776,\n",
       " 2.390543434925568,\n",
       " 2.361044951218825,\n",
       " 2.3931337063129137,\n",
       " 2.349304383106721,\n",
       " 2.3631850961538463,\n",
       " 2.352697652180989,\n",
       " 2.2910647954696266,\n",
       " 2.3430681570982315,\n",
       " 2.3699141947428393,\n",
       " 2.3923045681684454,\n",
       " 2.3019558647351404,\n",
       " 2.334838084685497,\n",
       " 2.254104955624311,\n",
       " 2.347922095274314,\n",
       " 2.241517593188163,\n",
       " 2.3281567402375054,\n",
       " 2.3292247654841494,\n",
       " 2.2744567695030797,\n",
       " 2.3478886491824422,\n",
       " 2.303464225377792,\n",
       " 2.3249975713094075,\n",
       " 2.2630002740713278,\n",
       " 2.2612974391839447,\n",
       " 2.242183221670298,\n",
       " 2.309842175214719,\n",
       " 2.2249782317723983,\n",
       " 2.251529932266626,\n",
       " 2.225222310775365,\n",
       " 2.198978722401155,\n",
       " 2.172217325063853,\n",
       " 2.219460436503091,\n",
       " 2.210014262077136,\n",
       " 2.210861914219,\n",
       " 2.2151634470621735,\n",
       " 2.221300354003906,\n",
       " 2.298454910669571,\n",
       " 0.778566653911884,\n",
       " 2.103678841224084,\n",
       " 2.1552190399169917,\n",
       " 2.1864922880515074,\n",
       " 2.065573055560773,\n",
       " 2.134891734000965,\n",
       " 2.0982279469416683,\n",
       " 2.09649910951272,\n",
       " 2.208886079054613,\n",
       " 2.158033914810573,\n",
       " 2.1358409470778255,\n",
       " 2.0778670149583083,\n",
       " 2.102691310980381,\n",
       " 2.113625806172688,\n",
       " 2.1918745031112277,\n",
       " 2.1255084306765832,\n",
       " 2.1132107856946116,\n",
       " 2.133593443846093,\n",
       " 2.1119468356401496,\n",
       " 2.1109117126464843,\n",
       " 2.098152677095854,\n",
       " 2.1131785906278178,\n",
       " 2.044828329819899,\n",
       " 2.070852124140812,\n",
       " 2.1410268812913165,\n",
       " 2.1266873687352885,\n",
       " 2.075348704411433,\n",
       " 2.137438379923502,\n",
       " 2.092231099055363,\n",
       " 2.116031995920033,\n",
       " 2.05163150982979,\n",
       " 2.0831100385616987,\n",
       " 2.1058386563032103,\n",
       " 2.0302348542824773,\n",
       " 2.010978003281813,\n",
       " 2.113821580348871,\n",
       " 2.11104505881285,\n",
       " 2.1342582908043495,\n",
       " 0.6871644210815429,\n",
       " 2.090304380563589,\n",
       " 1.9420497356316986,\n",
       " 1.8960807751386601,\n",
       " 2.0132170623388044,\n",
       " 1.956613237429888,\n",
       " 2.0069710110395387,\n",
       " 2.0226441759940905,\n",
       " 1.9692519652537817,\n",
       " 1.9875615966014366,\n",
       " 1.9621885104057113,\n",
       " 2.0022897739899466,\n",
       " 1.991414329333182,\n",
       " 1.9753265997079703,\n",
       " 2.0389219460120573,\n",
       " 2.0365376819708407,\n",
       " 2.032883353600135,\n",
       " 1.9859384849743968,\n",
       " 1.9406250880314746,\n",
       " 2.0061689396393603,\n",
       " 2.03153500190148,\n",
       " 2.040826102036697,\n",
       " 1.99500404064472,\n",
       " 1.9959452770917843,\n",
       " 2.0493369674682618,\n",
       " 1.981365037575747,\n",
       " 1.9124790768745619,\n",
       " 1.956354649861654,\n",
       " 1.9554655789106312,\n",
       " 1.9823470736772586,\n",
       " 1.9371209609202855,\n",
       " 2.0255497819949424,\n",
       " 2.04467302371294,\n",
       " 2.043101793924967,\n",
       " 1.9324923569116845,\n",
       " 1.9868118002475836,\n",
       " 1.9139819971720378,\n",
       " 2.009045365162386,\n",
       " 0.6752657904991736,\n",
       " 1.876099467155261,\n",
       " 1.932401087834285,\n",
       " 1.95663981804481,\n",
       " 1.91900806818253,\n",
       " 1.9811668562277764,\n",
       " 1.8200018496391095,\n",
       " 1.9235710809169668,\n",
       " 1.9128562848995885,\n",
       " 1.8610181788909133,\n",
       " 1.9627145239023063,\n",
       " 1.8461712284577194,\n",
       " 1.8963247768695541,\n",
       " 1.9172981135050455,\n",
       " 1.930756231454703,\n",
       " 1.9429913300734296,\n",
       " 2.0181449557573377,\n",
       " 1.9196075928516885,\n",
       " 1.9064788916172128,\n",
       " 1.9785768567598776,\n",
       " 1.8955642768664251,\n",
       " 1.9180325376070462,\n",
       " 1.8888519492516156,\n",
       " 1.9199012550940882,\n",
       " 1.9350426786373822,\n",
       " 1.8810676496456824,\n",
       " 1.9763122646625226,\n",
       " 1.8529749982784944,\n",
       " 1.869229385180352,\n",
       " 1.9624127158140525,\n",
       " 1.872778158921462,\n",
       " 1.913714126195663,\n",
       " 1.8850560740935498,\n",
       " 1.9741972155448706,\n",
       " 1.9387539614163911,\n",
       " 1.9500673841818783,\n",
       " 1.9404215025290465,\n",
       " 1.9454950939080649]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(hidden_size).to(device)\n",
    "decoder1 = AttnDecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "trainIters(encoder1, decoder1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "\n",
    "encoder2 = EncoderRNN(hidden_size).to(device)\n",
    "decoder2 = AttnDecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "encoder2.load_state_dict(torch.load(model_path + \"encoder_rnn_atten_vi.pth\"))\n",
    "decoder2.load_state_dict(torch.load(model_path + \"decoder_rnn_atten_vi.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, encoder, decoder):\n",
    "    decoded_words_list = []\n",
    "    decoder_attentions_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data1, data2, length1, length2) in enumerate(loader):\n",
    "            input_tensor = data1\n",
    "            input_length = input_tensor.size()[0]\n",
    "            \n",
    "            encoder_hidden = encoder.initHidden(input_length)\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "            \n",
    "            decoder_input = torch.tensor(np.array([[SOS_token]]), device=device)\n",
    "\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            decoded_words = []\n",
    "            decoder_attentions = torch.zeros(MAX_LENGTH_ZH, MAX_LENGTH_ZH)\n",
    "            \n",
    "            for di in range(MAX_LENGTH_EN):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.reshape(1,1), decoder_hidden, encoder_output)\n",
    "                decoder_attentions[di] = decoder_attention.data\n",
    "                topv, topi = decoder_output.data.topk(1) \n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_words.append('<eos>')\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_words.append(idx2words_ft_en[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = decoder_input.unsqueeze(0)\n",
    "                \n",
    "            decoded_words_list.append(decoded_words)\n",
    "            decoder_attentions_list.append(decoder_attentions[:di + 1])\n",
    "                   \n",
    "        return decoded_words_list, decoder_attentions_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list, attention_list = evaluate(val_loader2, encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for validation dataset: 6.8769534635183955\n"
     ]
    }
   ],
   "source": [
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(val_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in val_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for validation dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> And I didn t t t I I I saw that I , very , very .\n",
      "> I didn t know what it meant , but I could see that my father was very , very happy . \n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_list, attention_list = evaluate(test_loader, encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for test dataset: 7.376933713146011\n"
     ]
    }
   ],
   "source": [
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(test_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in test_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> After the in the , , , , , , the .\n",
      "> After three months in a refugee camp , we landed in Melbourne . \n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iii in range(len(predicted_list_nopad)):\n",
    "#     if predicted_list_nopad[iii][-5:] != '<eos>':\n",
    "#         predicted_list_nopad[iii] = predicted_list_nopad[iii] + ' <eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = []\n",
    "# for iii in range(len(val_en_indexes_filtered)):\n",
    "#     line = ''\n",
    "#     for jjj in val_en_indexes_filtered[iii]:\n",
    "#         line = line + ' ' + idx2words_ft_en[jjj]\n",
    "#     label_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for validation dataset: 21.93806547688664\n"
     ]
    }
   ],
   "source": [
    "#print('bleu score for validation dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <sos> But if we them them it , if they , they they the the the the and and , , they they they they to . <eos>\n",
      " <sos> But when none of this is presented to them , if they re not shown how food affects the mind and the body , they blindly eat whatever the hell you put in front of them . <eos>\n"
     ]
    }
   ],
   "source": [
    "# choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "# print(predicted_list_nopad[choice])\n",
    "# print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_list, attention_list = evaluate(test_loader, encoder2, decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_list_nopad = []\n",
    "# for ii in range(len(predicted_list)):\n",
    "#     line = ''\n",
    "#     for jj in predicted_list[ii]:\n",
    "#         if jj != '<pad>':\n",
    "#             line = line + ' ' + jj\n",
    "#     predicted_list_nopad.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iii in range(len(predicted_list_nopad)):\n",
    "#     if predicted_list_nopad[iii][-5:] != '<eos>':\n",
    "#         predicted_list_nopad[iii] = predicted_list_nopad[iii] + ' <eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = []\n",
    "# for iii in range(len(test_en_indexes_filtered)):\n",
    "#     line = ''\n",
    "#     for jjj in test_en_indexes_filtered[iii]:\n",
    "#         line = line + ' ' + idx2words_ft_en[jjj]\n",
    "#     label_list.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for test dataset: 22.999052090280383\n"
     ]
    }
   ],
   "source": [
    "#print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <sos> Let me show you a a picture . <eos>\n",
      " <sos> And let me show you a simple example . <eos>\n"
     ]
    }
   ],
   "source": [
    "# choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "# print(predicted_list_nopad[choice])\n",
    "# print(label_list[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
