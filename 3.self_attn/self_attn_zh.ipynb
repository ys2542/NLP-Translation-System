{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re  \n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "import pickle as pkl\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_zh = '../iwslt-zh-en-processed/'\n",
    "path_vi = '../iwslt-vi-en-processed/'\n",
    "\n",
    "PAD_token = 0\n",
    "UNK_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "\n",
    "EMBED_SIZE = 300\n",
    "words_to_load = 100000\n",
    "\n",
    "with open('/scratch/xm576/wiki-news-300d-1M.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_en = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_en = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_en = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_en = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_en[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = idx\n",
    "        idx2words_ft_en[idx] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])\n",
    "\n",
    "with open('/scratch/xm576/cc.zh.300.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_zh = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_zh = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_zh = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_zh = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_zh[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_zh[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_zh[s[0]] = idx\n",
    "        idx2words_ft_zh[idx] = s[0]\n",
    "        ordered_words_ft_zh.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load/clean/filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_zh_train = open(path_zh + 'train.tok.zh', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_train = open(path_zh + 'train.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_val = open(path_zh + 'dev.tok.zh', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_val = open(path_zh + 'dev.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_test = open(path_zh + 'test.tok.zh', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_test = open(path_zh + 'test.tok.en', encoding = 'utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_lines(lines, lang):\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "            line = ' '\n",
    "        if lang == 'en':\n",
    "            line = line.replace(\"&apos;\", \"\").replace(\"&quot;\", \"\")\n",
    "        if line[-1] != ' ':\n",
    "            line = line + ' '\n",
    "       \n",
    "        line = '<sos> ' + line + '<eos>'\n",
    "        data.append(line)\n",
    "    return data\n",
    "\n",
    "train_zh = clean_lines(lines_zh_train, 'zh')\n",
    "train_en = clean_lines(lines_en_train, 'en')\n",
    "\n",
    "val_zh = clean_lines(lines_zh_val, 'zh')\n",
    "val_en = clean_lines(lines_en_val, 'en')\n",
    "\n",
    "test_zh = clean_lines(lines_zh_test, 'zh')\n",
    "test_en = clean_lines(lines_en_test, 'en')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(data, lang):\n",
    "    indexes = []\n",
    "    for sentence in data:\n",
    "        index = []\n",
    "        for token in sentence.split():\n",
    "            if lang == 'zh':\n",
    "                try:\n",
    "                    index.append(words_ft_zh[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "            elif lang == 'en':\n",
    "                try:\n",
    "                    index.append(words_ft_en[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "        indexes.append(index)\n",
    "    return indexes\n",
    "\n",
    "train_zh_indexes = indexesFromSentence(train_zh, 'zh')\n",
    "train_en_indexes = indexesFromSentence(train_en, 'en')\n",
    "\n",
    "val_zh_indexes = indexesFromSentence(val_zh, 'zh')\n",
    "val_en_indexes = indexesFromSentence(val_en, 'en')\n",
    "\n",
    "test_zh_indexes = indexesFromSentence(test_zh, 'zh')\n",
    "test_en_indexes = indexesFromSentence(test_en, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH_ZH = 69\n",
    "MAX_LENGTH_EN = 40\n",
    "\n",
    "train_zh_indexes_filtered = []\n",
    "train_en_indexes_filtered = []\n",
    "for i in range(len(train_zh_indexes)):\n",
    "    if len(train_zh_indexes[i]) <= MAX_LENGTH_ZH and len(train_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        train_zh_indexes_filtered.append(train_zh_indexes[i])\n",
    "        train_en_indexes_filtered.append(train_en_indexes[i])\n",
    "\n",
    "val_zh_indexes_filtered = []\n",
    "val_en_indexes_filtered = []\n",
    "for i in range(len(val_zh_indexes)):\n",
    "    if len(val_zh_indexes[i]) <= MAX_LENGTH_ZH and len(val_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        val_zh_indexes_filtered.append(val_zh_indexes[i])\n",
    "        val_en_indexes_filtered.append(val_en_indexes[i])\n",
    "\n",
    "test_zh_indexes_filtered = []\n",
    "test_en_indexes_filtered = []\n",
    "for i in range(len(test_zh_indexes)):\n",
    "    if len(test_zh_indexes[i]) <= MAX_LENGTH_ZH and len(test_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        test_zh_indexes_filtered.append(test_zh_indexes[i])\n",
    "        test_en_indexes_filtered.append(test_en_indexes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    def __init__(self, data_list1, data_list2):\n",
    "        \n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        \n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "            \n",
    "    def __getitem__(self, key):        \n",
    "        return [self.data_list1[key], self.data_list2[key], len(self.data_list1[key]), len(self.data_list2[key])]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "        \n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_LENGTH_ZH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_LENGTH_EN-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        data_list1.append(padded_vec1[:MAX_LENGTH_ZH])\n",
    "        data_list2.append(padded_vec2[:MAX_LENGTH_EN])\n",
    "\n",
    "\n",
    "    return [torch.from_numpy(np.array(data_list1)).to(device), torch.from_numpy(np.array(data_list2)).to(device),\n",
    "                torch.LongTensor(length_list1).to(device), torch.LongTensor(length_list2).to(device)]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = VocabDataset(train_zh_indexes_filtered, train_en_indexes_filtered)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_zh_indexes_filtered, val_en_indexes_filtered)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = VocabDataset(test_zh_indexes_filtered, test_en_indexes_filtered)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-attention based encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, EMBED_SIZE, dropout_p, MAX_LENGTH_ZH=MAX_LENGTH_ZH):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        pe = torch.zeros(MAX_LENGTH_ZH, EMBED_SIZE)\n",
    "        position = torch.arange(0.0, MAX_LENGTH_ZH).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0.0, EMBED_SIZE, 2)*-(math.log(10000.0)/EMBED_SIZE))\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + torch.Tensor(self.pe[:, :x.size(1)]).to(device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def attention(query, key, value, dropout=None): \n",
    "    \"\"\"\n",
    "    return weighted context vector and attention weights\n",
    "    \"\"\"\n",
    "    d = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1))/math.sqrt(d)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, EMBED_SIZE, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.d_k = EMBED_SIZE // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(EMBED_SIZE, EMBED_SIZE),4)\n",
    "        self.attn = None\n",
    "        self.drouput = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value): \n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "            \n",
    "        x, self.attn = attention(query, key, value)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "ff_size = 1200\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    apply full connected feed-forward network for each position\n",
    "    \"\"\"\n",
    "    def __init__(self, EMBED_SIZE, ff_size, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(EMBED_SIZE, ff_size)\n",
    "        self.w_2 = nn.Linear(ff_size, EMBED_SIZE)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    create N identical layers\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    embedding with pre-trained vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_zh).float(), freeze=False)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    layer normalization: ((X-mean)/(std+eps)) * A + B\n",
    "    \"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x-mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    In each layer, connect two sublayer with: x + layer(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    create one layer with two sublayers\n",
    "    \"\"\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout),2)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class selfAttnEncoder(nn.Module):\n",
    "    def __init__(self, encoder, input_embed):\n",
    "        super(selfAttnEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.input_embed = input_embed\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return self.encoder(self.input_embed(input_data))\n",
    "\n",
    "def make_model(N=6, EMBED_SIZE=EMBED_SIZE, ff_size=ff_size, h=6, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, EMBED_SIZE)\n",
    "    ff = PositionwiseFeedForward(EMBED_SIZE, ff_size, dropout)\n",
    "    position = PositionalEncoding(EMBED_SIZE, dropout)\n",
    "    model = selfAttnEncoder(\n",
    "            Encoder(EncoderLayer(EMBED_SIZE, c(attn), c(ff), dropout), N),\n",
    "            nn.Sequential(Embeddings(EMBED_SIZE), c(position))\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH_ZH, embed_size=EMBED_SIZE):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_zh).float(), freeze=False)\n",
    "        self.attn = nn.Linear(hidden_size + embed_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size + embed_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_data)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded, hidden), 2)), dim=2)\n",
    "        attn_applied = torch.bmm(attn_weights[0].unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 1.0\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size, input_length = input_tensor.size()\n",
    "    _, target_length = target_tensor.size()\n",
    "    \n",
    "    encoder_output = encoder.forward(input_tensor)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_token]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "    decoder_hidden = decoder.initHidden(batch_size)\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di].unsqueeze(0) \n",
    "            break \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output)               \n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])     \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, encoder, decoder):\n",
    "    losses = 0    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for i, (input_tensor, target_tensor, input_length, target_length) in enumerate(loader):\n",
    "        \n",
    "        encoder_output = encoder.forward(input_tensor)\n",
    "        decoder_input = torch.tensor(np.array([[SOS_token]]), device=device)\n",
    "        decoder_hidden = torch.zeros(1, 1, 300, device=device)\n",
    "        \n",
    "        loss = 0\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input.reshape(1,1), decoder_hidden, encoder_output)\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                break\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(0)\n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "        losses += loss/target_length\n",
    "    return losses.item()/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    val_losses = []\n",
    "    val_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data1, data2, length1, length2) in enumerate(train_loader):\n",
    "            input_tensor = data1\n",
    "            target_tensor = data2\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                val_loss = test_model(val_loader, encoder, decoder)\n",
    "                val_losses.append(val_loss)\n",
    "                print('%s (%d %d%%) %.4f %.4f' % (timeSince(start, iter / n_iters),\n",
    "                            iter, iter / n_iters * 100, plot_loss_avg, val_loss))\n",
    "                \n",
    "        torch.save(encoder.state_dict(), \"encoder_zh_attn_new2.pth\")\n",
    "        torch.save(decoder.state_dict(), \"decoder_zh_attn_new2.pth\")\n",
    "        \n",
    "        pkl.dump(plot_losses, open('zh_train_loss_new2.pickle','wb'))\n",
    "        pkl.dump(val_losses, open('zh_val_loss_new2.pickle','wb'))\n",
    "\n",
    "    return plot_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 13s (- 1m 7s) (1 16%) 0.0029 11.0000\n",
      "0m 47s (- 3m 59s) (1 16%) 2.0832 6.5507\n",
      "1m 25s (- 7m 5s) (1 16%) 1.7378 6.2935\n",
      "1m 59s (- 9m 56s) (1 16%) 1.4216 5.8478\n",
      "2m 35s (- 12m 58s) (1 16%) 1.6818 5.8297\n",
      "3m 11s (- 15m 58s) (1 16%) 1.5027 5.8804\n",
      "3m 47s (- 18m 59s) (1 16%) 1.6204 6.3913\n",
      "4m 21s (- 21m 49s) (1 16%) 1.3811 5.6667\n",
      "4m 59s (- 24m 56s) (1 16%) 1.7142 5.7464\n",
      "5m 35s (- 27m 59s) (1 16%) 1.5507 5.6449\n",
      "6m 11s (- 30m 59s) (1 16%) 1.5483 5.5870\n",
      "6m 49s (- 34m 5s) (1 16%) 1.6368 5.5399\n",
      "7m 23s (- 36m 57s) (1 16%) 1.3452 5.6232\n",
      "7m 56s (- 39m 43s) (1 16%) 1.2352 5.6667\n",
      "8m 31s (- 42m 36s) (1 16%) 1.3675 5.5833\n",
      "9m 9s (- 45m 48s) (1 16%) 1.7552 5.6377\n",
      "9m 46s (- 48m 51s) (1 16%) 1.5425 5.5725\n",
      "10m 24s (- 52m 0s) (1 16%) 1.7244 5.4529\n",
      "11m 0s (- 55m 4s) (1 16%) 1.5930 5.5906\n",
      "11m 37s (- 58m 5s) (1 16%) 1.5216 5.5580\n",
      "12m 15s (- 61m 16s) (1 16%) 1.6980 5.5072\n",
      "12m 48s (- 64m 3s) (1 16%) 1.2771 5.4891\n",
      "13m 25s (- 67m 9s) (1 16%) 1.6377 5.7391\n",
      "14m 1s (- 70m 6s) (1 16%) 1.4421 5.3587\n",
      "14m 33s (- 72m 49s) (1 16%) 1.2174 5.3152\n",
      "15m 10s (- 75m 50s) (1 16%) 1.5153 5.5109\n",
      "15m 44s (- 78m 44s) (1 16%) 1.3996 5.3261\n",
      "16m 23s (- 81m 56s) (1 16%) 1.7054 5.4783\n",
      "17m 0s (- 85m 4s) (1 16%) 1.6255 5.4312\n",
      "17m 36s (- 88m 0s) (1 16%) 1.4066 5.4819\n",
      "18m 12s (- 91m 1s) (1 16%) 1.4992 5.3080\n",
      "18m 46s (- 93m 52s) (1 16%) 1.3971 5.3623\n",
      "19m 23s (- 96m 59s) (1 16%) 1.5527 5.2355\n",
      "20m 0s (- 100m 3s) (1 16%) 1.5466 5.4601\n",
      "20m 36s (- 103m 3s) (1 16%) 1.4795 5.3551\n",
      "21m 13s (- 106m 7s) (1 16%) 1.5569 5.3333\n",
      "21m 50s (- 109m 11s) (1 16%) 1.5853 5.4275\n",
      "22m 24s (- 112m 4s) (1 16%) 1.3642 5.4275\n",
      "23m 0s (- 115m 4s) (1 16%) 1.4706 5.1123\n",
      "23m 35s (- 117m 59s) (1 16%) 1.3380 5.2428\n",
      "24m 10s (- 120m 54s) (1 16%) 1.3844 5.4457\n",
      "24m 44s (- 123m 42s) (1 16%) 1.2880 5.4275\n",
      "25m 20s (- 126m 43s) (1 16%) 1.4922 5.4493\n",
      "25m 58s (- 129m 50s) (1 16%) 1.6553 5.2790\n",
      "26m 30s (- 132m 30s) (1 16%) 1.1271 5.2536\n",
      "27m 7s (- 135m 37s) (1 16%) 1.6134 5.4167\n",
      "27m 43s (- 138m 39s) (1 16%) 1.4954 5.4348\n",
      "28m 17s (- 141m 28s) (1 16%) 1.2800 5.3587\n",
      "28m 51s (- 144m 16s) (1 16%) 1.3191 5.0471\n",
      "29m 25s (- 147m 9s) (1 16%) 1.3609 5.2464\n",
      "30m 1s (- 150m 7s) (1 16%) 1.4401 5.3514\n",
      "30m 37s (- 153m 7s) (1 16%) 1.4510 5.3188\n",
      "31m 14s (- 156m 10s) (1 16%) 1.5605 5.1848\n",
      "31m 48s (- 159m 2s) (1 16%) 1.3174 5.1993\n",
      "32m 21s (- 161m 47s) (1 16%) 1.1937 5.2572\n",
      "32m 55s (- 164m 37s) (1 16%) 1.2788 5.1812\n",
      "33m 31s (- 167m 39s) (1 16%) 1.4884 5.2826\n",
      "34m 8s (- 170m 44s) (1 16%) 1.5417 5.3116\n",
      "34m 43s (- 173m 35s) (1 16%) 1.2857 5.2428\n",
      "35m 18s (- 176m 30s) (1 16%) 1.3571 5.2283\n",
      "35m 55s (- 179m 35s) (1 16%) 1.5465 5.3696\n",
      "36m 8s (- 72m 16s) (2 33%) 0.0262 5.3877\n",
      "36m 44s (- 73m 28s) (2 33%) 1.4440 5.1486\n",
      "37m 18s (- 74m 37s) (2 33%) 1.2970 5.3514\n",
      "37m 51s (- 75m 43s) (2 33%) 1.1890 5.2246\n",
      "38m 28s (- 76m 57s) (2 33%) 1.4792 5.2464\n",
      "39m 6s (- 78m 12s) (2 33%) 1.5927 5.3696\n",
      "39m 41s (- 79m 22s) (2 33%) 1.3541 5.3623\n",
      "40m 14s (- 80m 29s) (2 33%) 1.2616 5.2210\n",
      "40m 49s (- 81m 39s) (2 33%) 1.3398 5.2899\n",
      "41m 25s (- 82m 51s) (2 33%) 1.4710 5.2826\n",
      "42m 3s (- 84m 6s) (2 33%) 1.5469 5.3696\n",
      "42m 39s (- 85m 18s) (2 33%) 1.4589 5.2174\n",
      "43m 15s (- 86m 31s) (2 33%) 1.4547 5.2572\n",
      "43m 51s (- 87m 42s) (2 33%) 1.3600 5.3768\n",
      "44m 25s (- 88m 51s) (2 33%) 1.3571 5.2572\n",
      "45m 4s (- 90m 8s) (2 33%) 1.6755 5.2862\n",
      "45m 40s (- 91m 21s) (2 33%) 1.5078 5.2754\n",
      "46m 17s (- 92m 35s) (2 33%) 1.4891 5.0217\n",
      "46m 50s (- 93m 41s) (2 33%) 1.1557 5.1812\n",
      "47m 27s (- 94m 55s) (2 33%) 1.5613 5.2717\n",
      "48m 5s (- 96m 10s) (2 33%) 1.5203 5.3551\n",
      "48m 39s (- 97m 18s) (2 33%) 1.2479 5.1775\n",
      "49m 15s (- 98m 30s) (2 33%) 1.4481 5.2681\n",
      "49m 50s (- 99m 40s) (2 33%) 1.3404 5.2029\n",
      "50m 28s (- 100m 57s) (2 33%) 1.6587 5.1630\n",
      "51m 3s (- 102m 7s) (2 33%) 1.3806 5.2500\n",
      "51m 41s (- 103m 23s) (2 33%) 1.5848 5.2500\n",
      "52m 17s (- 104m 35s) (2 33%) 1.4438 5.2536\n",
      "52m 56s (- 105m 53s) (2 33%) 1.6866 5.2717\n",
      "53m 33s (- 107m 6s) (2 33%) 1.4508 5.2862\n",
      "54m 8s (- 108m 17s) (2 33%) 1.4094 5.2790\n",
      "54m 44s (- 109m 28s) (2 33%) 1.3709 5.2862\n",
      "55m 18s (- 110m 37s) (2 33%) 1.3264 5.2681\n",
      "55m 53s (- 111m 47s) (2 33%) 1.3227 5.1630\n",
      "56m 30s (- 113m 0s) (2 33%) 1.4511 5.2500\n",
      "57m 6s (- 114m 12s) (2 33%) 1.4854 5.1486\n",
      "57m 42s (- 115m 24s) (2 33%) 1.3942 5.2174\n",
      "58m 18s (- 116m 37s) (2 33%) 1.4547 5.1739\n",
      "58m 52s (- 117m 44s) (2 33%) 1.1976 5.2174\n",
      "59m 25s (- 118m 51s) (2 33%) 1.2270 5.2138\n",
      "60m 2s (- 120m 4s) (2 33%) 1.4311 5.0797\n",
      "60m 38s (- 121m 17s) (2 33%) 1.4830 5.2283\n",
      "61m 14s (- 122m 28s) (2 33%) 1.3763 5.0906\n",
      "61m 50s (- 123m 41s) (2 33%) 1.4232 5.1884\n",
      "62m 25s (- 124m 51s) (2 33%) 1.3146 5.1884\n",
      "63m 3s (- 126m 7s) (2 33%) 1.6010 4.9167\n",
      "63m 38s (- 127m 16s) (2 33%) 1.3159 5.1159\n",
      "64m 14s (- 128m 28s) (2 33%) 1.3748 5.2609\n",
      "64m 49s (- 129m 38s) (2 33%) 1.3146 5.1739\n",
      "65m 25s (- 130m 51s) (2 33%) 1.4556 5.1667\n",
      "66m 1s (- 132m 3s) (2 33%) 1.4046 5.1630\n",
      "66m 36s (- 133m 13s) (2 33%) 1.3419 5.2572\n",
      "67m 10s (- 134m 20s) (2 33%) 1.2413 5.1630\n",
      "67m 46s (- 135m 32s) (2 33%) 1.3661 5.1558\n",
      "68m 20s (- 136m 40s) (2 33%) 1.2277 5.1159\n",
      "68m 56s (- 137m 52s) (2 33%) 1.4401 5.2174\n",
      "69m 27s (- 138m 54s) (2 33%) 0.9823 5.1884\n",
      "70m 2s (- 140m 4s) (2 33%) 1.3118 5.2826\n",
      "70m 38s (- 141m 16s) (2 33%) 1.4420 5.2572\n",
      "71m 15s (- 142m 31s) (2 33%) 1.5547 5.1957\n",
      "71m 50s (- 143m 41s) (2 33%) 1.3168 5.1739\n",
      "72m 4s (- 72m 4s) (3 50%) 0.0256 5.1884\n",
      "72m 42s (- 72m 42s) (3 50%) 1.6095 5.0616\n",
      "73m 18s (- 73m 18s) (3 50%) 1.4380 5.1957\n",
      "73m 55s (- 73m 55s) (3 50%) 1.4456 5.1812\n",
      "74m 34s (- 74m 34s) (3 50%) 1.6548 5.1993\n",
      "75m 11s (- 75m 11s) (3 50%) 1.4427 5.1884\n",
      "75m 46s (- 75m 46s) (3 50%) 1.3224 5.2101\n",
      "76m 21s (- 76m 21s) (3 50%) 1.3742 5.1159\n",
      "76m 58s (- 76m 58s) (3 50%) 1.4543 5.2174\n",
      "77m 34s (- 77m 34s) (3 50%) 1.4276 5.1884\n",
      "78m 10s (- 78m 10s) (3 50%) 1.4223 5.1630\n",
      "78m 43s (- 78m 43s) (3 50%) 1.1576 5.1630\n",
      "79m 17s (- 79m 17s) (3 50%) 1.2904 5.1087\n",
      "79m 53s (- 79m 53s) (3 50%) 1.3642 5.1630\n",
      "80m 30s (- 80m 30s) (3 50%) 1.5883 5.0942\n",
      "81m 4s (- 81m 4s) (3 50%) 1.2607 5.0942\n",
      "81m 39s (- 81m 39s) (3 50%) 1.2986 5.1268\n",
      "82m 13s (- 82m 13s) (3 50%) 1.2361 5.1232\n",
      "82m 47s (- 82m 47s) (3 50%) 1.2562 5.0580\n",
      "83m 22s (- 83m 22s) (3 50%) 1.2843 5.3007\n",
      "83m 58s (- 83m 58s) (3 50%) 1.4564 5.1920\n",
      "84m 34s (- 84m 34s) (3 50%) 1.4265 5.2174\n",
      "85m 13s (- 85m 13s) (3 50%) 1.6433 5.1051\n",
      "85m 49s (- 85m 49s) (3 50%) 1.3925 5.1522\n",
      "86m 26s (- 86m 26s) (3 50%) 1.5373 5.1993\n",
      "87m 0s (- 87m 0s) (3 50%) 1.2695 5.2101\n",
      "87m 35s (- 87m 35s) (3 50%) 1.3638 5.1812\n",
      "88m 11s (- 88m 11s) (3 50%) 1.3814 5.1377\n",
      "88m 47s (- 88m 47s) (3 50%) 1.4575 5.1848\n",
      "89m 20s (- 89m 20s) (3 50%) 1.1735 5.1304\n",
      "89m 58s (- 89m 58s) (3 50%) 1.6240 5.0145\n",
      "90m 30s (- 90m 30s) (3 50%) 1.0103 5.2101\n",
      "91m 4s (- 91m 4s) (3 50%) 1.2289 5.1920\n",
      "91m 42s (- 91m 42s) (3 50%) 1.5480 5.2065\n",
      "92m 18s (- 92m 18s) (3 50%) 1.4939 5.1630\n",
      "92m 53s (- 92m 53s) (3 50%) 1.2346 5.1884\n",
      "93m 30s (- 93m 30s) (3 50%) 1.4964 5.1594\n",
      "94m 7s (- 94m 7s) (3 50%) 1.5360 5.1739\n",
      "94m 44s (- 94m 44s) (3 50%) 1.4558 5.1449\n",
      "95m 20s (- 95m 20s) (3 50%) 1.4522 5.1196\n",
      "95m 54s (- 95m 54s) (3 50%) 1.2370 5.1341\n",
      "96m 29s (- 96m 29s) (3 50%) 1.3280 5.1667\n",
      "97m 5s (- 97m 5s) (3 50%) 1.3841 5.1558\n",
      "97m 40s (- 97m 40s) (3 50%) 1.2933 5.1051\n",
      "98m 15s (- 98m 15s) (3 50%) 1.3708 5.0217\n",
      "98m 52s (- 98m 52s) (3 50%) 1.5132 5.1341\n",
      "99m 29s (- 99m 29s) (3 50%) 1.4492 5.1159\n",
      "100m 3s (- 100m 3s) (3 50%) 1.2395 5.1739\n",
      "100m 40s (- 100m 40s) (3 50%) 1.4265 5.1667\n",
      "101m 16s (- 101m 16s) (3 50%) 1.4357 5.1486\n",
      "101m 53s (- 101m 53s) (3 50%) 1.4815 5.1449\n",
      "102m 27s (- 102m 27s) (3 50%) 1.2672 5.0870\n",
      "103m 3s (- 103m 3s) (3 50%) 1.3714 5.1413\n",
      "103m 38s (- 103m 38s) (3 50%) 1.2930 5.1196\n",
      "104m 14s (- 104m 14s) (3 50%) 1.3963 5.2210\n",
      "104m 49s (- 104m 49s) (3 50%) 1.3070 5.1739\n",
      "105m 27s (- 105m 27s) (3 50%) 1.5889 5.1957\n",
      "106m 4s (- 106m 4s) (3 50%) 1.5043 5.1522\n",
      "106m 39s (- 106m 39s) (3 50%) 1.2813 5.1123\n",
      "107m 13s (- 107m 13s) (3 50%) 1.2123 5.1486\n",
      "107m 45s (- 107m 45s) (3 50%) 1.0144 5.1413\n",
      "107m 58s (- 53m 59s) (4 66%) 0.0310 5.1159\n",
      "108m 36s (- 54m 18s) (4 66%) 1.5601 5.1268\n",
      "109m 13s (- 54m 36s) (4 66%) 1.4646 5.1957\n",
      "109m 49s (- 54m 54s) (4 66%) 1.3918 5.1739\n",
      "110m 25s (- 55m 12s) (4 66%) 1.3467 5.0906\n",
      "110m 59s (- 55m 29s) (4 66%) 1.2144 5.1486\n",
      "111m 34s (- 55m 47s) (4 66%) 1.3685 5.1812\n",
      "112m 11s (- 56m 5s) (4 66%) 1.4676 5.0761\n",
      "112m 46s (- 56m 23s) (4 66%) 1.2812 5.2138\n",
      "113m 23s (- 56m 41s) (4 66%) 1.4728 5.1739\n",
      "114m 0s (- 57m 0s) (4 66%) 1.4486 5.1848\n",
      "114m 35s (- 57m 17s) (4 66%) 1.2886 5.1558\n",
      "115m 13s (- 57m 36s) (4 66%) 1.6064 5.1522\n",
      "115m 48s (- 57m 54s) (4 66%) 1.2821 5.1449\n",
      "116m 25s (- 58m 12s) (4 66%) 1.4791 5.1268\n",
      "117m 0s (- 58m 30s) (4 66%) 1.2666 5.0942\n",
      "117m 35s (- 58m 47s) (4 66%) 1.3125 5.1014\n",
      "118m 11s (- 59m 5s) (4 66%) 1.3022 5.1051\n",
      "118m 46s (- 59m 23s) (4 66%) 1.2862 5.2500\n",
      "119m 24s (- 59m 42s) (4 66%) 1.4946 5.1196\n",
      "119m 58s (- 59m 59s) (4 66%) 1.2980 5.0833\n",
      "120m 33s (- 60m 16s) (4 66%) 1.2485 5.1123\n",
      "121m 8s (- 60m 34s) (4 66%) 1.3000 5.0942\n",
      "121m 43s (- 60m 51s) (4 66%) 1.2177 5.1486\n",
      "122m 15s (- 61m 7s) (4 66%) 1.0757 5.0978\n",
      "122m 50s (- 61m 25s) (4 66%) 1.2229 5.1449\n",
      "123m 28s (- 61m 44s) (4 66%) 1.6030 5.1232\n",
      "124m 4s (- 62m 2s) (4 66%) 1.3894 5.1413\n",
      "124m 40s (- 62m 20s) (4 66%) 1.4396 5.1630\n",
      "125m 18s (- 62m 39s) (4 66%) 1.4767 5.2246\n",
      "125m 53s (- 62m 56s) (4 66%) 1.3889 4.9638\n",
      "126m 32s (- 63m 16s) (4 66%) 1.5705 5.1087\n",
      "127m 7s (- 63m 33s) (4 66%) 1.2745 5.0326\n",
      "127m 45s (- 63m 52s) (4 66%) 1.5796 5.1558\n",
      "128m 24s (- 64m 12s) (4 66%) 1.5920 5.1232\n",
      "129m 0s (- 64m 30s) (4 66%) 1.4293 5.0870\n",
      "129m 37s (- 64m 48s) (4 66%) 1.4577 5.0688\n",
      "130m 15s (- 65m 7s) (4 66%) 1.5574 5.0580\n",
      "130m 53s (- 65m 26s) (4 66%) 1.5545 5.0652\n",
      "131m 28s (- 65m 44s) (4 66%) 1.3358 5.1123\n",
      "132m 3s (- 66m 1s) (4 66%) 1.2707 5.1341\n",
      "132m 41s (- 66m 20s) (4 66%) 1.5728 5.1739\n",
      "133m 17s (- 66m 38s) (4 66%) 1.3428 5.1449\n",
      "133m 51s (- 66m 55s) (4 66%) 1.2347 5.1413\n",
      "134m 26s (- 67m 13s) (4 66%) 1.2642 5.0688\n",
      "135m 0s (- 67m 30s) (4 66%) 1.2289 5.0942\n",
      "135m 38s (- 67m 49s) (4 66%) 1.5355 5.0000\n",
      "136m 12s (- 68m 6s) (4 66%) 1.2876 5.1051\n",
      "136m 49s (- 68m 24s) (4 66%) 1.4408 5.1232\n",
      "137m 29s (- 68m 44s) (4 66%) 1.7117 5.0761\n",
      "138m 4s (- 69m 2s) (4 66%) 1.3622 4.9275\n",
      "138m 39s (- 69m 19s) (4 66%) 1.2938 5.1123\n",
      "139m 14s (- 69m 37s) (4 66%) 1.2842 5.1377\n",
      "139m 51s (- 69m 55s) (4 66%) 1.3919 5.1159\n",
      "140m 24s (- 70m 12s) (4 66%) 1.1461 5.0942\n",
      "141m 1s (- 70m 30s) (4 66%) 1.3855 5.1051\n",
      "141m 38s (- 70m 49s) (4 66%) 1.4774 5.0543\n",
      "142m 14s (- 71m 7s) (4 66%) 1.4665 5.0906\n",
      "142m 53s (- 71m 26s) (4 66%) 1.5657 5.1014\n",
      "143m 27s (- 71m 43s) (4 66%) 1.3206 4.9891\n",
      "144m 5s (- 72m 2s) (4 66%) 1.4592 5.0833\n",
      "144m 19s (- 28m 51s) (5 83%) 0.0538 5.0725\n",
      "144m 56s (- 28m 59s) (5 83%) 1.4870 5.0362\n",
      "145m 32s (- 29m 6s) (5 83%) 1.3828 5.0652\n",
      "146m 6s (- 29m 13s) (5 83%) 1.2797 5.1014\n",
      "146m 42s (- 29m 20s) (5 83%) 1.2961 5.1920\n",
      "147m 18s (- 29m 27s) (5 83%) 1.3631 5.0797\n",
      "147m 54s (- 29m 34s) (5 83%) 1.3997 5.0543\n",
      "148m 29s (- 29m 41s) (5 83%) 1.2981 5.0036\n",
      "149m 6s (- 29m 49s) (5 83%) 1.3717 5.0978\n",
      "149m 45s (- 29m 57s) (5 83%) 1.6645 5.0435\n",
      "150m 20s (- 30m 4s) (5 83%) 1.2682 5.0688\n",
      "150m 57s (- 30m 11s) (5 83%) 1.4119 5.0399\n",
      "151m 33s (- 30m 18s) (5 83%) 1.4494 5.0543\n",
      "152m 8s (- 30m 25s) (5 83%) 1.2741 5.0399\n",
      "152m 43s (- 30m 32s) (5 83%) 1.3309 5.1341\n",
      "153m 21s (- 30m 40s) (5 83%) 1.4613 5.0507\n",
      "153m 58s (- 30m 47s) (5 83%) 1.5230 4.9601\n",
      "154m 34s (- 30m 54s) (5 83%) 1.3425 5.0580\n",
      "155m 13s (- 31m 2s) (5 83%) 1.7010 5.0761\n",
      "155m 50s (- 31m 10s) (5 83%) 1.4164 5.0833\n",
      "156m 26s (- 31m 17s) (5 83%) 1.3623 5.1159\n",
      "157m 3s (- 31m 24s) (5 83%) 1.4385 5.0833\n",
      "157m 42s (- 31m 32s) (5 83%) 1.6028 5.1051\n",
      "158m 19s (- 31m 39s) (5 83%) 1.4863 5.0688\n",
      "158m 57s (- 31m 47s) (5 83%) 1.4936 5.0688\n",
      "159m 32s (- 31m 54s) (5 83%) 1.2457 5.0688\n",
      "160m 6s (- 32m 1s) (5 83%) 1.1691 5.0543\n",
      "160m 43s (- 32m 8s) (5 83%) 1.5015 5.0978\n",
      "161m 19s (- 32m 15s) (5 83%) 1.3658 5.1087\n",
      "161m 53s (- 32m 22s) (5 83%) 1.2577 5.0616\n",
      "162m 26s (- 32m 29s) (5 83%) 1.0278 5.0761\n",
      "163m 3s (- 32m 36s) (5 83%) 1.5051 5.0761\n",
      "163m 42s (- 32m 44s) (5 83%) 1.6352 5.0362\n",
      "164m 18s (- 32m 51s) (5 83%) 1.3631 5.0978\n",
      "164m 51s (- 32m 58s) (5 83%) 1.1323 4.9746\n",
      "165m 27s (- 33m 5s) (5 83%) 1.3367 5.0797\n",
      "166m 2s (- 33m 12s) (5 83%) 1.3433 5.0761\n",
      "166m 40s (- 33m 20s) (5 83%) 1.4555 5.0761\n",
      "167m 16s (- 33m 27s) (5 83%) 1.3970 5.1051\n",
      "167m 53s (- 33m 34s) (5 83%) 1.4373 5.0507\n",
      "168m 28s (- 33m 41s) (5 83%) 1.2910 5.0181\n",
      "169m 4s (- 33m 48s) (5 83%) 1.3776 5.1268\n",
      "169m 42s (- 33m 56s) (5 83%) 1.5014 5.0978\n",
      "170m 18s (- 34m 3s) (5 83%) 1.3886 5.1449\n",
      "170m 58s (- 34m 11s) (5 83%) 1.7219 5.1087\n",
      "171m 34s (- 34m 18s) (5 83%) 1.2759 5.0906\n",
      "172m 9s (- 34m 25s) (5 83%) 1.3261 5.0616\n",
      "172m 46s (- 34m 33s) (5 83%) 1.3727 5.0761\n",
      "173m 22s (- 34m 40s) (5 83%) 1.4581 5.0978\n",
      "173m 57s (- 34m 47s) (5 83%) 1.2847 5.0580\n",
      "174m 32s (- 34m 54s) (5 83%) 1.3110 5.0870\n",
      "175m 6s (- 35m 1s) (5 83%) 1.2102 5.1159\n",
      "175m 41s (- 35m 8s) (5 83%) 1.2923 5.1232\n",
      "176m 17s (- 35m 15s) (5 83%) 1.3588 5.0906\n",
      "176m 55s (- 35m 23s) (5 83%) 1.5415 5.0471\n",
      "177m 33s (- 35m 30s) (5 83%) 1.5953 5.0326\n",
      "178m 10s (- 35m 38s) (5 83%) 1.4313 5.1304\n",
      "178m 45s (- 35m 45s) (5 83%) 1.2711 5.0399\n",
      "179m 19s (- 35m 51s) (5 83%) 1.2751 5.0942\n",
      "179m 56s (- 35m 59s) (5 83%) 1.4272 5.0181\n",
      "180m 32s (- 36m 6s) (5 83%) 1.3492 5.0580\n",
      "180m 46s (- 0m 0s) (6 100%) 0.0594 5.0399\n",
      "181m 19s (- 0m 0s) (6 100%) 1.1859 5.0326\n",
      "181m 56s (- 0m 0s) (6 100%) 1.4044 5.0362\n",
      "182m 31s (- 0m 0s) (6 100%) 1.3252 5.0290\n",
      "183m 7s (- 0m 0s) (6 100%) 1.3786 5.0870\n",
      "183m 43s (- 0m 0s) (6 100%) 1.3343 5.0725\n",
      "184m 22s (- 0m 0s) (6 100%) 1.5772 5.0761\n",
      "184m 59s (- 0m 0s) (6 100%) 1.4621 5.0145\n",
      "185m 37s (- 0m 0s) (6 100%) 1.5504 5.0543\n",
      "186m 14s (- 0m 0s) (6 100%) 1.3958 5.0725\n",
      "186m 46s (- 0m 0s) (6 100%) 1.0586 5.1087\n",
      "187m 22s (- 0m 0s) (6 100%) 1.3600 5.0870\n",
      "187m 53s (- 0m 0s) (6 100%) 0.9248 5.0652\n",
      "188m 27s (- 0m 0s) (6 100%) 1.1854 5.0761\n",
      "189m 3s (- 0m 0s) (6 100%) 1.3908 5.0580\n",
      "189m 37s (- 0m 0s) (6 100%) 1.2383 5.0109\n",
      "190m 13s (- 0m 0s) (6 100%) 1.3372 5.0254\n",
      "190m 47s (- 0m 0s) (6 100%) 1.2510 5.1087\n",
      "191m 21s (- 0m 0s) (6 100%) 1.1888 5.0870\n",
      "191m 57s (- 0m 0s) (6 100%) 1.3038 5.1123\n",
      "192m 33s (- 0m 0s) (6 100%) 1.3936 5.0797\n",
      "193m 11s (- 0m 0s) (6 100%) 1.4951 5.0906\n",
      "193m 49s (- 0m 0s) (6 100%) 1.5187 5.0507\n",
      "194m 25s (- 0m 0s) (6 100%) 1.3855 5.1014\n",
      "195m 0s (- 0m 0s) (6 100%) 1.2871 5.0906\n",
      "195m 35s (- 0m 0s) (6 100%) 1.2733 5.1159\n",
      "196m 8s (- 0m 0s) (6 100%) 1.1475 5.0870\n",
      "196m 44s (- 0m 0s) (6 100%) 1.3415 5.0797\n",
      "197m 18s (- 0m 0s) (6 100%) 1.2614 5.0725\n",
      "197m 56s (- 0m 0s) (6 100%) 1.5591 5.1159\n",
      "198m 34s (- 0m 0s) (6 100%) 1.4707 5.0254\n",
      "199m 8s (- 0m 0s) (6 100%) 1.1781 4.9891\n",
      "199m 41s (- 0m 0s) (6 100%) 1.1386 5.0616\n",
      "200m 15s (- 0m 0s) (6 100%) 1.1798 5.0942\n",
      "200m 51s (- 0m 0s) (6 100%) 1.3716 5.0580\n",
      "201m 28s (- 0m 0s) (6 100%) 1.4224 5.0688\n",
      "202m 3s (- 0m 0s) (6 100%) 1.3001 5.0471\n",
      "202m 40s (- 0m 0s) (6 100%) 1.4154 5.0906\n",
      "203m 14s (- 0m 0s) (6 100%) 1.2601 5.0761\n",
      "203m 49s (- 0m 0s) (6 100%) 1.2829 5.0833\n",
      "204m 26s (- 0m 0s) (6 100%) 1.4531 5.0543\n",
      "205m 4s (- 0m 0s) (6 100%) 1.5150 5.0543\n",
      "205m 40s (- 0m 0s) (6 100%) 1.3800 5.0471\n",
      "206m 18s (- 0m 0s) (6 100%) 1.4852 5.0399\n",
      "206m 54s (- 0m 0s) (6 100%) 1.3596 5.0000\n",
      "207m 33s (- 0m 0s) (6 100%) 1.6097 5.0399\n",
      "208m 9s (- 0m 0s) (6 100%) 1.3962 5.0217\n",
      "208m 44s (- 0m 0s) (6 100%) 1.2897 5.0290\n",
      "209m 22s (- 0m 0s) (6 100%) 1.4563 5.0471\n",
      "209m 59s (- 0m 0s) (6 100%) 1.4549 5.0471\n",
      "210m 37s (- 0m 0s) (6 100%) 1.5227 5.0652\n",
      "211m 12s (- 0m 0s) (6 100%) 1.2142 5.0580\n",
      "211m 47s (- 0m 0s) (6 100%) 1.3306 5.0507\n",
      "212m 22s (- 0m 0s) (6 100%) 1.2152 5.0507\n",
      "212m 57s (- 0m 0s) (6 100%) 1.3149 5.0580\n",
      "213m 33s (- 0m 0s) (6 100%) 1.3739 5.0217\n",
      "214m 7s (- 0m 0s) (6 100%) 1.1919 5.0326\n",
      "214m 42s (- 0m 0s) (6 100%) 1.2661 5.0435\n",
      "215m 18s (- 0m 0s) (6 100%) 1.2738 5.0471\n",
      "215m 55s (- 0m 0s) (6 100%) 1.3952 5.0290\n",
      "216m 31s (- 0m 0s) (6 100%) 1.4066 5.0362\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder = make_model().to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "plot_losses, val_losses = trainIters(encoder, decoder, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(loader, encoder, decoder):\n",
    "    decoded_words_list = []\n",
    "    decoder_attentions_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data1, data2, length1, length2) in enumerate(loader):\n",
    "            input_tensor = data1\n",
    "            input_length = input_tensor.size()[0]\n",
    "\n",
    "            encoder_output = encoder.forward(input_tensor)\n",
    "            \n",
    "            decoder_input = torch.tensor(np.array([[SOS_token]]), device=device)\n",
    "            decoder_hidden = torch.zeros(1, 1, 300, device=device)\n",
    "            \n",
    "            decoded_words = []\n",
    "            decoder_attentions = torch.zeros(MAX_LENGTH_ZH, MAX_LENGTH_ZH).to(device)\n",
    "            \n",
    "            for di in range(MAX_LENGTH_EN):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.reshape(1,1), decoder_hidden, encoder_output)\n",
    "                decoder_attentions[di] = decoder_attention.data\n",
    "                topv, topi = decoder_output.data.topk(1) \n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_words.append('<eos>')\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_words.append(idx2words_ft_en[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = decoder_input.unsqueeze(0)\n",
    "                \n",
    "            decoded_words_list.append(decoded_words)\n",
    "            decoder_attentions_list.append(decoder_attentions[:di + 1])\n",
    "                   \n",
    "        return decoded_words_list, decoder_attentions_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for validation dataset: 8.962194414739384\n"
     ]
    }
   ],
   "source": [
    "predicted_list, attention_list = evaluate(val_loader, encoder, decoder)\n",
    "\n",
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(val_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in val_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for validation dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for test dataset: 9.562411588680305\n"
     ]
    }
   ],
   "source": [
    "predicted_list, attention_list = evaluate(test_loader, encoder, decoder)\n",
    "\n",
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(test_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in test_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> If we look at the world , what happens if we can test it in a way , and doing it ? \n",
      "> If we take a look at what s really happening in the online world , we can group the attacks based on the attackers .\n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b96eb026160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b96eafaeac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(plot_losses,label=\"training loss\")\n",
    "plt.plot(val_losses,label=\"validation loss\")\n",
    "plt.title(\"Training and Validation Loss for Self-Attention Model for zh\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"self_attn_zh.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
