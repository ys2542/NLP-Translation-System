{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re  \n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from sacrebleu import corpus_bleu\n",
    "import pickle as pkl\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_vi = '../iwslt-vi-en-processed/'\n",
    "\n",
    "PAD_token = 0\n",
    "UNK_token = 1\n",
    "SOS_token = 2\n",
    "EOS_token = 3\n",
    "\n",
    "EMBED_SIZE = 300\n",
    "words_to_load = 100000\n",
    "\n",
    "with open('/scratch/xm576/wiki-news-300d-1M.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_en = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_en = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_en = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_en = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_en[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_en[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_en[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_en[s[0]] = idx\n",
    "        idx2words_ft_en[idx] = s[0]\n",
    "        ordered_words_ft_en.append(s[0])\n",
    "\n",
    "with open('/scratch/xm576/cc.vi.300.vec') as f:\n",
    "    matrix_size = words_to_load + 4\n",
    "    loaded_embeddings_ft_zh = np.zeros((matrix_size, EMBED_SIZE))\n",
    "    words_ft_zh = {'<pad>': PAD_token, '<unk>': UNK_token, '<sos>': SOS_token, '<eos>': EOS_token,}\n",
    "    idx2words_ft_zh = {PAD_token: '<pad>', UNK_token: '<unk>', SOS_token: '<sos>', EOS_token: '<eos>'}\n",
    "    ordered_words_ft_zh = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    \n",
    "    loaded_embeddings_ft_zh[0,:] = np.zeros(EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[1,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[2,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    loaded_embeddings_ft_zh[3,:] = np.random.uniform(-1.0, 1.0, EMBED_SIZE)\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == words_to_load + 1: \n",
    "            break\n",
    "        s = line.split()\n",
    "        idx = i + 3\n",
    "        loaded_embeddings_ft_zh[idx, :] = np.asarray(s[1:])\n",
    "        words_ft_zh[s[0]] = idx\n",
    "        idx2words_ft_zh[idx] = s[0]\n",
    "        ordered_words_ft_zh.append(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load/clean/filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_zh_train = open(path_vi + 'train.tok.vi', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_train = open(path_vi + 'train.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_val = open(path_vi + 'dev.tok.vi', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_val = open(path_vi + 'dev.tok.en', encoding = 'utf-8').read().strip().split('\\n')\n",
    "\n",
    "lines_zh_test = open(path_vi + 'test.tok.vi', encoding = 'utf-8').read().strip().split('\\n')\n",
    "lines_en_test = open(path_vi + 'test.tok.en', encoding = 'utf-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_lines(lines, lang):\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line == '':\n",
    "            line = ' '\n",
    "        if lang == 'en':\n",
    "            line = line.replace(\"&apos;\", \"\").replace(\"&quot;\", \"\")\n",
    "        if line[-1] != ' ':\n",
    "            line = line + ' '\n",
    "       \n",
    "        line = '<sos> ' + line + '<eos>'\n",
    "        data.append(line)\n",
    "    return data\n",
    "\n",
    "train_zh = clean_lines(lines_zh_train, 'vi')\n",
    "train_en = clean_lines(lines_en_train, 'en')\n",
    "\n",
    "val_zh = clean_lines(lines_zh_val, 'vi')\n",
    "val_en = clean_lines(lines_en_val, 'en')\n",
    "\n",
    "test_zh = clean_lines(lines_zh_test, 'vi')\n",
    "test_en = clean_lines(lines_en_test, 'en')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(data, lang):\n",
    "    indexes = []\n",
    "    for sentence in data:\n",
    "        index = []\n",
    "        for token in sentence.split():\n",
    "            if lang == 'vi':\n",
    "                try:\n",
    "                    index.append(words_ft_zh[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "            elif lang == 'en':\n",
    "                try:\n",
    "                    index.append(words_ft_en[token])\n",
    "                except KeyError:\n",
    "                    index.append(UNK_token)\n",
    "        indexes.append(index)\n",
    "    return indexes\n",
    "\n",
    "train_zh_indexes = indexesFromSentence(train_zh, 'vi')\n",
    "train_en_indexes = indexesFromSentence(train_en, 'en')\n",
    "\n",
    "val_zh_indexes = indexesFromSentence(val_zh, 'vi')\n",
    "val_en_indexes = indexesFromSentence(val_en, 'en')\n",
    "\n",
    "test_zh_indexes = indexesFromSentence(test_zh, 'vi')\n",
    "test_en_indexes = indexesFromSentence(test_en, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH_ZH = 74\n",
    "MAX_LENGTH_EN = 39\n",
    "\n",
    "train_zh_indexes_filtered = []\n",
    "train_en_indexes_filtered = []\n",
    "for i in range(len(train_zh_indexes)):\n",
    "    if len(train_zh_indexes[i]) <= MAX_LENGTH_ZH and len(train_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        train_zh_indexes_filtered.append(train_zh_indexes[i])\n",
    "        train_en_indexes_filtered.append(train_en_indexes[i])\n",
    "\n",
    "val_zh_indexes_filtered = []\n",
    "val_en_indexes_filtered = []\n",
    "for i in range(len(val_zh_indexes)):\n",
    "    if len(val_zh_indexes[i]) <= MAX_LENGTH_ZH and len(val_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        val_zh_indexes_filtered.append(val_zh_indexes[i])\n",
    "        val_en_indexes_filtered.append(val_en_indexes[i])\n",
    "\n",
    "test_zh_indexes_filtered = []\n",
    "test_en_indexes_filtered = []\n",
    "for i in range(len(test_zh_indexes)):\n",
    "    if len(test_zh_indexes[i]) <= MAX_LENGTH_ZH and len(test_en_indexes[i]) <= MAX_LENGTH_EN:\n",
    "        test_zh_indexes_filtered.append(test_zh_indexes[i])\n",
    "        test_en_indexes_filtered.append(test_en_indexes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VocabDataset(Dataset):\n",
    "    def __init__(self, data_list1, data_list2):\n",
    "        \n",
    "        self.data_list1 = data_list1\n",
    "        self.data_list2 = data_list2\n",
    "        \n",
    "        assert (len(self.data_list1) == len(self.data_list2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list1)\n",
    "            \n",
    "    def __getitem__(self, key):        \n",
    "        return [self.data_list1[key], self.data_list2[key], len(self.data_list1[key]), len(self.data_list2[key])]\n",
    "\n",
    "def vocab_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list1 = []\n",
    "    data_list2 = []\n",
    "    length_list1 = []\n",
    "    length_list2 = []\n",
    "    \n",
    "    for datum in batch:\n",
    "        length_list1.append(datum[2])\n",
    "        length_list2.append(datum[3])\n",
    "        \n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_LENGTH_ZH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_LENGTH_EN-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        \n",
    "        data_list1.append(padded_vec1[:MAX_LENGTH_ZH])\n",
    "        data_list2.append(padded_vec2[:MAX_LENGTH_EN])\n",
    "\n",
    "\n",
    "    return [torch.from_numpy(np.array(data_list1)).to(device), torch.from_numpy(np.array(data_list2)).to(device),\n",
    "                torch.LongTensor(length_list1).to(device), torch.LongTensor(length_list2).to(device)]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = VocabDataset(train_zh_indexes_filtered, train_en_indexes_filtered)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = VocabDataset(val_zh_indexes_filtered, val_en_indexes_filtered)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "test_dataset = VocabDataset(test_zh_indexes_filtered, test_en_indexes_filtered)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=1,\n",
    "                                           collate_fn=vocab_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-attention based encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, EMBED_SIZE, dropout_p, MAX_LENGTH_ZH=MAX_LENGTH_ZH):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        pe = torch.zeros(MAX_LENGTH_ZH, EMBED_SIZE)\n",
    "        position = torch.arange(0.0, MAX_LENGTH_ZH).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0.0, EMBED_SIZE, 2)*-(math.log(10000.0)/EMBED_SIZE))\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "#         self.register_buffer('pe',pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + torch.Tensor(self.pe[:, :x.size(1)]).to(device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def attention(query, key, value, dropout=None): \n",
    "    \"\"\"\n",
    "    return weighted context vector and attention weights\n",
    "    \"\"\"\n",
    "    d = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2,-1))/math.sqrt(d)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, EMBED_SIZE, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.d_k = EMBED_SIZE // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(EMBED_SIZE, EMBED_SIZE),4)\n",
    "        self.attn = None\n",
    "        self.drouput = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value): \n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "            \n",
    "        x, self.attn = attention(query, key, value)\n",
    "        \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "ff_size = 1200\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    apply full connected feed-forward network for each position\n",
    "    \"\"\"\n",
    "    def __init__(self, EMBED_SIZE, ff_size, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(EMBED_SIZE, ff_size)\n",
    "        self.w_2 = nn.Linear(ff_size, EMBED_SIZE)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    create N identical layers\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    embedding with pre-trained vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_zh).float(), freeze=False)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    layer normalization: ((X-mean)/(std+eps)) * A + B\n",
    "    \"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x-mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    In each layer, connect two sublayer with: x + layer(x)\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    create one layer with two sublayers\n",
    "    \"\"\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout),2)\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class selfAttnEncoder(nn.Module):\n",
    "    def __init__(self, encoder, input_embed):\n",
    "        super(selfAttnEncoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.input_embed = input_embed\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        return self.encoder(self.input_embed(input_data))\n",
    "\n",
    "def make_model(N=6, EMBED_SIZE=EMBED_SIZE, ff_size=ff_size, h=6, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, EMBED_SIZE)\n",
    "    ff = PositionwiseFeedForward(EMBED_SIZE, ff_size, dropout)\n",
    "    position = PositionalEncoding(EMBED_SIZE, dropout)\n",
    "    model = selfAttnEncoder(\n",
    "            Encoder(EncoderLayer(EMBED_SIZE, c(attn), c(ff), dropout), N),\n",
    "            nn.Sequential(Embeddings(EMBED_SIZE), c(position))\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH_ZH, embed_size=EMBED_SIZE):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(loaded_embeddings_ft_zh).float(), freeze=False)\n",
    "        self.attn = nn.Linear(hidden_size + embed_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(hidden_size + embed_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_data)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded, hidden), 2)), dim=2)\n",
    "        \n",
    "        attn_applied = torch.bmm(attn_weights[0].unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    batch_size, input_length = input_tensor.size()\n",
    "    _, target_length = target_tensor.size()\n",
    "    \n",
    "    encoder_output = encoder.forward(input_tensor)\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    decoder_input = torch.tensor(np.array([[SOS_token]] * batch_size).reshape(1, batch_size), device=device)\n",
    "    \n",
    "    decoder_hidden = decoder.initHidden(batch_size)\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "            decoder_input = target_tensor[:,di].unsqueeze(0) \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_output)               \n",
    "            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  \n",
    "            decoder_input = decoder_input.unsqueeze(0)\n",
    " \n",
    "            loss += criterion(decoder_output, target_tensor[:,di]) \n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(loader, encoder, decoder):\n",
    "    losses = 0\n",
    "    criterion = nn.NLLLoss()\n",
    "    for i, (input_tensor, target_tensor, input_length, target_length) in enumerate(loader):\n",
    "        encoder_output = encoder.forward(input_tensor)\n",
    "\n",
    "        decoder_input = torch.tensor(np.array([[SOS_token]]), device=device)\n",
    "        decoder_hidden = torch.zeros(1, 1, 300, device=device)\n",
    "        \n",
    "        loss = 0\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, _ = decoder(decoder_input.reshape(1,1), decoder_hidden, encoder_output)\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach().unsqueeze(0)\n",
    "            loss += criterion(decoder_output, target_tensor[:,di])\n",
    "        losses += loss/target_length\n",
    "    return losses.item()/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, plot_every=100, learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    val_losses = []\n",
    "    val_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data1, data2, length1, length2) in enumerate(train_loader):\n",
    "            input_tensor = data1\n",
    "            target_tensor = data2\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                val_loss = test_model(val_loader, encoder, decoder)\n",
    "                val_losses.append(val_loss)\n",
    "                print('%s (%d %d%%) %.4f %.4f' % (timeSince(start, iter / n_iters),\n",
    "                            iter, iter / n_iters * 100, plot_loss_avg, val_loss))\n",
    "                \n",
    "        torch.save(encoder.state_dict(), \"encoder_vi_attn_new.pth\")\n",
    "        torch.save(decoder.state_dict(), \"decoder_vi_attn_new.pth\")\n",
    "        \n",
    "        pkl.dump(plot_losses, open('vi_train_loss_new.pickle','wb'))\n",
    "        pkl.dump(val_losses, open('vi_val_loss_new.pickle','wb'))\n",
    "\n",
    "    return plot_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    \n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 13s (- 1m 6s) (1 16%) 0.1149 11.0000\n",
      "0m 46s (- 3m 52s) (1 16%) 3.8110 5.9220\n",
      "1m 20s (- 6m 40s) (1 16%) 3.0460 5.7837\n",
      "1m 53s (- 9m 29s) (1 16%) 3.0662 6.1667\n",
      "2m 27s (- 12m 15s) (1 16%) 2.9646 6.0355\n",
      "3m 0s (- 15m 3s) (1 16%) 2.9555 5.5248\n",
      "3m 34s (- 17m 52s) (1 16%) 2.9104 5.4574\n",
      "4m 8s (- 20m 40s) (1 16%) 2.8318 5.5035\n",
      "4m 42s (- 23m 31s) (1 16%) 2.8077 5.4220\n",
      "5m 16s (- 26m 21s) (1 16%) 2.8142 5.3972\n",
      "5m 50s (- 29m 10s) (1 16%) 2.7740 5.3865\n",
      "6m 23s (- 31m 58s) (1 16%) 2.7901 5.5000\n",
      "6m 57s (- 34m 47s) (1 16%) 2.7129 5.3511\n",
      "7m 31s (- 37m 35s) (1 16%) 2.7001 5.3901\n",
      "8m 4s (- 40m 24s) (1 16%) 2.7142 5.4255\n",
      "8m 38s (- 43m 10s) (1 16%) 2.6147 5.3865\n",
      "9m 11s (- 45m 58s) (1 16%) 2.6186 5.3085\n",
      "9m 45s (- 48m 46s) (1 16%) 2.5884 5.3156\n",
      "10m 18s (- 51m 34s) (1 16%) 2.5799 5.3475\n",
      "10m 52s (- 54m 22s) (1 16%) 2.5973 5.2376\n",
      "11m 26s (- 57m 11s) (1 16%) 2.5557 5.2482\n",
      "11m 59s (- 59m 57s) (1 16%) 2.5734 5.3617\n",
      "12m 33s (- 62m 45s) (1 16%) 2.5729 5.2092\n",
      "13m 6s (- 65m 33s) (1 16%) 2.5326 5.1986\n",
      "13m 40s (- 68m 20s) (1 16%) 2.5201 5.2411\n",
      "14m 13s (- 71m 9s) (1 16%) 2.5802 5.1454\n",
      "14m 47s (- 73m 58s) (1 16%) 2.5937 5.1312\n",
      "15m 21s (- 76m 46s) (1 16%) 2.4856 5.2092\n",
      "15m 55s (- 79m 35s) (1 16%) 2.5473 5.1915\n",
      "16m 28s (- 82m 24s) (1 16%) 2.5719 5.1525\n",
      "17m 2s (- 85m 12s) (1 16%) 2.4890 5.1738\n",
      "17m 35s (- 87m 59s) (1 16%) 2.4394 5.0674\n",
      "18m 9s (- 90m 48s) (1 16%) 2.4665 5.1277\n",
      "18m 43s (- 93m 37s) (1 16%) 2.4882 5.1135\n",
      "19m 17s (- 96m 26s) (1 16%) 2.4858 5.0284\n",
      "19m 50s (- 99m 14s) (1 16%) 2.4471 5.0071\n",
      "20m 24s (- 102m 1s) (1 16%) 2.3873 5.1064\n",
      "20m 57s (- 104m 49s) (1 16%) 2.4539 5.0390\n",
      "21m 16s (- 42m 32s) (2 33%) 0.8315 5.0461\n",
      "21m 49s (- 43m 39s) (2 33%) 2.3970 5.0284\n",
      "22m 23s (- 44m 47s) (2 33%) 2.4309 5.0496\n",
      "22m 57s (- 45m 54s) (2 33%) 2.4024 5.0000\n",
      "23m 30s (- 47m 1s) (2 33%) 2.3283 4.9965\n",
      "24m 4s (- 48m 8s) (2 33%) 2.3823 5.0248\n",
      "24m 38s (- 49m 16s) (2 33%) 2.4066 4.9326\n",
      "25m 11s (- 50m 23s) (2 33%) 2.3874 4.9752\n",
      "25m 45s (- 51m 30s) (2 33%) 2.3536 4.8794\n",
      "26m 19s (- 52m 38s) (2 33%) 2.3597 4.9220\n",
      "26m 52s (- 53m 45s) (2 33%) 2.3750 4.9255\n",
      "27m 26s (- 54m 53s) (2 33%) 2.3622 4.9397\n",
      "28m 0s (- 56m 0s) (2 33%) 2.3716 4.9184\n",
      "28m 33s (- 57m 7s) (2 33%) 2.3351 4.8617\n",
      "29m 7s (- 58m 15s) (2 33%) 2.3095 4.8972\n",
      "29m 41s (- 59m 22s) (2 33%) 2.2912 4.8511\n",
      "30m 15s (- 60m 30s) (2 33%) 2.3436 4.7908\n",
      "30m 48s (- 61m 37s) (2 33%) 2.2434 4.8759\n",
      "31m 22s (- 62m 44s) (2 33%) 2.3643 4.7979\n",
      "31m 56s (- 63m 52s) (2 33%) 2.3341 4.8014\n",
      "32m 29s (- 64m 59s) (2 33%) 2.2429 4.8546\n",
      "33m 3s (- 66m 6s) (2 33%) 2.3356 4.7979\n",
      "33m 36s (- 67m 13s) (2 33%) 2.3030 4.8014\n",
      "34m 9s (- 68m 19s) (2 33%) 2.2770 4.7943\n",
      "34m 43s (- 69m 26s) (2 33%) 2.2946 4.7908\n",
      "35m 17s (- 70m 34s) (2 33%) 2.3383 4.7908\n",
      "35m 50s (- 71m 41s) (2 33%) 2.2548 4.8156\n",
      "36m 24s (- 72m 49s) (2 33%) 2.3143 4.7766\n",
      "36m 58s (- 73m 56s) (2 33%) 2.3609 4.7411\n",
      "37m 31s (- 75m 3s) (2 33%) 2.2878 4.7695\n",
      "38m 5s (- 76m 10s) (2 33%) 2.2183 4.7447\n",
      "38m 38s (- 77m 17s) (2 33%) 2.2633 4.7021\n",
      "39m 12s (- 78m 25s) (2 33%) 2.3327 4.7163\n",
      "39m 46s (- 79m 32s) (2 33%) 2.3098 4.6950\n",
      "40m 19s (- 80m 39s) (2 33%) 2.2248 4.7234\n",
      "40m 53s (- 81m 46s) (2 33%) 2.2920 4.7270\n",
      "41m 26s (- 82m 53s) (2 33%) 2.2354 4.6809\n",
      "42m 0s (- 84m 0s) (2 33%) 2.2154 4.7163\n",
      "42m 18s (- 42m 18s) (3 50%) 0.8071 4.6915\n",
      "42m 52s (- 42m 52s) (3 50%) 2.1924 4.7482\n",
      "43m 26s (- 43m 26s) (3 50%) 2.2112 4.6879\n",
      "43m 59s (- 43m 59s) (3 50%) 2.2207 4.6702\n",
      "44m 33s (- 44m 33s) (3 50%) 2.2591 4.6312\n",
      "45m 7s (- 45m 7s) (3 50%) 2.2182 4.6631\n",
      "45m 40s (- 45m 40s) (3 50%) 2.1615 4.7057\n",
      "46m 14s (- 46m 14s) (3 50%) 2.1661 4.6879\n",
      "46m 47s (- 46m 47s) (3 50%) 2.1629 4.7730\n",
      "47m 21s (- 47m 21s) (3 50%) 2.2338 4.6277\n",
      "47m 55s (- 47m 55s) (3 50%) 2.2413 4.6348\n",
      "48m 28s (- 48m 28s) (3 50%) 2.1822 4.6135\n",
      "49m 2s (- 49m 2s) (3 50%) 2.2359 4.6773\n",
      "49m 35s (- 49m 35s) (3 50%) 2.1821 4.6702\n",
      "50m 9s (- 50m 9s) (3 50%) 2.2576 4.6135\n",
      "50m 43s (- 50m 43s) (3 50%) 2.1214 4.6064\n",
      "51m 16s (- 51m 16s) (3 50%) 2.1468 4.6170\n",
      "51m 50s (- 51m 50s) (3 50%) 2.1599 4.6454\n",
      "52m 24s (- 52m 24s) (3 50%) 2.1986 4.6667\n",
      "52m 57s (- 52m 57s) (3 50%) 2.2031 4.6383\n",
      "53m 31s (- 53m 31s) (3 50%) 2.2110 4.5922\n",
      "54m 5s (- 54m 5s) (3 50%) 2.1318 4.6028\n",
      "54m 38s (- 54m 38s) (3 50%) 2.1422 4.6206\n",
      "55m 12s (- 55m 12s) (3 50%) 2.2009 4.5851\n",
      "55m 46s (- 55m 46s) (3 50%) 2.1824 4.6383\n",
      "56m 19s (- 56m 19s) (3 50%) 2.1634 4.5816\n",
      "56m 53s (- 56m 53s) (3 50%) 2.1842 4.6489\n",
      "57m 27s (- 57m 27s) (3 50%) 2.2166 4.5426\n",
      "58m 0s (- 58m 0s) (3 50%) 2.1470 4.5887\n",
      "58m 34s (- 58m 34s) (3 50%) 2.1739 4.5496\n",
      "59m 7s (- 59m 7s) (3 50%) 2.1928 4.5426\n",
      "59m 41s (- 59m 41s) (3 50%) 2.0841 4.5887\n",
      "60m 14s (- 60m 14s) (3 50%) 2.0998 4.5461\n",
      "60m 48s (- 60m 48s) (3 50%) 2.1975 4.5851\n",
      "61m 21s (- 61m 21s) (3 50%) 2.1534 4.5496\n",
      "61m 55s (- 61m 55s) (3 50%) 2.1447 4.5532\n",
      "62m 28s (- 62m 28s) (3 50%) 2.1381 4.5532\n",
      "63m 2s (- 63m 2s) (3 50%) 2.1157 4.5674\n",
      "63m 21s (- 31m 40s) (4 66%) 0.7500 4.6028\n",
      "63m 55s (- 31m 57s) (4 66%) 2.1276 4.5142\n",
      "64m 28s (- 32m 14s) (4 66%) 2.0625 4.5106\n",
      "65m 2s (- 32m 31s) (4 66%) 2.0937 4.5426\n",
      "65m 36s (- 32m 48s) (4 66%) 2.0837 4.5745\n",
      "66m 9s (- 33m 4s) (4 66%) 2.0169 4.5709\n",
      "66m 43s (- 33m 21s) (4 66%) 2.1251 4.5319\n",
      "67m 16s (- 33m 38s) (4 66%) 2.0247 4.5709\n",
      "67m 49s (- 33m 54s) (4 66%) 2.0239 4.5390\n",
      "68m 23s (- 34m 11s) (4 66%) 2.1659 4.5390\n",
      "68m 56s (- 34m 28s) (4 66%) 2.0919 4.5390\n",
      "69m 30s (- 34m 45s) (4 66%) 2.1554 4.4823\n",
      "70m 4s (- 35m 2s) (4 66%) 2.1271 4.5142\n",
      "70m 37s (- 35m 18s) (4 66%) 2.0821 4.5390\n",
      "71m 11s (- 35m 35s) (4 66%) 2.0678 4.5284\n",
      "71m 44s (- 35m 52s) (4 66%) 2.0426 4.4929\n",
      "72m 18s (- 36m 9s) (4 66%) 2.1131 4.5461\n",
      "72m 52s (- 36m 26s) (4 66%) 2.0705 4.5142\n",
      "73m 25s (- 36m 42s) (4 66%) 2.0593 4.4929\n",
      "73m 58s (- 36m 59s) (4 66%) 2.0541 4.5213\n",
      "74m 32s (- 37m 16s) (4 66%) 2.0724 4.4610\n",
      "75m 5s (- 37m 32s) (4 66%) 2.0356 4.5071\n",
      "75m 39s (- 37m 49s) (4 66%) 2.0996 4.5532\n",
      "76m 13s (- 38m 6s) (4 66%) 2.1103 4.5142\n",
      "76m 46s (- 38m 23s) (4 66%) 2.0877 4.4681\n",
      "77m 20s (- 38m 40s) (4 66%) 2.1341 4.4929\n",
      "77m 54s (- 38m 57s) (4 66%) 2.0366 4.5000\n",
      "78m 27s (- 39m 13s) (4 66%) 2.0732 4.5248\n",
      "79m 1s (- 39m 30s) (4 66%) 2.1560 4.4397\n",
      "79m 34s (- 39m 47s) (4 66%) 2.0700 4.5035\n",
      "80m 8s (- 40m 4s) (4 66%) 2.0548 4.4858\n",
      "80m 41s (- 40m 20s) (4 66%) 2.0601 4.4965\n",
      "81m 15s (- 40m 37s) (4 66%) 2.0393 4.5780\n",
      "81m 48s (- 40m 54s) (4 66%) 2.0992 4.4787\n",
      "82m 22s (- 41m 11s) (4 66%) 2.0999 4.4362\n",
      "82m 55s (- 41m 27s) (4 66%) 2.0371 4.5284\n",
      "83m 29s (- 41m 44s) (4 66%) 2.0074 4.4504\n",
      "84m 2s (- 42m 1s) (4 66%) 2.0352 4.4539\n",
      "84m 21s (- 16m 52s) (5 83%) 0.7199 4.4504\n",
      "84m 54s (- 16m 58s) (5 83%) 1.9140 4.4397\n",
      "85m 28s (- 17m 5s) (5 83%) 2.0053 4.4326\n",
      "86m 2s (- 17m 12s) (5 83%) 2.0642 4.4645\n",
      "86m 35s (- 17m 19s) (5 83%) 1.9939 4.4752\n",
      "87m 9s (- 17m 25s) (5 83%) 2.0110 4.4539\n",
      "87m 42s (- 17m 32s) (5 83%) 1.9373 4.4078\n",
      "88m 16s (- 17m 39s) (5 83%) 2.0460 4.5213\n",
      "88m 49s (- 17m 45s) (5 83%) 1.9389 4.4716\n",
      "89m 23s (- 17m 52s) (5 83%) 2.0525 4.5035\n",
      "89m 57s (- 17m 59s) (5 83%) 2.0839 4.5496\n",
      "90m 30s (- 18m 6s) (5 83%) 2.0528 4.4787\n",
      "91m 4s (- 18m 12s) (5 83%) 2.0199 4.4574\n",
      "91m 38s (- 18m 19s) (5 83%) 2.0442 4.4539\n",
      "92m 11s (- 18m 26s) (5 83%) 1.9955 4.4716\n",
      "92m 45s (- 18m 33s) (5 83%) 2.0347 4.4574\n",
      "93m 19s (- 18m 39s) (5 83%) 2.0475 4.4539\n",
      "93m 52s (- 18m 46s) (5 83%) 2.0497 4.4681\n",
      "94m 26s (- 18m 53s) (5 83%) 2.0628 4.4610\n",
      "94m 59s (- 18m 59s) (5 83%) 1.9877 4.4681\n",
      "95m 33s (- 19m 6s) (5 83%) 2.0251 4.4397\n",
      "96m 7s (- 19m 13s) (5 83%) 1.9645 4.4574\n",
      "96m 40s (- 19m 20s) (5 83%) 1.9908 4.4645\n",
      "97m 14s (- 19m 26s) (5 83%) 2.0497 4.4468\n",
      "97m 47s (- 19m 33s) (5 83%) 1.9733 4.4149\n",
      "98m 21s (- 19m 40s) (5 83%) 2.0138 4.4078\n",
      "98m 54s (- 19m 46s) (5 83%) 1.9883 4.5213\n",
      "99m 28s (- 19m 53s) (5 83%) 1.9828 4.4362\n",
      "100m 1s (- 20m 0s) (5 83%) 1.9971 4.4007\n",
      "100m 35s (- 20m 7s) (5 83%) 2.0049 4.4433\n",
      "101m 8s (- 20m 13s) (5 83%) 1.9877 4.4468\n",
      "101m 42s (- 20m 20s) (5 83%) 1.9864 4.4255\n",
      "102m 15s (- 20m 27s) (5 83%) 2.0615 4.3865\n",
      "102m 49s (- 20m 33s) (5 83%) 1.9310 4.4433\n",
      "103m 22s (- 20m 40s) (5 83%) 2.0161 4.4291\n",
      "103m 56s (- 20m 47s) (5 83%) 1.9933 4.4752\n",
      "104m 29s (- 20m 53s) (5 83%) 2.0749 4.3972\n",
      "105m 3s (- 21m 0s) (5 83%) 1.9609 4.4681\n",
      "105m 21s (- 0m 0s) (6 100%) 0.7239 4.4752\n",
      "105m 55s (- 0m 0s) (6 100%) 1.8454 4.5071\n",
      "106m 28s (- 0m 0s) (6 100%) 1.8886 4.5177\n",
      "107m 2s (- 0m 0s) (6 100%) 1.9475 4.4078\n",
      "107m 35s (- 0m 0s) (6 100%) 1.9582 4.3865\n",
      "108m 9s (- 0m 0s) (6 100%) 1.8986 4.4752\n",
      "108m 42s (- 0m 0s) (6 100%) 1.9793 4.4433\n",
      "109m 15s (- 0m 0s) (6 100%) 1.8674 4.5284\n",
      "109m 49s (- 0m 0s) (6 100%) 1.8782 4.4858\n",
      "110m 22s (- 0m 0s) (6 100%) 1.9332 4.4255\n",
      "110m 56s (- 0m 0s) (6 100%) 1.9234 4.4433\n",
      "111m 30s (- 0m 0s) (6 100%) 1.9874 4.4397\n",
      "112m 3s (- 0m 0s) (6 100%) 2.0132 4.4184\n",
      "112m 37s (- 0m 0s) (6 100%) 1.9849 4.3830\n",
      "113m 10s (- 0m 0s) (6 100%) 1.9546 4.3972\n",
      "113m 44s (- 0m 0s) (6 100%) 1.8743 4.4539\n",
      "114m 17s (- 0m 0s) (6 100%) 1.9143 4.5000\n",
      "114m 50s (- 0m 0s) (6 100%) 1.8579 4.4078\n",
      "115m 24s (- 0m 0s) (6 100%) 1.9553 4.3901\n",
      "115m 58s (- 0m 0s) (6 100%) 1.9272 4.4716\n",
      "116m 31s (- 0m 0s) (6 100%) 1.9872 4.4220\n",
      "117m 5s (- 0m 0s) (6 100%) 2.0092 4.3865\n",
      "117m 39s (- 0m 0s) (6 100%) 1.9707 4.4468\n",
      "118m 12s (- 0m 0s) (6 100%) 1.8832 4.3369\n",
      "118m 45s (- 0m 0s) (6 100%) 1.9239 4.4326\n",
      "119m 19s (- 0m 0s) (6 100%) 1.9351 4.4184\n",
      "119m 53s (- 0m 0s) (6 100%) 1.9613 4.4078\n",
      "120m 26s (- 0m 0s) (6 100%) 1.9572 4.3227\n",
      "121m 0s (- 0m 0s) (6 100%) 1.9663 4.3475\n",
      "121m 34s (- 0m 0s) (6 100%) 2.0216 4.4078\n",
      "122m 7s (- 0m 0s) (6 100%) 1.8718 4.4113\n",
      "122m 41s (- 0m 0s) (6 100%) 1.9304 4.3440\n",
      "123m 14s (- 0m 0s) (6 100%) 1.9336 4.4220\n",
      "123m 48s (- 0m 0s) (6 100%) 1.9728 4.3652\n",
      "124m 21s (- 0m 0s) (6 100%) 1.9670 4.3511\n",
      "124m 55s (- 0m 0s) (6 100%) 1.9806 4.3830\n",
      "125m 28s (- 0m 0s) (6 100%) 1.9301 4.3617\n",
      "126m 2s (- 0m 0s) (6 100%) 1.9485 4.3972\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 300\n",
    "encoder = make_model().to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, len(ordered_words_ft_zh)).to(device)\n",
    "\n",
    "plot_losses, val_losses = trainIters(encoder, decoder, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(loader, encoder, decoder):\n",
    "    decoded_words_list = []\n",
    "    decoder_attentions_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, (data1, data2, length1, length2) in enumerate(loader):\n",
    "            input_tensor = data1\n",
    "            input_length = input_tensor.size()[0]\n",
    "\n",
    "            encoder_output = encoder.forward(input_tensor)\n",
    "            \n",
    "            decoder_input = torch.tensor(np.array([[SOS_token]]), device=device)\n",
    "            decoder_hidden = torch.zeros(1, 1, 300, device=device)\n",
    "            \n",
    "            decoded_words = []\n",
    "            decoder_attentions = torch.zeros(MAX_LENGTH_ZH, MAX_LENGTH_ZH).to(device)\n",
    "            \n",
    "            for di in range(MAX_LENGTH_EN):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input.reshape(1,1), decoder_hidden, encoder_output)\n",
    "                decoder_attentions[di] = decoder_attention.data\n",
    "                topv, topi = decoder_output.data.topk(1) \n",
    "                if topi.item() == EOS_token:\n",
    "                    decoded_words.append('<eos>')\n",
    "                    break\n",
    "                else:\n",
    "                    decoded_words.append(idx2words_ft_en[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "                decoder_input = decoder_input.unsqueeze(0)\n",
    "                \n",
    "            decoded_words_list.append(decoded_words)\n",
    "            decoder_attentions_list.append(decoder_attentions[:di + 1])\n",
    "                   \n",
    "        return decoded_words_list, decoder_attentions_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for validation dataset: 8.668512930771618\n"
     ]
    }
   ],
   "source": [
    "predicted_list, attention_list = evaluate(val_loader, encoder, decoder)\n",
    "\n",
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(val_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in val_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for validation dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
      "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bleu score for test dataset: 8.620389068108379\n"
     ]
    }
   ],
   "source": [
    "predicted_list, attention_list = evaluate(test_loader, encoder, decoder)\n",
    "\n",
    "predicted_list_nopad = []\n",
    "for ii in range(len(predicted_list)):\n",
    "    line = ''\n",
    "    for jj in predicted_list[ii]:\n",
    "        if jj != '<pad>':\n",
    "            line = line + ' ' + jj\n",
    "    predicted_list_nopad.append(line)\n",
    "\n",
    "for iii in range(len(predicted_list_nopad)):\n",
    "    if predicted_list_nopad[iii][-5:] == '<eos>':\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:-5]\n",
    "    else:\n",
    "        predicted_list_nopad[iii] = predicted_list_nopad[iii][5:]\n",
    "\n",
    "label_list = []\n",
    "for iii in range(len(test_en_indexes_filtered)):\n",
    "    line = ''\n",
    "    for jjj in test_en_indexes_filtered[iii]:\n",
    "        line = line + ' ' + idx2words_ft_en[jjj]\n",
    "    label_list.append(line[5:-5])\n",
    "\n",
    "print('bleu score for test dataset:', corpus_bleu(predicted_list_nopad, [label_list]).score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump(predicted_list, open('vi_predicted_list.pickle','wb'))\n",
    "pkl.dump(attention_list, open('vi_attention_list.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> And I was very good . \n",
      "> And I felt really good . \n"
     ]
    }
   ],
   "source": [
    "choice = random.randint(0, len(predicted_list_nopad)-1)\n",
    "print(predicted_list_nopad[choice])\n",
    "print(label_list[choice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss/attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEWCAYAAACAOivfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGBezJGCMMS5mScAYY1zsf/qBe203jlGMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b411d2eba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b414cfa1cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(plot_losses,label=\"training loss\")\n",
    "plt.plot(val_losses,label=\"validation loss\")\n",
    "plt.title(\"Training and Validation Loss for Self-Attention Model for vi\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"self_attn_vi_new.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAc0AAAECCAYAAACc+4x8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b41528d44a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b415297ae48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "choice = random.randint(0, len(attention_list)-1)\n",
    "plt.matshow(attention_list[choice].cpu().numpy())\n",
    "plt.savefig(\"self_attn_vi_attn_new.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
